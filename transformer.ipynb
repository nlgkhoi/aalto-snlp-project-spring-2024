{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\tprint(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "\tprint('Using MPS')\n",
    "\tdevice = torch.device(\"mps\")\n",
    "else:\n",
    "\tprint('Using CPU')\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "CLS_token = 3\n",
    "lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "\tdef __init__(self, csv_path, dataset_type='train', vocab=None):\n",
    "\t\tdf = pd.read_csv(csv_path, quoting=3)\n",
    "\t\tprint(f'len df: {len(df)}')\n",
    "\t\tif dataset_type in ['train', 'val']:\n",
    "\t\t\tself.text, self.labels = zip(*[(text, label) for text, label in zip(df['text'], df['label'])])\n",
    "\t\telse:\n",
    "\t\t\tself.text = df['text'].tolist()\n",
    "\t\t\tself.labels = [0 for _ in range(len(self.text))]\n",
    "\t\tself.ids = df['id'].tolist()\n",
    "\t\tself.dataset_type = dataset_type\n",
    "\t\tself.tokenizer = get_tokenizer('basic_english')\n",
    "\t\tself._preprocess(vocab)\n",
    "\n",
    "\tdef _preprocess(self, vocab):\n",
    "\t\t# preprocess text\n",
    "\t\tself.text = [self._preprocess_sentence(text) for text in tqdm(self.text)]\n",
    "\n",
    "\t\tif vocab is None:\n",
    "\t\t\tself.vocab = build_vocab_from_iterator(self._yield_tokens(), specials=[\"<unk>\"])\n",
    "\t\t\tself.vocab.set_default_index(self.vocab['<UNK>']) # Set the default index to <UNK> token, which mean \n",
    "\t\t\tself.vocab.insert_token('<SOS>', SOS_token)  # Insert <EOS> token with index 1\n",
    "\t\t\tself.vocab.insert_token('<EOS>', EOS_token)  # Insert <EOS> token with index 1\n",
    "\t\t\tself.vocab.insert_token('<CLS>', CLS_token)  # Insert <CLS> token with index 2\n",
    "\t\telse:\n",
    "\t\t\tself.vocab = vocab\n",
    "\t\t\t\n",
    "\t\tself.vocab_size = len(self.vocab)\n",
    "\t\n",
    "\tdef _preprocess_sentence(self, sentence):\n",
    "\t\tsentence = normalizeString(sentence)\n",
    "\t\tsentence = self.tokenizer(sentence)\n",
    "\t\tsentence = lemmaString(sentence)\n",
    "\t\treturn sentence\n",
    "\n",
    "\tdef _yield_tokens(self):\n",
    "\t\tfor text_sample in self.text:\n",
    "\t\t\tyield text_sample\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tinput_seq = text_to_indices(self.vocab, self.text[idx])\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\treturn input_seq, label, self.ids[idx]\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "\treturn ''.join(\n",
    "\t\tc for c in unicodedata.normalize('NFD', s)\n",
    "\t\tif unicodedata.category(c) != 'Mn'\n",
    "\t)\n",
    "\n",
    "def normalizeString(s):\n",
    "\ts = unicodeToAscii(s.lower().strip())\n",
    "\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\treturn s\n",
    "\n",
    "def lemmaString(tokens):\n",
    "\treturn [token.lemma_ for token in lemmatizer(' '.join(tokens))]\n",
    "\n",
    "def text_to_indices(vocab, tokens):\n",
    "\tindices = [vocab[token] for token in tokens]\n",
    "\t# add EOS token at the end\n",
    "\tindices.append(EOS_token)\n",
    "\treturn torch.tensor(indices, dtype=torch.long).view(-1)\n",
    "\n",
    "def seq_to_tokens(seq, vocab):\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[idx] for idx in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df: 99000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12569/99000 [01:43<13:47, 104.47it/s]"
     ]
    }
   ],
   "source": [
    "trainset = TranslationDataset('train_2024.csv', dataset_type='train', vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainset.vocab['<UNK>'])\n",
    "print(trainset.vocab['<SOS>'])\n",
    "print(trainset.vocab['<EOS>'])\n",
    "print(trainset.vocab['<CLS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.load('datasets/trainset.pth')\n",
    "valset = torch.load('datasets/valset.pth')\n",
    "testset = torch.load('datasets/testset.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "\t\"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "\tArgs:\n",
    "\tlist_of_samples is a list of tuples (src_seq, tgt_label, id):\n",
    "\t\tsrc_seq is of shape (src_seq_length,)\n",
    "\t\ttgt_label is of shape (1,)\n",
    "\t\tid is an int\n",
    "\n",
    "\tReturns:\n",
    "\tsrc_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "\tsrc_mask of shape (max_src_seq_length, batch_size): Boolean tensor showing which elements of the src_seqs tensor should be ignored in computations.\n",
    "\ttgt_labels of shape (batch_size, 1): Tensor of labels for each sequence.\n",
    "\t\"\"\"\n",
    "\t# YOUR CODE HERE\n",
    "\tsrc_seqs = [s[0] for s in list_of_samples]\n",
    "\tsrc_seqs = pad_sequence(src_seqs, batch_first=False, padding_value=PADDING_VALUE, )\n",
    "\tsrc_masks = (src_seqs == PADDING_VALUE)\n",
    "\n",
    "\ttgt_labels = torch.LongTensor([s[1] for s in list_of_samples])\n",
    "\t\n",
    "\t# Add CLS token at the beginning of each src sequence\n",
    "\tcls_tensor = torch.full((1, src_seqs.shape[1]), CLS_token, dtype=torch.long)\n",
    "\tsrc_seqs = torch.cat((cls_tensor, src_seqs), dim=0)\n",
    "\t\n",
    "\tids = [s[2] for s in list_of_samples]\n",
    "\n",
    "\treturn src_seqs, src_masks, tgt_labels, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)\n",
    "valloader = DataLoader(dataset=valset, batch_size=250, shuffle=False, collate_fn=collate, pin_memory=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=1, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_seqs:\n",
      " tensor([[2, 2],\n",
      "        [2, 6],\n",
      "        [1, 7],\n",
      "        [0, 1]])\n",
      "src_mask:\n",
      " tensor([[False, False],\n",
      "        [False, False],\n",
      "        [ True, False]])\n",
      "tgt_seqs:\n",
      " tensor([0, 1])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate()\n",
    "def test_collate():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([2, EOS_token]), 0, 0),\n",
    "        (torch.LongTensor([6, 7, EOS_token]), 1, 1),\n",
    "    ]\n",
    "    src_seqs, src_mask, tgt_seqs, ids = collate(pairs)\n",
    "    #src_seqs, src_mask, tgt_seqs = src_seqs[:, [1, 0]], src_mask[:, [1, 0]], tgt_seqs[:, [1, 0]]\n",
    "    print('src_seqs:\\n', src_seqs)\n",
    "    print('src_mask:\\n', src_mask)\n",
    "    print('tgt_seqs:\\n', tgt_seqs)\n",
    "    expected_src_seqs = torch.tensor([\n",
    "        [CLS_token, CLS_token],\n",
    "        [2,         6],\n",
    "        [EOS_token, 7],\n",
    "        [0,         EOS_token]\n",
    "    ])\n",
    "    expected_src_mask = torch.tensor([\n",
    "        [False, False],\n",
    "        [False, False],\n",
    "        [ True, False]\n",
    "    ])\n",
    "    expected_tgt_seqs = torch.tensor([0, 1])\n",
    "    \n",
    "    assert ((\n",
    "        (src_seqs == expected_src_seqs).all()\n",
    "         and (src_mask == expected_src_mask).all()\n",
    "         and (tgt_seqs == expected_tgt_seqs).all()\n",
    "        ) or (\n",
    "        (src_seqs == expected_src_seqs[:, [1, 0]]).all()\n",
    "         and (src_mask == expected_src_mask[:, [1, 0]]).all()\n",
    "         and (tgt_seqs == expected_tgt_seqs[:, [1, 0]]).all()\n",
    "        )\n",
    "    ), \"Wrong outputs of collate.\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class EncoderOnly(nn.Module):\n",
    "\tdef __init__(self, vocab_size, d_model, max_seq_length, nhead, num_layers, dim_feedforward, dropout):\n",
    "\t\tsuper(EncoderOnly, self).__init__()\n",
    "\t\t\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\t\tself.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "\t\t\n",
    "\t\tencoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "\t\tself.encoder = TransformerEncoder(encoder_layer, num_layers)\n",
    "\t\t\n",
    "\t\tfc_hidden = 128\n",
    "\t\tself.fc1 = nn.Linear(d_model, fc_hidden)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.fc2 = nn.Linear(fc_hidden, 1)\n",
    "\t\t\n",
    "\tdef forward(self, src, src_mask):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tsrc of shape (max_seq_length, batch_size): Input sequence.\n",
    "\t\tsrc_mask of shape (max_seq_length, batch_size): Boolean tensor indicating which elements should be ignored.\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\tprobs of shape (batch_size,): Probabilities of the sequences being toxic.\n",
    "\t\t\"\"\"\n",
    "\t\t# src: (max_seq_length, batch_size)\n",
    "\t\t# permute to (batch_size, max_seq_length)\n",
    "\t\tsrc = src.permute(1, 0)\n",
    "\t\t\n",
    "\t\tseq_length = src.shape[1]\t# src: (batch_size, max_seq_length)\n",
    "\t\t\n",
    "\t\t# Create positional indices\n",
    "\t\tpositions = torch.arange(seq_length, dtype=torch.long, device=src.device).unsqueeze(0) # shape (1, max_seq_length)\n",
    "\t\t\n",
    "\t\t# Embed the input sequences and add positional embeddings\n",
    "\t\tembeddings = self.embedding(src) + self.pos_embedding(positions) # (batch_size, max_seq_length, d_model)\n",
    "\t\t\n",
    "\t\t# Transpose the embeddings for input to the encoder\n",
    "\t\tembeddings = embeddings.transpose(0, 1)  # (max_seq_length, batch_size, d_model)\n",
    "\t\t\n",
    "\t\t# Pass the embeddings through the encoder\n",
    "\t\tencoder_output = self.encoder(embeddings, src_key_padding_mask=src_mask) # (max_seq_length, batch_size, d_model)\n",
    "\t\t\n",
    "\t\t# Take the first output of the encoder (corresponding to the CLS token)\n",
    "\t\tcls_output = encoder_output[0] # (batch_size, d_model)\n",
    "\t\t\n",
    "\t\t# Pass the averaged output through the fully connected layer for classification\n",
    "\t\tout = self.fc1(cls_output)\n",
    "\t\tout = F.relu(out)\n",
    "\t\tout = self.dropout(out)\n",
    "\t\tout = self.fc2(out)\n",
    "\t\tprobs = torch.sigmoid(out) # (batch_size, )\n",
    "\t\t\n",
    "\t\treturn probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, val_loader):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0\n",
    "\tcriterion = nn.BCELoss()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (src_seqs, src_mask, tgt_labels, ids) in enumerate(val_loader):\n",
    "\t\t\tsrc_seqs, tgt_labels, src_mask = src_seqs.to(device), tgt_labels.to(device), src_mask.to(device)\n",
    "\t\t\toutputs = model(src_seqs, src_mask)\n",
    "\t\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\treturn total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder config\n",
    "encoder = EncoderOnly(vocab_size=trainset.vocab_size, d_model=512, max_seq_length=1000, nhead=8, num_layers=2, dim_feedforward=2048, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "n_epochs = 10\n",
    "# prompt ask for continue training or not\n",
    "cont = input('Continue training? yes or no')\n",
    "if cont == 'no':\n",
    "\tprint('Fresh training')\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "else:\n",
    "\tprint(f'Continue training')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\tencoder.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_mask, tgt_labels, ids) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, src_mask, tgt_labels = src_seqs.to(device), src_mask.to(device), tgt_labels.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = encoder(src_seqs, src_mask)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(encoder, valloader)\n",
    "\n",
    "\tif eval_loss < best_val_loss:\n",
    "\t\tbest_val_loss = eval_loss\n",
    "\t\tbest_model = encoder.state_dict()\n",
    "\t\tif best_model is not None:\n",
    "\t\t\tprint(f'find new best model, save to models/lstm_glove.pth, eval_loss: {eval_loss:.4f}')\n",
    "\t\t\ttorch.save(best_model, os.path.join('models', 'lstm_glove.pth'))\n",
    "\n",
    "\t# if early_stop.stop_criterion(val_losses):\n",
    "\t# \tprint(f'Early stopping at epoch {epoch + 1}')\n",
    "\t# \tbreak\n",
    "\t\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([206, 640])\n"
     ]
    }
   ],
   "source": [
    "# loop through the trainset to get the sequence length\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = next(iter(trainloader))\n",
    "print(pad_src_seqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
