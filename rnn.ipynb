{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Except that Desmond played first base last nig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What i find funny is the loyalty and blindness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Read the article  not just the headline &amp; you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Speaking of a horses backside  is that where y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Michael Barone- gee are you dumb.  No other wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label\n",
       "0   0  Except that Desmond played first base last nig...      0\n",
       "1   1  What i find funny is the loyalty and blindness...      0\n",
       "2   2  Read the article  not just the headline & you ...      0\n",
       "3   3  Speaking of a horses backside  is that where y...      1\n",
       "4   4  Michael Barone- gee are you dumb.  No other wo...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_2024.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "EOS_token = 1\n",
    "class TranslationDataset(Dataset):\n",
    "\tdef __init__(self, csv_path, dataset_type='train', vocab=None):\n",
    "\t\tdf = pd.read_csv(csv_path)\n",
    "\t\tself.text, self.labels = zip(*[(text, label) for text, label in zip(df['text'], df['label'])])\n",
    "\t\tself.dataset_type = dataset_type\n",
    "\t\tself.tokenizer = get_tokenizer('basic_english')\n",
    "\t\tif vocab is None:\n",
    "\t\t\tself._preprocess()\n",
    "\t\telse:\n",
    "\t\t\tself.vocab = vocab\n",
    "\t\t\tself.vocab_size = len(vocab)\n",
    "\n",
    "\tdef _preprocess(self):\n",
    "\t\tself.vocab = build_vocab_from_iterator(self._yield_tokens(), specials=[\"<unk>\"])\n",
    "\t\tself.vocab.set_default_index(self.vocab['<unk>'])\n",
    "\t\tself.vocab.insert_token('<eos>', EOS_token)  # Insert <eos> token with index 1\n",
    "\t\tself.vocab_size = len(self.vocab)\n",
    "\t\t\n",
    "\tdef _yield_tokens(self):\n",
    "\t\tfor text_sample in self.text:\n",
    "\t\t\t# preprocess text\n",
    "\t\t\ttext_sample = normalizeString(text_sample)\n",
    "\t\t\tyield self.tokenizer(text_sample)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tprint(self.text[idx])\n",
    "\t\tinput_seq = text_to_indices(self.tokenizer, self.vocab, self.text[idx])\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\tprint(f'encoded: {input_seq}')\n",
    "\t\treturn input_seq, label\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "\treturn ''.join(\n",
    "\t\tc for c in unicodedata.normalize('NFD', s)\n",
    "\t\tif unicodedata.category(c) != 'Mn'\n",
    "\t)\n",
    "\n",
    "def normalizeString(s):\n",
    "\ts = unicodeToAscii(s.lower().strip())\n",
    "\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\treturn s\n",
    "\n",
    "def text_to_indices(tokenizer, vocab, text_sample):\n",
    "\ttokens = tokenizer(text_sample)\n",
    "\tindices = [vocab[token] for token in tokens]\n",
    "\tindices.append(EOS_token)\n",
    "\treturn torch.tensor(indices, dtype=torch.long).view(-1)\n",
    "\n",
    "def seq_to_tokens(seq, vocab):\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[idx] for idx in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TranslationDataset('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 65,   8,  29,  65,  80,   2, 105,  50,  52,  28,   6, 749, 457,  79,\n",
      "         25, 536,  47,  23,  26,  16,   1]) 1\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "src_sentence, label = trainset[100]\n",
    "print(src_sentence, label)\n",
    "print(type(src_sentence), type(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "\t\"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "\tArgs:\n",
    "\tlist_of_samples is a list of tuples (src_seq, tgt_label):\n",
    "\t\tsrc_seq is of shape (src_seq_length,)\n",
    "\t\ttgt_label is of shape (1,)\n",
    "\n",
    "\tReturns:\n",
    "\tsrc_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "\t\tThe sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "\t\tthe longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "\tsrc_seq_lengths: List of lengths of source sequences.\n",
    "\ttgt_labels of shape (batch_size, 1): Tensor of labels for each sequence.\n",
    "\t\"\"\"\n",
    "\t# YOUR CODE HERE\n",
    "\tsrc_seqs = [s[0] for s in list_of_samples]\n",
    "\ttgt_labels = torch.LongTensor([s[1] for s in list_of_samples])\n",
    "\tsrc_seq_lengths = [len(s) for s in src_seqs]\n",
    "\tsrc_seqs = pad_sequence(src_seqs, padding_value=PADDING_VALUE)\n",
    "\n",
    "\tsrc_seq_lengths, indices = torch.sort(torch.tensor(src_seq_lengths), descending=True)\n",
    "\tsrc_seqs = src_seqs[:, indices]\n",
    "\ttgt_labels = tgt_labels[indices]\n",
    "\n",
    "\treturn src_seqs, src_seq_lengths.tolist(), tgt_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), 1),\n",
    "        (torch.LongTensor([6, 7, 8]), 0),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert type(src_seq_lengths) == list, \"src_seq_lengths should be a list.\"\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences combined:\n",
      "tensor([[11,  6,  1],\n",
      "        [12,  7,  2],\n",
      "        [13,  8,  0],\n",
      "        [14,  0,  0]])\n",
      "[4, 3, 2]\n",
      "Target sequences combined:\n",
      "tensor([0, 1, 0])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate() function\n",
    "\n",
    "def test_collate_fn():\n",
    "    pairs = [\n",
    "        (torch.tensor([1, 2]), 0),\n",
    "        (torch.tensor([6, 7, 8]), 1),\n",
    "        (torch.tensor([11, 12, 13, 14]), 0),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert pad_src_seqs.shape == torch.Size([4, 3]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    print('Source sequences combined:')\n",
    "    print(pad_src_seqs)\n",
    "    expected = torch.tensor([\n",
    "      [11, 6, 1],\n",
    "      [12, 7, 2],\n",
    "      [13, 8, 0],\n",
    "      [14, 0, 0],\n",
    "    ])\n",
    "    assert (pad_src_seqs == expected).all(), \"pad_src_seqs does not match expected values\"\n",
    "\n",
    "    print(src_seq_lengths)\n",
    "    if isinstance(src_seq_lengths[0], torch.Size):\n",
    "        src_seq_lengths = sum((list(l) for l in src_seq_lengths), [])\n",
    "    else:\n",
    "        src_seq_lengths = [int(l) for l in src_seq_lengths]\n",
    "    assert src_seq_lengths == [4, 3, 2], f\"Bad src_seq_lengths: {src_seq_lengths}\"\n",
    "\n",
    "    print('Target sequences combined:')\n",
    "    print(pad_tgt_seqs)\n",
    "    expected = torch.tensor([\n",
    "      0, 1, 0\n",
    "    ])\n",
    "    assert (pad_tgt_seqs == expected).all(), \"pad_tgt_seqs0 does not match expected values\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We create custom DataLoader using the implemented collate function\n",
    "# # We are going to process 64 sequences at the same time (batch_size=64)\n",
    "# trainset = TranslationDataset('train_2024.csv')\n",
    "# trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data loader\n",
    "# for i, (src_seqs, src_seq_lengths, tgt_seqs) in enumerate(trainloader):\n",
    "#     print(f\"Batch {i} src_seqs:\")\n",
    "#     print(src_seqs)\n",
    "#     print(f'src_seqs.shape: {src_seqs.shape}')\n",
    "#     print(f\"Batch {i} src_seq_lengths:\")\n",
    "#     print(src_seq_lengths)\n",
    "#     print(f\"Batch {i} tgt_seqs:\")\n",
    "#     print(tgt_seqs)\n",
    "#     print(f'tgt_seqs.shape: {tgt_seqs.shape}')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self, src_dictionary_size, embed_size, hidden_size, dropout=0.2):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tsrc_dictionary_size: The number of words in the source dictionary.\n",
    "\t\tembed_size: The number of dimensions in the word embeddings.\n",
    "\t\thidden_size: The number of features in the hidden state of GRU.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(LSTM, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "\t\tself.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=1, batch_first=False, dropout=dropout, bidirectional=False)\n",
    "\t\tself.fc1 = nn.Linear(hidden_size, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t\n",
    "\tdef forward(self, pad_seqs, seq_lengths, hidden):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tpad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "\t\tseq_lengths: List of sequence lengths.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\toutputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "\t\t\"\"\"\n",
    "\t\t# YOUR CODE HERE\n",
    "\t\tembedded = self.embedding(pad_seqs)\n",
    "\t\tpacked = pack_padded_sequence(embedded, seq_lengths)\n",
    "\t\toutputs, hidden = self.lstm(packed, hidden)\n",
    "\t\toutputs, output_lengths = pad_packed_sequence(outputs, batch_first=False)\n",
    "\t\tlast_timesteps = torch.stack([outputs[length-1, i] for i, length in enumerate(output_lengths)]) # shape: (batch_size, hidden_size)\n",
    "\t\t# feed through the fully connected layer\n",
    "\t\toutputs = self.fc1(last_timesteps)\n",
    "\t\toutputs = self.dropout(outputs)\n",
    "\t\toutputs = self.relu(outputs)\n",
    "\t\toutputs = self.fc2(outputs)\n",
    "\t\toutputs = self.sigmoid(outputs)\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef init_hidden(self, batch_size=1, device='cpu'):\n",
    "\t\tnum_directions = 1\n",
    "\t\treturn (\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "def test_LSTM_shapes():\n",
    "    hidden_size = 3\n",
    "    lstm = LSTM(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = lstm.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])\n",
    "\n",
    "    outputs = lstm.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([batch_size, 1]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_LSTM_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, val_loader):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0\n",
    "\tcriterion = nn.BCELoss()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (src_seqs, src_seq_lengths, tgt_labels) in enumerate(val_loader):\n",
    "\t\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\t\thidden = model.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\t\toutputs = model(src_seqs, src_seq_lengths, hidden)\n",
    "\t\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\treturn total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "trainset = TranslationDataset('train_2024.csv')\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "valset = TranslationDataset('dev_2024.csv', vocab=trainset.vocab)\n",
    "valloader = DataLoader(dataset=trainset, batch_size=256, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM model\n",
    "hidden_size = embed_size = 128\n",
    "lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.1573, Time spent: 2.43s\n",
      "Epoch 1, iter 20: avg. loss = 0.1543, Time spent: 1.32s\n",
      "Epoch 1, iter 30: avg. loss = 0.1487, Time spent: 4.89s\n",
      "Epoch 1, iter 40: avg. loss = 0.1450, Time spent: 1.38s\n",
      "Epoch 1, iter 50: avg. loss = 0.1421, Time spent: 2.34s\n",
      "Epoch 1, iter 60: avg. loss = 0.1392, Time spent: 1.29s\n",
      "Epoch 1, iter 70: avg. loss = 0.1379, Time spent: 1.25s\n",
      "Epoch 1, iter 80: avg. loss = 0.1367, Time spent: 1.24s\n",
      "Epoch 1, iter 90: avg. loss = 0.1359, Time spent: 2.35s\n",
      "Epoch 1, iter 100: avg. loss = 0.1361, Time spent: 1.36s\n",
      "Epoch 1, iter 110: avg. loss = 0.1359, Time spent: 1.25s\n",
      "Epoch 1, iter 120: avg. loss = 0.1365, Time spent: 1.29s\n",
      "Epoch 1, iter 130: avg. loss = 0.1366, Time spent: 1.39s\n",
      "Epoch 1, iter 140: avg. loss = 0.1369, Time spent: 1.32s\n",
      "Epoch 1, iter 150: avg. loss = 0.1381, Time spent: 1.33s\n",
      "Epoch 1, val loss = 0.1204, train loss = 0.1375; Time spent: 292.67s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.1290, Time spent: 2.43s\n",
      "Epoch 2, iter 20: avg. loss = 0.1323, Time spent: 1.32s\n",
      "Epoch 2, iter 30: avg. loss = 0.1312, Time spent: 4.90s\n",
      "Epoch 2, iter 40: avg. loss = 0.1304, Time spent: 1.37s\n",
      "Epoch 2, iter 50: avg. loss = 0.1292, Time spent: 2.35s\n",
      "Epoch 2, iter 60: avg. loss = 0.1267, Time spent: 1.29s\n",
      "Epoch 2, iter 70: avg. loss = 0.1254, Time spent: 1.26s\n",
      "Epoch 2, iter 80: avg. loss = 0.1258, Time spent: 1.24s\n",
      "Epoch 2, iter 90: avg. loss = 0.1248, Time spent: 2.35s\n",
      "Epoch 2, iter 100: avg. loss = 0.1249, Time spent: 1.36s\n",
      "Epoch 2, iter 110: avg. loss = 0.1240, Time spent: 1.25s\n",
      "Epoch 2, iter 120: avg. loss = 0.1244, Time spent: 1.29s\n",
      "Epoch 2, iter 130: avg. loss = 0.1255, Time spent: 1.40s\n",
      "Epoch 2, iter 140: avg. loss = 0.1266, Time spent: 1.32s\n",
      "Epoch 2, iter 150: avg. loss = 0.1275, Time spent: 1.33s\n",
      "Epoch 2, val loss = 0.1121, train loss = 0.1271; Time spent: 292.40s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.1183, Time spent: 2.42s\n",
      "Epoch 3, iter 20: avg. loss = 0.1228, Time spent: 1.32s\n",
      "Epoch 3, iter 30: avg. loss = 0.1194, Time spent: 4.88s\n",
      "Epoch 3, iter 40: avg. loss = 0.1192, Time spent: 1.37s\n",
      "Epoch 3, iter 50: avg. loss = 0.1189, Time spent: 2.34s\n",
      "Epoch 3, iter 60: avg. loss = 0.1172, Time spent: 1.30s\n",
      "Epoch 3, iter 70: avg. loss = 0.1160, Time spent: 1.25s\n",
      "Epoch 3, iter 80: avg. loss = 0.1153, Time spent: 1.24s\n",
      "Epoch 3, iter 90: avg. loss = 0.1143, Time spent: 2.35s\n",
      "Epoch 3, iter 100: avg. loss = 0.1144, Time spent: 1.36s\n",
      "Epoch 3, iter 110: avg. loss = 0.1137, Time spent: 1.25s\n",
      "Epoch 3, iter 120: avg. loss = 0.1132, Time spent: 1.30s\n",
      "Epoch 3, iter 130: avg. loss = 0.1136, Time spent: 1.40s\n",
      "Epoch 3, iter 140: avg. loss = 0.1140, Time spent: 1.32s\n",
      "Epoch 3, iter 150: avg. loss = 0.1150, Time spent: 1.32s\n",
      "Epoch 3, val loss = 0.0966, train loss = 0.1143; Time spent: 292.58s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.1020, Time spent: 2.43s\n",
      "Epoch 4, iter 20: avg. loss = 0.1064, Time spent: 1.31s\n",
      "Epoch 4, iter 30: avg. loss = 0.1065, Time spent: 4.89s\n",
      "Epoch 4, iter 40: avg. loss = 0.1041, Time spent: 1.37s\n",
      "Epoch 4, iter 50: avg. loss = 0.1066, Time spent: 2.35s\n",
      "Epoch 4, iter 60: avg. loss = 0.1066, Time spent: 1.29s\n",
      "Epoch 4, iter 70: avg. loss = 0.1076, Time spent: 1.25s\n",
      "Epoch 4, iter 80: avg. loss = 0.1072, Time spent: 1.24s\n",
      "Epoch 4, iter 90: avg. loss = 0.1067, Time spent: 2.35s\n",
      "Epoch 4, iter 100: avg. loss = 0.1069, Time spent: 1.36s\n",
      "Epoch 4, iter 110: avg. loss = 0.1062, Time spent: 1.25s\n",
      "Epoch 4, iter 120: avg. loss = 0.1056, Time spent: 1.29s\n",
      "Epoch 4, iter 130: avg. loss = 0.1056, Time spent: 1.40s\n",
      "Epoch 4, iter 140: avg. loss = 0.1062, Time spent: 1.32s\n",
      "Epoch 4, iter 150: avg. loss = 0.1070, Time spent: 1.32s\n",
      "Epoch 4, val loss = 0.0893, train loss = 0.1067; Time spent: 292.24s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.0957, Time spent: 2.44s\n",
      "Epoch 5, iter 20: avg. loss = 0.0997, Time spent: 1.32s\n",
      "Epoch 5, iter 30: avg. loss = 0.0984, Time spent: 4.90s\n",
      "Epoch 5, iter 40: avg. loss = 0.0951, Time spent: 1.37s\n",
      "Epoch 5, iter 50: avg. loss = 0.0935, Time spent: 2.34s\n",
      "Epoch 5, iter 60: avg. loss = 0.0917, Time spent: 1.29s\n",
      "Epoch 5, iter 70: avg. loss = 0.0910, Time spent: 1.26s\n",
      "Epoch 5, iter 80: avg. loss = 0.0910, Time spent: 1.24s\n",
      "Epoch 5, iter 90: avg. loss = 0.0906, Time spent: 2.34s\n",
      "Epoch 5, iter 100: avg. loss = 0.0913, Time spent: 1.35s\n",
      "Epoch 5, iter 110: avg. loss = 0.0912, Time spent: 1.25s\n",
      "Epoch 5, iter 120: avg. loss = 0.0913, Time spent: 1.30s\n",
      "Epoch 5, iter 130: avg. loss = 0.0920, Time spent: 1.39s\n",
      "Epoch 5, iter 140: avg. loss = 0.0928, Time spent: 1.32s\n",
      "Epoch 5, iter 150: avg. loss = 0.0936, Time spent: 1.31s\n",
      "Epoch 5, val loss = 0.1005, train loss = 0.0937; Time spent: 292.59s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 6, iter 10: avg. loss = 0.0923, Time spent: 2.43s\n",
      "Epoch 6, iter 20: avg. loss = 0.0954, Time spent: 1.32s\n",
      "Epoch 6, iter 30: avg. loss = 0.0927, Time spent: 4.89s\n",
      "Epoch 6, iter 40: avg. loss = 0.0895, Time spent: 1.36s\n",
      "Epoch 6, iter 50: avg. loss = 0.0873, Time spent: 2.33s\n",
      "Epoch 6, iter 60: avg. loss = 0.0846, Time spent: 1.29s\n",
      "Epoch 6, iter 70: avg. loss = 0.0831, Time spent: 1.25s\n",
      "Epoch 6, iter 80: avg. loss = 0.0826, Time spent: 1.24s\n",
      "Epoch 6, iter 90: avg. loss = 0.0813, Time spent: 2.35s\n",
      "Epoch 6, iter 100: avg. loss = 0.0814, Time spent: 1.35s\n",
      "Epoch 6, iter 110: avg. loss = 0.0813, Time spent: 1.25s\n",
      "Epoch 6, iter 120: avg. loss = 0.0814, Time spent: 1.29s\n",
      "Epoch 6, iter 130: avg. loss = 0.0823, Time spent: 1.39s\n",
      "Epoch 6, iter 140: avg. loss = 0.0841, Time spent: 1.32s\n",
      "Epoch 6, iter 150: avg. loss = 0.0857, Time spent: 1.32s\n",
      "Epoch 6, val loss = 0.1121, train loss = 0.0855; Time spent: 292.53s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 7, iter 10: avg. loss = 0.0994, Time spent: 2.44s\n",
      "Epoch 7, iter 20: avg. loss = 0.0985, Time spent: 1.32s\n",
      "Epoch 7, iter 30: avg. loss = 0.0983, Time spent: 4.91s\n",
      "Epoch 7, iter 40: avg. loss = 0.0934, Time spent: 1.37s\n",
      "Epoch 7, iter 50: avg. loss = 0.0891, Time spent: 2.34s\n",
      "Epoch 7, iter 60: avg. loss = 0.0856, Time spent: 1.29s\n",
      "Epoch 7, iter 70: avg. loss = 0.0829, Time spent: 1.25s\n",
      "Epoch 7, iter 80: avg. loss = 0.0811, Time spent: 1.23s\n",
      "Epoch 7, iter 90: avg. loss = 0.0796, Time spent: 2.34s\n",
      "Epoch 7, iter 100: avg. loss = 0.0801, Time spent: 1.34s\n",
      "Epoch 7, iter 110: avg. loss = 0.0817, Time spent: 1.25s\n",
      "Epoch 7, iter 120: avg. loss = 0.0827, Time spent: 1.30s\n",
      "Epoch 7, iter 130: avg. loss = 0.0839, Time spent: 1.40s\n",
      "Epoch 7, iter 140: avg. loss = 0.0838, Time spent: 1.32s\n",
      "Epoch 7, iter 150: avg. loss = 0.0846, Time spent: 1.32s\n",
      "Epoch 7, val loss = 0.0825, train loss = 0.0843; Time spent: 292.29s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 8, iter 10: avg. loss = 0.0858, Time spent: 2.44s\n",
      "Epoch 8, iter 20: avg. loss = 0.0924, Time spent: 1.31s\n",
      "Epoch 8, iter 30: avg. loss = 0.0902, Time spent: 4.89s\n",
      "Epoch 8, iter 40: avg. loss = 0.0853, Time spent: 1.37s\n",
      "Epoch 8, iter 50: avg. loss = 0.0827, Time spent: 2.36s\n",
      "Epoch 8, iter 60: avg. loss = 0.0800, Time spent: 1.29s\n",
      "Epoch 8, iter 70: avg. loss = 0.0782, Time spent: 1.25s\n",
      "Epoch 8, iter 80: avg. loss = 0.0765, Time spent: 1.24s\n",
      "Epoch 8, iter 90: avg. loss = 0.0747, Time spent: 2.34s\n",
      "Epoch 8, iter 100: avg. loss = 0.0736, Time spent: 1.36s\n",
      "Epoch 8, iter 110: avg. loss = 0.0730, Time spent: 1.25s\n",
      "Epoch 8, iter 120: avg. loss = 0.0722, Time spent: 1.30s\n",
      "Epoch 8, iter 130: avg. loss = 0.0724, Time spent: 1.40s\n",
      "Epoch 8, iter 140: avg. loss = 0.0723, Time spent: 1.32s\n",
      "Epoch 8, iter 150: avg. loss = 0.0726, Time spent: 1.32s\n",
      "Epoch 8, val loss = 0.0656, train loss = 0.0725; Time spent: 292.57s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 9, iter 10: avg. loss = 0.0735, Time spent: 2.44s\n",
      "Epoch 9, iter 20: avg. loss = 0.0756, Time spent: 1.31s\n",
      "Epoch 9, iter 30: avg. loss = 0.0735, Time spent: 4.89s\n",
      "Epoch 9, iter 40: avg. loss = 0.0719, Time spent: 1.37s\n",
      "Epoch 9, iter 50: avg. loss = 0.0714, Time spent: 2.35s\n",
      "Epoch 9, iter 60: avg. loss = 0.0703, Time spent: 1.29s\n",
      "Epoch 9, iter 70: avg. loss = 0.0697, Time spent: 1.25s\n",
      "Epoch 9, iter 80: avg. loss = 0.0681, Time spent: 1.24s\n",
      "Epoch 9, iter 90: avg. loss = 0.0668, Time spent: 2.34s\n",
      "Epoch 9, iter 100: avg. loss = 0.0657, Time spent: 1.36s\n",
      "Epoch 9, iter 110: avg. loss = 0.0649, Time spent: 1.25s\n",
      "Epoch 9, iter 120: avg. loss = 0.0640, Time spent: 1.30s\n",
      "Epoch 9, iter 130: avg. loss = 0.0642, Time spent: 1.40s\n",
      "Epoch 9, iter 140: avg. loss = 0.0640, Time spent: 1.32s\n",
      "Epoch 9, iter 150: avg. loss = 0.0639, Time spent: 1.32s\n",
      "Epoch 9, val loss = 0.0498, train loss = 0.0634; Time spent: 292.52s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 10, iter 10: avg. loss = 0.0584, Time spent: 2.44s\n",
      "Epoch 10, iter 20: avg. loss = 0.0599, Time spent: 1.32s\n",
      "Epoch 10, iter 30: avg. loss = 0.0590, Time spent: 4.89s\n",
      "Epoch 10, iter 40: avg. loss = 0.0580, Time spent: 1.37s\n",
      "Epoch 10, iter 50: avg. loss = 0.0600, Time spent: 2.34s\n",
      "Epoch 10, iter 60: avg. loss = 0.0601, Time spent: 1.29s\n",
      "Epoch 10, iter 70: avg. loss = 0.0597, Time spent: 1.25s\n",
      "Epoch 10, iter 80: avg. loss = 0.0583, Time spent: 1.24s\n",
      "Epoch 10, iter 90: avg. loss = 0.0569, Time spent: 2.35s\n",
      "Epoch 10, iter 100: avg. loss = 0.0565, Time spent: 1.36s\n",
      "Epoch 10, iter 110: avg. loss = 0.0570, Time spent: 1.24s\n",
      "Epoch 10, iter 120: avg. loss = 0.0571, Time spent: 1.30s\n",
      "Epoch 10, iter 130: avg. loss = 0.0584, Time spent: 1.40s\n",
      "Epoch 10, iter 140: avg. loss = 0.0582, Time spent: 1.32s\n",
      "Epoch 10, iter 150: avg. loss = 0.0583, Time spent: 1.32s\n",
      "Epoch 10, val loss = 0.0468, train loss = 0.0578; Time spent: 292.78s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 11, iter 10: avg. loss = 0.0567, Time spent: 2.43s\n",
      "Epoch 11, iter 20: avg. loss = 0.0581, Time spent: 1.31s\n",
      "Epoch 11, iter 30: avg. loss = 0.0592, Time spent: 4.90s\n",
      "Epoch 11, iter 40: avg. loss = 0.0576, Time spent: 1.37s\n",
      "Epoch 11, iter 50: avg. loss = 0.0567, Time spent: 2.34s\n",
      "Epoch 11, iter 60: avg. loss = 0.0568, Time spent: 1.29s\n",
      "Epoch 11, iter 70: avg. loss = 0.0556, Time spent: 1.25s\n",
      "Epoch 11, iter 80: avg. loss = 0.0547, Time spent: 1.23s\n",
      "Epoch 11, iter 90: avg. loss = 0.0538, Time spent: 2.35s\n",
      "Epoch 11, iter 100: avg. loss = 0.0528, Time spent: 1.36s\n",
      "Epoch 11, iter 110: avg. loss = 0.0523, Time spent: 1.25s\n",
      "Epoch 11, iter 120: avg. loss = 0.0513, Time spent: 1.30s\n",
      "Epoch 11, iter 130: avg. loss = 0.0512, Time spent: 1.40s\n",
      "Epoch 11, iter 140: avg. loss = 0.0505, Time spent: 1.32s\n",
      "Epoch 11, iter 150: avg. loss = 0.0506, Time spent: 1.32s\n",
      "Epoch 11, val loss = 0.0398, train loss = 0.0502; Time spent: 292.98s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 12, iter 10: avg. loss = 0.0428, Time spent: 2.45s\n",
      "Epoch 12, iter 20: avg. loss = 0.0466, Time spent: 1.31s\n",
      "Epoch 12, iter 30: avg. loss = 0.0465, Time spent: 4.90s\n",
      "Epoch 12, iter 40: avg. loss = 0.0454, Time spent: 1.37s\n",
      "Epoch 12, iter 50: avg. loss = 0.0433, Time spent: 2.35s\n",
      "Epoch 12, iter 60: avg. loss = 0.0434, Time spent: 1.29s\n",
      "Epoch 12, iter 70: avg. loss = 0.0424, Time spent: 1.24s\n",
      "Epoch 12, iter 80: avg. loss = 0.0421, Time spent: 1.24s\n",
      "Epoch 12, iter 90: avg. loss = 0.0419, Time spent: 2.35s\n",
      "Epoch 12, iter 100: avg. loss = 0.0414, Time spent: 1.36s\n",
      "Epoch 12, iter 110: avg. loss = 0.0410, Time spent: 1.24s\n",
      "Epoch 12, iter 120: avg. loss = 0.0401, Time spent: 1.30s\n",
      "Epoch 12, iter 130: avg. loss = 0.0403, Time spent: 1.40s\n",
      "Epoch 12, iter 140: avg. loss = 0.0402, Time spent: 1.32s\n",
      "Epoch 12, iter 150: avg. loss = 0.0404, Time spent: 1.32s\n",
      "Epoch 12, val loss = 0.0399, train loss = 0.0401; Time spent: 292.99s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 13, iter 10: avg. loss = 0.0431, Time spent: 2.44s\n",
      "Epoch 13, iter 20: avg. loss = 0.0440, Time spent: 1.32s\n",
      "Epoch 13, iter 30: avg. loss = 0.0424, Time spent: 4.89s\n",
      "Epoch 13, iter 40: avg. loss = 0.0407, Time spent: 1.37s\n",
      "Epoch 13, iter 50: avg. loss = 0.0384, Time spent: 2.34s\n",
      "Epoch 13, iter 60: avg. loss = 0.0393, Time spent: 1.29s\n",
      "Epoch 13, iter 70: avg. loss = 0.0395, Time spent: 1.26s\n",
      "Epoch 13, iter 80: avg. loss = 0.0421, Time spent: 1.23s\n",
      "Epoch 13, iter 90: avg. loss = 0.0432, Time spent: 2.35s\n",
      "Epoch 13, iter 100: avg. loss = 0.0430, Time spent: 1.35s\n",
      "Epoch 13, iter 110: avg. loss = 0.0427, Time spent: 1.25s\n",
      "Epoch 13, iter 120: avg. loss = 0.0420, Time spent: 1.30s\n",
      "Epoch 13, iter 130: avg. loss = 0.0418, Time spent: 1.40s\n",
      "Epoch 13, iter 140: avg. loss = 0.0413, Time spent: 1.32s\n",
      "Epoch 13, iter 150: avg. loss = 0.0411, Time spent: 1.32s\n",
      "Epoch 13, val loss = 0.0348, train loss = 0.0406; Time spent: 292.65s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 14, iter 10: avg. loss = 0.0309, Time spent: 2.44s\n",
      "Epoch 14, iter 20: avg. loss = 0.0314, Time spent: 1.32s\n",
      "Epoch 14, iter 30: avg. loss = 0.0321, Time spent: 4.91s\n",
      "Epoch 14, iter 40: avg. loss = 0.0318, Time spent: 1.37s\n",
      "Epoch 14, iter 50: avg. loss = 0.0317, Time spent: 2.34s\n",
      "Epoch 14, iter 60: avg. loss = 0.0313, Time spent: 1.35s\n",
      "Epoch 14, iter 70: avg. loss = 0.0308, Time spent: 1.25s\n",
      "Epoch 14, iter 80: avg. loss = 0.0318, Time spent: 1.24s\n",
      "Epoch 14, iter 90: avg. loss = 0.0337, Time spent: 2.35s\n",
      "Epoch 14, iter 100: avg. loss = 0.0348, Time spent: 1.36s\n",
      "Epoch 14, iter 110: avg. loss = 0.0352, Time spent: 1.25s\n",
      "Epoch 14, iter 120: avg. loss = 0.0348, Time spent: 1.29s\n",
      "Epoch 14, iter 130: avg. loss = 0.0344, Time spent: 1.41s\n",
      "Epoch 14, iter 140: avg. loss = 0.0341, Time spent: 1.32s\n",
      "Epoch 14, iter 150: avg. loss = 0.0340, Time spent: 1.32s\n",
      "Epoch 14, val loss = 0.0299, train loss = 0.0336; Time spent: 1185.33s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 15, iter 10: avg. loss = 0.0299, Time spent: 2.43s\n",
      "Epoch 15, iter 20: avg. loss = 0.0327, Time spent: 1.31s\n",
      "Epoch 15, iter 30: avg. loss = 0.0353, Time spent: 4.89s\n",
      "Epoch 15, iter 40: avg. loss = 0.0353, Time spent: 1.38s\n",
      "Epoch 15, iter 50: avg. loss = 0.0335, Time spent: 2.34s\n",
      "Epoch 15, iter 60: avg. loss = 0.0324, Time spent: 1.29s\n",
      "Epoch 15, iter 70: avg. loss = 0.0315, Time spent: 1.26s\n",
      "Epoch 15, iter 80: avg. loss = 0.0318, Time spent: 1.24s\n",
      "Epoch 15, iter 90: avg. loss = 0.0340, Time spent: 2.34s\n",
      "Epoch 15, iter 100: avg. loss = 0.0380, Time spent: 1.36s\n",
      "Epoch 15, iter 110: avg. loss = 0.0397, Time spent: 1.25s\n",
      "Epoch 15, iter 120: avg. loss = 0.0403, Time spent: 1.29s\n",
      "Epoch 15, iter 130: avg. loss = 0.0401, Time spent: 1.40s\n",
      "Epoch 15, iter 140: avg. loss = 0.0402, Time spent: 1.32s\n",
      "Epoch 15, iter 150: avg. loss = 0.0402, Time spent: 1.32s\n",
      "Epoch 15, val loss = 0.0312, train loss = 0.0399; Time spent: 292.47s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 16, iter 10: avg. loss = 0.0301, Time spent: 2.44s\n",
      "Epoch 16, iter 20: avg. loss = 0.0307, Time spent: 1.31s\n",
      "Epoch 16, iter 30: avg. loss = 0.0318, Time spent: 4.89s\n",
      "Epoch 16, iter 40: avg. loss = 0.0320, Time spent: 1.37s\n",
      "Epoch 16, iter 50: avg. loss = 0.0313, Time spent: 2.35s\n",
      "Epoch 16, iter 60: avg. loss = 0.0299, Time spent: 1.29s\n",
      "Epoch 16, iter 70: avg. loss = 0.0302, Time spent: 1.25s\n",
      "Epoch 16, iter 80: avg. loss = 0.0305, Time spent: 1.23s\n",
      "Epoch 16, iter 90: avg. loss = 0.0317, Time spent: 2.36s\n",
      "Epoch 16, iter 100: avg. loss = 0.0331, Time spent: 1.35s\n",
      "Epoch 16, iter 110: avg. loss = 0.0346, Time spent: 1.25s\n",
      "Epoch 16, iter 120: avg. loss = 0.0347, Time spent: 1.30s\n",
      "Epoch 16, iter 130: avg. loss = 0.0341, Time spent: 1.40s\n",
      "Epoch 16, iter 140: avg. loss = 0.0338, Time spent: 1.32s\n",
      "Epoch 16, iter 150: avg. loss = 0.0334, Time spent: 1.33s\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm, valloader)\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.5103, Time spent: 4.86s\n",
      "Epoch 1, iter 20: avg. loss = 0.5001, Time spent: 2.63s\n",
      "Epoch 1, iter 30: avg. loss = 0.4859, Time spent: 9.74s\n",
      "Epoch 1, iter 40: avg. loss = 0.4745, Time spent: 2.72s\n",
      "Epoch 1, iter 50: avg. loss = 0.4682, Time spent: 4.65s\n",
      "Epoch 1, iter 60: avg. loss = 0.4589, Time spent: 2.58s\n",
      "Epoch 1, iter 70: avg. loss = 0.4506, Time spent: 2.48s\n",
      "Epoch 1, iter 80: avg. loss = 0.4419, Time spent: 2.46s\n",
      "Epoch 1, iter 90: avg. loss = 0.4335, Time spent: 4.71s\n",
      "Epoch 1, iter 100: avg. loss = 0.4273, Time spent: 2.71s\n",
      "Epoch 1, iter 110: avg. loss = 0.4230, Time spent: 2.49s\n",
      "Epoch 1, iter 120: avg. loss = 0.4187, Time spent: 2.59s\n",
      "Epoch 1, iter 130: avg. loss = 0.4133, Time spent: 2.78s\n",
      "Epoch 1, iter 140: avg. loss = 0.4078, Time spent: 2.62s\n",
      "Epoch 1, iter 150: avg. loss = 0.4018, Time spent: 2.61s\n",
      "Epoch 1, val loss = 0.9767, train loss = 0.3986; Time spent: 543.38s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.2934, Time spent: 4.84s\n",
      "Epoch 2, iter 20: avg. loss = 0.2960, Time spent: 2.61s\n",
      "Epoch 2, iter 30: avg. loss = 0.2875, Time spent: 9.76s\n",
      "Epoch 2, iter 40: avg. loss = 0.2819, Time spent: 2.73s\n",
      "Epoch 2, iter 50: avg. loss = 0.2751, Time spent: 4.66s\n",
      "Epoch 2, iter 60: avg. loss = 0.2692, Time spent: 2.58s\n",
      "Epoch 2, iter 70: avg. loss = 0.2657, Time spent: 2.49s\n",
      "Epoch 2, iter 80: avg. loss = 0.2617, Time spent: 2.49s\n",
      "Epoch 2, iter 90: avg. loss = 0.2578, Time spent: 4.68s\n",
      "Epoch 2, iter 100: avg. loss = 0.2552, Time spent: 2.72s\n",
      "Epoch 2, iter 110: avg. loss = 0.2532, Time spent: 2.50s\n",
      "Epoch 2, iter 120: avg. loss = 0.2521, Time spent: 2.61s\n",
      "Epoch 2, iter 130: avg. loss = 0.2504, Time spent: 2.79s\n",
      "Epoch 2, iter 140: avg. loss = 0.2482, Time spent: 2.62s\n",
      "Epoch 2, iter 150: avg. loss = 0.2462, Time spent: 2.62s\n",
      "Epoch 2, val loss = 1.2228, train loss = 0.2442; Time spent: 545.64s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.2082, Time spent: 4.84s\n",
      "Epoch 3, iter 20: avg. loss = 0.2090, Time spent: 2.61s\n",
      "Epoch 3, iter 30: avg. loss = 0.2037, Time spent: 9.75s\n",
      "Epoch 3, iter 40: avg. loss = 0.2030, Time spent: 2.72s\n",
      "Epoch 3, iter 50: avg. loss = 0.1999, Time spent: 4.65s\n",
      "Epoch 3, iter 60: avg. loss = 0.1971, Time spent: 2.56s\n",
      "Epoch 3, iter 70: avg. loss = 0.1958, Time spent: 2.47s\n",
      "Epoch 3, iter 80: avg. loss = 0.1929, Time spent: 2.46s\n",
      "Epoch 3, iter 90: avg. loss = 0.1905, Time spent: 4.67s\n",
      "Epoch 3, iter 100: avg. loss = 0.1901, Time spent: 2.70s\n",
      "Epoch 3, iter 110: avg. loss = 0.1899, Time spent: 2.47s\n",
      "Epoch 3, iter 120: avg. loss = 0.1899, Time spent: 2.58s\n",
      "Epoch 3, iter 130: avg. loss = 0.1889, Time spent: 2.77s\n",
      "Epoch 3, iter 140: avg. loss = 0.1881, Time spent: 2.62s\n",
      "Epoch 3, iter 150: avg. loss = 0.1873, Time spent: 2.62s\n",
      "Epoch 3, val loss = 1.3591, train loss = 0.1861; Time spent: 542.05s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.1720, Time spent: 4.86s\n",
      "Epoch 4, iter 20: avg. loss = 0.1744, Time spent: 2.62s\n",
      "Epoch 4, iter 30: avg. loss = 0.1718, Time spent: 9.76s\n",
      "Epoch 4, iter 40: avg. loss = 0.1704, Time spent: 2.72s\n",
      "Epoch 4, iter 50: avg. loss = 0.1672, Time spent: 4.65s\n",
      "Epoch 4, iter 60: avg. loss = 0.1654, Time spent: 2.57s\n",
      "Epoch 4, iter 70: avg. loss = 0.1640, Time spent: 2.47s\n",
      "Epoch 4, iter 80: avg. loss = 0.1611, Time spent: 2.46s\n",
      "Epoch 4, iter 90: avg. loss = 0.1587, Time spent: 4.68s\n",
      "Epoch 4, iter 100: avg. loss = 0.1579, Time spent: 2.70s\n",
      "Epoch 4, iter 110: avg. loss = 0.1573, Time spent: 2.49s\n",
      "Epoch 4, iter 120: avg. loss = 0.1573, Time spent: 2.57s\n",
      "Epoch 4, iter 130: avg. loss = 0.1565, Time spent: 2.77s\n",
      "Epoch 4, iter 140: avg. loss = 0.1556, Time spent: 2.62s\n",
      "Epoch 4, iter 150: avg. loss = 0.1549, Time spent: 2.62s\n",
      "Epoch 4, val loss = 1.4408, train loss = 0.1537; Time spent: 541.74s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.1423, Time spent: 4.85s\n",
      "Epoch 5, iter 20: avg. loss = 0.1441, Time spent: 2.62s\n",
      "Epoch 5, iter 30: avg. loss = 0.1426, Time spent: 9.75s\n",
      "Epoch 5, iter 40: avg. loss = 0.1421, Time spent: 2.72s\n",
      "Epoch 5, iter 50: avg. loss = 0.1407, Time spent: 4.66s\n",
      "Epoch 5, iter 60: avg. loss = 0.1393, Time spent: 2.57s\n",
      "Epoch 5, iter 70: avg. loss = 0.1392, Time spent: 2.47s\n",
      "Epoch 5, iter 80: avg. loss = 0.1374, Time spent: 2.45s\n",
      "Epoch 5, iter 90: avg. loss = 0.1351, Time spent: 4.66s\n",
      "Epoch 5, iter 100: avg. loss = 0.1354, Time spent: 2.69s\n",
      "Epoch 5, iter 110: avg. loss = 0.1349, Time spent: 2.49s\n",
      "Epoch 5, iter 120: avg. loss = 0.1347, Time spent: 2.57s\n",
      "Epoch 5, iter 130: avg. loss = 0.1345, Time spent: 2.78s\n",
      "Epoch 5, iter 140: avg. loss = 0.1334, Time spent: 2.62s\n",
      "Epoch 5, iter 150: avg. loss = 0.1328, Time spent: 2.63s\n",
      "Epoch 5, val loss = 1.4700, train loss = 0.1316; Time spent: 541.86s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 6, iter 10: avg. loss = 0.1233, Time spent: 4.85s\n",
      "Epoch 6, iter 20: avg. loss = 0.1237, Time spent: 2.62s\n",
      "Epoch 6, iter 30: avg. loss = 0.1230, Time spent: 9.76s\n",
      "Epoch 6, iter 40: avg. loss = 0.1217, Time spent: 2.71s\n",
      "Epoch 6, iter 50: avg. loss = 0.1220, Time spent: 4.65s\n",
      "Epoch 6, iter 60: avg. loss = 0.1228, Time spent: 2.56s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\aalto-snlp-project-spring-2024\\rnn.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m lstm(src_seqs, src_seq_lengths, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), tgt_labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm, valloader)\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(lstm, pad_src_seqs, src_seq_lengths):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "\n",
    "    Args:\n",
    "    lstm (LSTM): Trained lstm.\n",
    "    pad_src_seqs of shape (max_src_seq_length, batch_size): Padded source sequences.\n",
    "    src_seq_lengths: List of source sequence lengths.\n",
    "\n",
    "    Returns:\n",
    "    out_seqs of shape (batch_size, 1): LongTensor of word indices of the output sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        pad_src_seqs = pad_src_seqs.to(device)\n",
    "        lstm_hidden = lstm.init_hidden(pad_src_seqs.shape[1], device)\n",
    "        outputs = lstm(pad_src_seqs, src_seq_lengths, lstm_hidden)\n",
    "        out_seqs = outputs > 0.5\n",
    "        return out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    pad_src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths=[4, 2])\n",
    "    assert out_seqs.shape == torch.Size([2, 1]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify training data:\n",
      "-----------------------------\n",
      "Except that Desmond played first base last night. Tapia was in LF  and Reynolds had a night off.\n",
      "encoded: tensor([  574,     9,  7565,  1241,   150,   890,   172,   813,     2, 20459,\n",
      "           29,    11, 28257,     5, 12133,    93,     6,   813,   139,     2,\n",
      "            1])\n",
      "What i find funny is the loyalty and blindness of english community. The worst possible choice for them is liberal and yet they keep voting for them every time. They keep renewing hope every election 1 year prior to it  just to ignore them at the winning speach already. Honestly PQ have more respect for english community then liberal  at least they dont lie to you just to get your vote. That being said i dont vote PQ either tired of those old man but that is another story. I mostly vote local candidate regardless of party even voted liberal once.. Outch that was hard to admit. But seriously guy's drop the act anti PQ anti QS dont vote for CAQ cause they dont win etc.. Any of those will at least respect you when they say no. And most of time they will say yes and ACT on it not just saying it like liberals do.\n",
      "encoded: tensor([   30,    12,   235,   655,     8,     3,  4219,     5, 12778,     7,\n",
      "         1366,   548,     2,     3,   785,   636,   584,    14,    65,     8,\n",
      "          233,     5,   245,    22,   169,   619,    14,    65,   145,    72,\n",
      "            2,    22,   169, 17261,   270,   145,   268,     0,   144,  1689,\n",
      "            4,    13,    49,     4,  1041,    65,    45,     3,  1556, 25458,\n",
      "          252,     2,  2272, 13750,    25,    57,   649,    14,  1366,   548,\n",
      "           86,   233,    45,   272,    22,  1050,   617,     4,    10,    49,\n",
      "            4,    62,    35,   198,     2,     9,   102,   133,    12,  1050,\n",
      "          198, 13750,   308,  1259,     7,    84,   227,   190,    32,     9,\n",
      "            8,   137,   321,     2,    12,  1163,   198,   508,   787,  1077,\n",
      "            7,   209,    91,   397,   233,   335,     2,     2, 57328,     9,\n",
      "           29,   314,     4,  1276,     2,    32,   716,   264,     0,    16,\n",
      "         1179,     3,   461,   452, 13750,   452, 36545,  1050,   198,    14,\n",
      "        31701,   552,    22,  1050,   532,   293,     2,     2,    80,     7,\n",
      "           84,    37,    45,   272,   649,    10,    64,    22,   123,    41,\n",
      "            2,     5,   112,     7,    72,    22,    37,   123,   193,     5,\n",
      "          461,    24,    13,    20,    49,   331,    13,    48,   301,    46,\n",
      "            2,     1])\n",
      "Read the article  not just the headline & you will find out.\n",
      "encoded: tensor([ 216,    3,  204,   20,   49,    3, 1492,    0,   10,   37,  235,   59,\n",
      "           2,    1])\n",
      "SRC: ['what', 'i', 'find', 'funny', 'is', 'the', 'loyalty', 'and', 'blindness', 'of', 'english', 'community', '.', 'the', 'worst', 'possible', 'choice', 'for', 'them', 'is', 'liberal', 'and', 'yet', 'they', 'keep', 'voting', 'for', 'them', 'every', 'time', '.', 'they', 'keep', 'renewing', 'hope', 'every', 'election', '<unk>', 'year', 'prior', 'to', 'it', 'just', 'to', 'ignore', 'them', 'at', 'the', 'winning', 'speach', 'already', '.', 'honestly', 'pq', 'have', 'more', 'respect', 'for', 'english', 'community', 'then', 'liberal', 'at', 'least', 'they', 'dont', 'lie', 'to', 'you', 'just', 'to', 'get', 'your', 'vote', '.', 'that', 'being', 'said', 'i', 'dont', 'vote', 'pq', 'either', 'tired', 'of', 'those', 'old', 'man', 'but', 'that', 'is', 'another', 'story', '.', 'i', 'mostly', 'vote', 'local', 'candidate', 'regardless', 'of', 'party', 'even', 'voted', 'liberal', 'once', '.', '.', 'outch', 'that', 'was', 'hard', 'to', 'admit', '.', 'but', 'seriously', 'guy', '<unk>', 's', 'drop', 'the', 'act', 'anti', 'pq', 'anti', 'qs', 'dont', 'vote', 'for', 'caq', 'cause', 'they', 'dont', 'win', 'etc', '.', '.', 'any', 'of', 'those', 'will', 'at', 'least', 'respect', 'you', 'when', 'they', 'say', 'no', '.', 'and', 'most', 'of', 'time', 'they', 'will', 'say', 'yes', 'and', 'act', 'on', 'it', 'not', 'just', 'saying', 'it', 'like', 'liberals', 'do', '.', '<eos>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['except', 'that', 'desmond', 'played', 'first', 'base', 'last', 'night', '.', 'tapia', 'was', 'in', 'lf', 'and', 'reynolds', 'had', 'a', 'night', 'off', '.', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Classify training data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs = next(iter(valloader))\n",
    "out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_tokens(pad_src_seqs[:,i], trainset.vocab))\n",
    "    print('TGT:', pad_tgt_seqs[i].item())\n",
    "    print('OUT:', out_seqs[i].item())\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
