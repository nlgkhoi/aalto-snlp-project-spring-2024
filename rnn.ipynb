{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Except that Desmond played first base last nig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What i find funny is the loyalty and blindness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Read the article  not just the headline &amp; you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Speaking of a horses backside  is that where y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Michael Barone- gee are you dumb.  No other wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label\n",
       "0   0  Except that Desmond played first base last nig...      0\n",
       "1   1  What i find funny is the loyalty and blindness...      0\n",
       "2   2  Read the article  not just the headline & you ...      0\n",
       "3   3  Speaking of a horses backside  is that where y...      1\n",
       "4   4  Michael Barone- gee are you dumb.  No other wo...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_2024.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "EOS_token = 1\n",
    "class TranslationDataset(Dataset):\n",
    "\tdef __init__(self, csv_path, dataset_type='train', vocab=None):\n",
    "\t\tdf = pd.read_csv(csv_path)\n",
    "\t\tself.text, self.labels = zip(*[(text, label) for text, label in zip(df['text'], df['label'])])\n",
    "\t\tself.dataset_type = dataset_type\n",
    "\t\tself.tokenizer = get_tokenizer('basic_english')\n",
    "\t\tif vocab is None:\n",
    "\t\t\tself._preprocess()\n",
    "\t\telse:\n",
    "\t\t\tself.vocab = vocab\n",
    "\t\t\tself.vocab_size = len(vocab)\n",
    "\n",
    "\tdef _preprocess(self):\n",
    "\t\tself.vocab = build_vocab_from_iterator(self._yield_tokens(), specials=[\"<unk>\"])\n",
    "\t\tself.vocab.set_default_index(self.vocab['<unk>'])\n",
    "\t\tself.vocab.insert_token('<eos>', EOS_token)  # Insert <eos> token with index 1\n",
    "\t\tself.vocab_size = len(self.vocab)\n",
    "\t\t\n",
    "\tdef _yield_tokens(self):\n",
    "\t\tfor text_sample in self.text:\n",
    "\t\t\t# preprocess text\n",
    "\t\t\ttext_sample = normalizeString(text_sample)\n",
    "\t\t\tyield self.tokenizer(text_sample)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tprint(self.text[idx])\n",
    "\t\tinput_seq = text_to_indices(self.tokenizer, self.vocab, self.text[idx])\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\tprint(f'encoded: {input_seq}')\n",
    "\t\treturn input_seq, label\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "\treturn ''.join(\n",
    "\t\tc for c in unicodedata.normalize('NFD', s)\n",
    "\t\tif unicodedata.category(c) != 'Mn'\n",
    "\t)\n",
    "\n",
    "def normalizeString(s):\n",
    "\ts = unicodeToAscii(s.lower().strip())\n",
    "\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\treturn s\n",
    "\n",
    "def text_to_indices(tokenizer, vocab, text_sample):\n",
    "\ttokens = tokenizer(text_sample)\n",
    "\tindices = [vocab[token] for token in tokens]\n",
    "\tindices.append(EOS_token)\n",
    "\treturn torch.tensor(indices, dtype=torch.long).view(-1)\n",
    "\n",
    "def seq_to_tokens(seq, vocab):\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[idx] for idx in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TranslationDataset('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 65,   8,  29,  65,  80,   2, 105,  50,  52,  28,   6, 749, 457,  79,\n",
      "         25, 536,  47,  23,  26,  16,   1]) 1\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "src_sentence, label = trainset[100]\n",
    "print(src_sentence, label)\n",
    "print(type(src_sentence), type(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "\t\"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "\tArgs:\n",
    "\tlist_of_samples is a list of tuples (src_seq, tgt_label):\n",
    "\t\tsrc_seq is of shape (src_seq_length,)\n",
    "\t\ttgt_label is of shape (1,)\n",
    "\n",
    "\tReturns:\n",
    "\tsrc_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "\t\tThe sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "\t\tthe longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "\tsrc_seq_lengths: List of lengths of source sequences.\n",
    "\ttgt_labels of shape (batch_size, 1): Tensor of labels for each sequence.\n",
    "\t\"\"\"\n",
    "\t# YOUR CODE HERE\n",
    "\tsrc_seqs = [s[0] for s in list_of_samples]\n",
    "\ttgt_labels = torch.LongTensor([s[1] for s in list_of_samples])\n",
    "\tsrc_seq_lengths = [len(s) for s in src_seqs]\n",
    "\tsrc_seqs = pad_sequence(src_seqs, padding_value=PADDING_VALUE)\n",
    "\n",
    "\tsrc_seq_lengths, indices = torch.sort(torch.tensor(src_seq_lengths), descending=True)\n",
    "\tsrc_seqs = src_seqs[:, indices]\n",
    "\ttgt_labels = tgt_labels[indices]\n",
    "\n",
    "\treturn src_seqs, src_seq_lengths.tolist(), tgt_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), 1),\n",
    "        (torch.LongTensor([6, 7, 8]), 0),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert type(src_seq_lengths) == list, \"src_seq_lengths should be a list.\"\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences combined:\n",
      "tensor([[11,  6,  1],\n",
      "        [12,  7,  2],\n",
      "        [13,  8,  0],\n",
      "        [14,  0,  0]])\n",
      "[4, 3, 2]\n",
      "Target sequences combined:\n",
      "tensor([0, 1, 0])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate() function\n",
    "\n",
    "def test_collate_fn():\n",
    "    pairs = [\n",
    "        (torch.tensor([1, 2]), 0),\n",
    "        (torch.tensor([6, 7, 8]), 1),\n",
    "        (torch.tensor([11, 12, 13, 14]), 0),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert pad_src_seqs.shape == torch.Size([4, 3]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    print('Source sequences combined:')\n",
    "    print(pad_src_seqs)\n",
    "    expected = torch.tensor([\n",
    "      [11, 6, 1],\n",
    "      [12, 7, 2],\n",
    "      [13, 8, 0],\n",
    "      [14, 0, 0],\n",
    "    ])\n",
    "    assert (pad_src_seqs == expected).all(), \"pad_src_seqs does not match expected values\"\n",
    "\n",
    "    print(src_seq_lengths)\n",
    "    if isinstance(src_seq_lengths[0], torch.Size):\n",
    "        src_seq_lengths = sum((list(l) for l in src_seq_lengths), [])\n",
    "    else:\n",
    "        src_seq_lengths = [int(l) for l in src_seq_lengths]\n",
    "    assert src_seq_lengths == [4, 3, 2], f\"Bad src_seq_lengths: {src_seq_lengths}\"\n",
    "\n",
    "    print('Target sequences combined:')\n",
    "    print(pad_tgt_seqs)\n",
    "    expected = torch.tensor([\n",
    "      0, 1, 0\n",
    "    ])\n",
    "    assert (pad_tgt_seqs == expected).all(), \"pad_tgt_seqs0 does not match expected values\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We create custom DataLoader using the implemented collate function\n",
    "# # We are going to process 64 sequences at the same time (batch_size=64)\n",
    "# trainset = TranslationDataset('train_2024.csv')\n",
    "# trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data loader\n",
    "# for i, (src_seqs, src_seq_lengths, tgt_seqs) in enumerate(trainloader):\n",
    "#     print(f\"Batch {i} src_seqs:\")\n",
    "#     print(src_seqs)\n",
    "#     print(f'src_seqs.shape: {src_seqs.shape}')\n",
    "#     print(f\"Batch {i} src_seq_lengths:\")\n",
    "#     print(src_seq_lengths)\n",
    "#     print(f\"Batch {i} tgt_seqs:\")\n",
    "#     print(tgt_seqs)\n",
    "#     print(f'tgt_seqs.shape: {tgt_seqs.shape}')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self, src_dictionary_size, embed_size, hidden_size, dropout=0.2):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tsrc_dictionary_size: The number of words in the source dictionary.\n",
    "\t\tembed_size: The number of dimensions in the word embeddings.\n",
    "\t\thidden_size: The number of features in the hidden state of GRU.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(LSTM, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "\t\tself.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=1, batch_first=False, dropout=dropout, bidirectional=False)\n",
    "\t\tself.fc1 = nn.Linear(hidden_size, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t\n",
    "\tdef forward(self, pad_seqs, seq_lengths, hidden):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tpad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "\t\tseq_lengths: List of sequence lengths.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\toutputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "\t\t\"\"\"\n",
    "\t\t# YOUR CODE HERE\n",
    "\t\tembedded = self.embedding(pad_seqs)\n",
    "\t\tpacked = pack_padded_sequence(embedded, seq_lengths)\n",
    "\t\toutputs, hidden = self.lstm(packed, hidden)\n",
    "\t\toutputs, output_lengths = pad_packed_sequence(outputs, batch_first=False)\n",
    "\t\tlast_timesteps = torch.stack([outputs[length-1, i] for i, length in enumerate(output_lengths)]) # shape: (batch_size, hidden_size)\n",
    "\t\t# feed through the fully connected layer\n",
    "\t\toutputs = self.fc1(last_timesteps)\n",
    "\t\toutputs = self.dropout(outputs)\n",
    "\t\toutputs = self.relu(outputs)\n",
    "\t\toutputs = self.fc2(outputs)\n",
    "\t\toutputs = self.sigmoid(outputs)\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef init_hidden(self, batch_size=1, device='cpu'):\n",
    "\t\tnum_directions = 1\n",
    "\t\treturn (\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "def test_LSTM_shapes():\n",
    "    hidden_size = 3\n",
    "    lstm = LSTM(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = lstm.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])\n",
    "\n",
    "    outputs = lstm.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([batch_size, 1]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_LSTM_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, val_loader):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0\n",
    "\tcriterion = nn.BCELoss()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (src_seqs, src_seq_lengths, tgt_labels) in enumerate(val_loader):\n",
    "\t\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\t\thidden = model.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\t\toutputs = model(src_seqs, src_seq_lengths, hidden)\n",
    "\t\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\treturn total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "trainset = TranslationDataset('train_2024.csv')\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "valset = TranslationDataset('dev_2024.csv', vocab=trainset.vocab)\n",
    "valloader = DataLoader(dataset=trainset, batch_size=256, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM model\n",
    "hidden_size = embed_size = 128\n",
    "lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.6632, Time spent: 2.43s\n",
      "Epoch 1, iter 20: avg. loss = 0.6563, Time spent: 1.31s\n",
      "Epoch 1, iter 30: avg. loss = 0.6523, Time spent: 4.89s\n",
      "Epoch 1, iter 40: avg. loss = 0.6420, Time spent: 1.36s\n",
      "Epoch 1, iter 50: avg. loss = 0.6323, Time spent: 2.34s\n",
      "Epoch 1, iter 60: avg. loss = 0.6211, Time spent: 1.30s\n",
      "Epoch 1, iter 70: avg. loss = 0.6115, Time spent: 1.25s\n",
      "Epoch 1, iter 80: avg. loss = 0.6001, Time spent: 1.24s\n",
      "Epoch 1, iter 90: avg. loss = 0.5876, Time spent: 2.34s\n",
      "Epoch 1, iter 100: avg. loss = 0.5775, Time spent: 1.36s\n",
      "Epoch 1, iter 110: avg. loss = 0.5676, Time spent: 1.25s\n",
      "Epoch 1, iter 120: avg. loss = 0.5594, Time spent: 1.29s\n",
      "Epoch 1, iter 130: avg. loss = 0.5502, Time spent: 1.40s\n",
      "Epoch 1, iter 140: avg. loss = 0.5412, Time spent: 1.32s\n",
      "Epoch 1, iter 150: avg. loss = 0.5335, Time spent: 1.32s\n",
      "Epoch 1, val loss = 0.8439, train loss = 0.5287; Time spent: 276.71s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.4133, Time spent: 2.43s\n",
      "Epoch 2, iter 20: avg. loss = 0.4074, Time spent: 1.31s\n",
      "Epoch 2, iter 30: avg. loss = 0.4018, Time spent: 4.88s\n",
      "Epoch 2, iter 40: avg. loss = 0.3933, Time spent: 1.37s\n",
      "Epoch 2, iter 50: avg. loss = 0.3918, Time spent: 2.34s\n",
      "Epoch 2, iter 60: avg. loss = 0.3961, Time spent: 1.29s\n",
      "Epoch 2, iter 70: avg. loss = 0.3940, Time spent: 1.24s\n",
      "Epoch 2, iter 80: avg. loss = 0.3895, Time spent: 1.23s\n",
      "Epoch 2, iter 90: avg. loss = 0.3830, Time spent: 2.35s\n",
      "Epoch 2, iter 100: avg. loss = 0.3757, Time spent: 1.35s\n",
      "Epoch 2, iter 110: avg. loss = 0.3703, Time spent: 1.26s\n",
      "Epoch 2, iter 120: avg. loss = 0.3657, Time spent: 1.29s\n",
      "Epoch 2, iter 130: avg. loss = 0.3596, Time spent: 1.40s\n",
      "Epoch 2, iter 140: avg. loss = 0.3550, Time spent: 1.32s\n",
      "Epoch 2, iter 150: avg. loss = 0.3502, Time spent: 1.32s\n",
      "Epoch 2, val loss = 1.0709, train loss = 0.3474; Time spent: 275.97s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.2927, Time spent: 2.44s\n",
      "Epoch 3, iter 20: avg. loss = 0.3041, Time spent: 1.31s\n",
      "Epoch 3, iter 30: avg. loss = 0.3036, Time spent: 4.89s\n",
      "Epoch 3, iter 40: avg. loss = 0.2943, Time spent: 1.37s\n",
      "Epoch 3, iter 50: avg. loss = 0.2852, Time spent: 2.33s\n",
      "Epoch 3, iter 60: avg. loss = 0.2793, Time spent: 1.29s\n",
      "Epoch 3, iter 70: avg. loss = 0.2736, Time spent: 1.24s\n",
      "Epoch 3, iter 80: avg. loss = 0.2680, Time spent: 1.23s\n",
      "Epoch 3, iter 90: avg. loss = 0.2631, Time spent: 2.33s\n",
      "Epoch 3, iter 100: avg. loss = 0.2590, Time spent: 1.35s\n",
      "Epoch 3, iter 110: avg. loss = 0.2565, Time spent: 1.25s\n",
      "Epoch 3, iter 120: avg. loss = 0.2546, Time spent: 1.30s\n",
      "Epoch 3, iter 130: avg. loss = 0.2520, Time spent: 1.40s\n",
      "Epoch 3, iter 140: avg. loss = 0.2512, Time spent: 1.32s\n",
      "Epoch 3, iter 150: avg. loss = 0.2494, Time spent: 1.33s\n",
      "Epoch 3, val loss = 1.1671, train loss = 0.2478; Time spent: 275.79s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.2238, Time spent: 2.44s\n",
      "Epoch 4, iter 20: avg. loss = 0.2268, Time spent: 1.32s\n",
      "Epoch 4, iter 30: avg. loss = 0.2211, Time spent: 4.89s\n",
      "Epoch 4, iter 40: avg. loss = 0.2161, Time spent: 1.37s\n",
      "Epoch 4, iter 50: avg. loss = 0.2121, Time spent: 2.33s\n",
      "Epoch 4, iter 60: avg. loss = 0.2089, Time spent: 1.30s\n",
      "Epoch 4, iter 70: avg. loss = 0.2069, Time spent: 1.25s\n",
      "Epoch 4, iter 80: avg. loss = 0.2048, Time spent: 1.24s\n",
      "Epoch 4, iter 90: avg. loss = 0.2029, Time spent: 2.35s\n",
      "Epoch 4, iter 100: avg. loss = 0.2021, Time spent: 1.36s\n",
      "Epoch 4, iter 110: avg. loss = 0.2008, Time spent: 1.25s\n",
      "Epoch 4, iter 120: avg. loss = 0.1997, Time spent: 1.29s\n",
      "Epoch 4, iter 130: avg. loss = 0.1983, Time spent: 1.40s\n",
      "Epoch 4, iter 140: avg. loss = 0.1977, Time spent: 1.32s\n",
      "Epoch 4, iter 150: avg. loss = 0.1968, Time spent: 1.32s\n",
      "Epoch 4, val loss = 1.3407, train loss = 0.1954; Time spent: 275.97s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.1841, Time spent: 2.43s\n",
      "Epoch 5, iter 20: avg. loss = 0.1876, Time spent: 1.33s\n",
      "Epoch 5, iter 30: avg. loss = 0.1847, Time spent: 4.88s\n",
      "Epoch 5, iter 40: avg. loss = 0.1833, Time spent: 1.37s\n",
      "Epoch 5, iter 50: avg. loss = 0.1800, Time spent: 2.33s\n",
      "Epoch 5, iter 60: avg. loss = 0.1768, Time spent: 1.29s\n",
      "Epoch 5, iter 70: avg. loss = 0.1753, Time spent: 1.25s\n",
      "Epoch 5, iter 80: avg. loss = 0.1741, Time spent: 1.25s\n",
      "Epoch 5, iter 90: avg. loss = 0.1727, Time spent: 2.34s\n",
      "Epoch 5, iter 100: avg. loss = 0.1721, Time spent: 1.36s\n",
      "Epoch 5, iter 110: avg. loss = 0.1717, Time spent: 1.25s\n",
      "Epoch 5, iter 120: avg. loss = 0.1711, Time spent: 1.30s\n",
      "Epoch 5, iter 130: avg. loss = 0.1705, Time spent: 1.40s\n",
      "Epoch 5, iter 140: avg. loss = 0.1702, Time spent: 1.32s\n",
      "Epoch 5, iter 150: avg. loss = 0.1697, Time spent: 1.31s\n",
      "Epoch 5, val loss = 1.4495, train loss = 0.1685; Time spent: 275.96s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 6, iter 10: avg. loss = 0.1578, Time spent: 2.44s\n",
      "Epoch 6, iter 20: avg. loss = 0.1625, Time spent: 1.32s\n",
      "Epoch 6, iter 30: avg. loss = 0.1600, Time spent: 4.89s\n",
      "Epoch 6, iter 40: avg. loss = 0.1578, Time spent: 1.37s\n",
      "Epoch 6, iter 50: avg. loss = 0.1555, Time spent: 2.33s\n",
      "Epoch 6, iter 60: avg. loss = 0.1532, Time spent: 1.29s\n",
      "Epoch 6, iter 70: avg. loss = 0.1518, Time spent: 1.25s\n",
      "Epoch 6, iter 80: avg. loss = 0.1510, Time spent: 1.24s\n",
      "Epoch 6, iter 90: avg. loss = 0.1503, Time spent: 2.33s\n",
      "Epoch 6, iter 100: avg. loss = 0.1502, Time spent: 1.36s\n",
      "Epoch 6, iter 110: avg. loss = 0.1497, Time spent: 1.25s\n",
      "Epoch 6, iter 120: avg. loss = 0.1497, Time spent: 1.30s\n",
      "Epoch 6, iter 130: avg. loss = 0.1491, Time spent: 1.40s\n",
      "Epoch 6, iter 140: avg. loss = 0.1487, Time spent: 1.32s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\aalto-snlp-project-spring-2024\\rnn.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X41sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m lstm(src_seqs, src_seq_lengths, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), tgt_labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X41sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X41sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X41sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm, valloader)\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.5103, Time spent: 4.86s\n",
      "Epoch 1, iter 20: avg. loss = 0.5001, Time spent: 2.63s\n",
      "Epoch 1, iter 30: avg. loss = 0.4859, Time spent: 9.74s\n",
      "Epoch 1, iter 40: avg. loss = 0.4745, Time spent: 2.72s\n",
      "Epoch 1, iter 50: avg. loss = 0.4682, Time spent: 4.65s\n",
      "Epoch 1, iter 60: avg. loss = 0.4589, Time spent: 2.58s\n",
      "Epoch 1, iter 70: avg. loss = 0.4506, Time spent: 2.48s\n",
      "Epoch 1, iter 80: avg. loss = 0.4419, Time spent: 2.46s\n",
      "Epoch 1, iter 90: avg. loss = 0.4335, Time spent: 4.71s\n",
      "Epoch 1, iter 100: avg. loss = 0.4273, Time spent: 2.71s\n",
      "Epoch 1, iter 110: avg. loss = 0.4230, Time spent: 2.49s\n",
      "Epoch 1, iter 120: avg. loss = 0.4187, Time spent: 2.59s\n",
      "Epoch 1, iter 130: avg. loss = 0.4133, Time spent: 2.78s\n",
      "Epoch 1, iter 140: avg. loss = 0.4078, Time spent: 2.62s\n",
      "Epoch 1, iter 150: avg. loss = 0.4018, Time spent: 2.61s\n",
      "Epoch 1, val loss = 0.9767, train loss = 0.3986; Time spent: 543.38s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.2934, Time spent: 4.84s\n",
      "Epoch 2, iter 20: avg. loss = 0.2960, Time spent: 2.61s\n",
      "Epoch 2, iter 30: avg. loss = 0.2875, Time spent: 9.76s\n",
      "Epoch 2, iter 40: avg. loss = 0.2819, Time spent: 2.73s\n",
      "Epoch 2, iter 50: avg. loss = 0.2751, Time spent: 4.66s\n",
      "Epoch 2, iter 60: avg. loss = 0.2692, Time spent: 2.58s\n",
      "Epoch 2, iter 70: avg. loss = 0.2657, Time spent: 2.49s\n",
      "Epoch 2, iter 80: avg. loss = 0.2617, Time spent: 2.49s\n",
      "Epoch 2, iter 90: avg. loss = 0.2578, Time spent: 4.68s\n",
      "Epoch 2, iter 100: avg. loss = 0.2552, Time spent: 2.72s\n",
      "Epoch 2, iter 110: avg. loss = 0.2532, Time spent: 2.50s\n",
      "Epoch 2, iter 120: avg. loss = 0.2521, Time spent: 2.61s\n",
      "Epoch 2, iter 130: avg. loss = 0.2504, Time spent: 2.79s\n",
      "Epoch 2, iter 140: avg. loss = 0.2482, Time spent: 2.62s\n",
      "Epoch 2, iter 150: avg. loss = 0.2462, Time spent: 2.62s\n",
      "Epoch 2, val loss = 1.2228, train loss = 0.2442; Time spent: 545.64s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.2082, Time spent: 4.84s\n",
      "Epoch 3, iter 20: avg. loss = 0.2090, Time spent: 2.61s\n",
      "Epoch 3, iter 30: avg. loss = 0.2037, Time spent: 9.75s\n",
      "Epoch 3, iter 40: avg. loss = 0.2030, Time spent: 2.72s\n",
      "Epoch 3, iter 50: avg. loss = 0.1999, Time spent: 4.65s\n",
      "Epoch 3, iter 60: avg. loss = 0.1971, Time spent: 2.56s\n",
      "Epoch 3, iter 70: avg. loss = 0.1958, Time spent: 2.47s\n",
      "Epoch 3, iter 80: avg. loss = 0.1929, Time spent: 2.46s\n",
      "Epoch 3, iter 90: avg. loss = 0.1905, Time spent: 4.67s\n",
      "Epoch 3, iter 100: avg. loss = 0.1901, Time spent: 2.70s\n",
      "Epoch 3, iter 110: avg. loss = 0.1899, Time spent: 2.47s\n",
      "Epoch 3, iter 120: avg. loss = 0.1899, Time spent: 2.58s\n",
      "Epoch 3, iter 130: avg. loss = 0.1889, Time spent: 2.77s\n",
      "Epoch 3, iter 140: avg. loss = 0.1881, Time spent: 2.62s\n",
      "Epoch 3, iter 150: avg. loss = 0.1873, Time spent: 2.62s\n",
      "Epoch 3, val loss = 1.3591, train loss = 0.1861; Time spent: 542.05s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.1720, Time spent: 4.86s\n",
      "Epoch 4, iter 20: avg. loss = 0.1744, Time spent: 2.62s\n",
      "Epoch 4, iter 30: avg. loss = 0.1718, Time spent: 9.76s\n",
      "Epoch 4, iter 40: avg. loss = 0.1704, Time spent: 2.72s\n",
      "Epoch 4, iter 50: avg. loss = 0.1672, Time spent: 4.65s\n",
      "Epoch 4, iter 60: avg. loss = 0.1654, Time spent: 2.57s\n",
      "Epoch 4, iter 70: avg. loss = 0.1640, Time spent: 2.47s\n",
      "Epoch 4, iter 80: avg. loss = 0.1611, Time spent: 2.46s\n",
      "Epoch 4, iter 90: avg. loss = 0.1587, Time spent: 4.68s\n",
      "Epoch 4, iter 100: avg. loss = 0.1579, Time spent: 2.70s\n",
      "Epoch 4, iter 110: avg. loss = 0.1573, Time spent: 2.49s\n",
      "Epoch 4, iter 120: avg. loss = 0.1573, Time spent: 2.57s\n",
      "Epoch 4, iter 130: avg. loss = 0.1565, Time spent: 2.77s\n",
      "Epoch 4, iter 140: avg. loss = 0.1556, Time spent: 2.62s\n",
      "Epoch 4, iter 150: avg. loss = 0.1549, Time spent: 2.62s\n",
      "Epoch 4, val loss = 1.4408, train loss = 0.1537; Time spent: 541.74s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.1423, Time spent: 4.85s\n",
      "Epoch 5, iter 20: avg. loss = 0.1441, Time spent: 2.62s\n",
      "Epoch 5, iter 30: avg. loss = 0.1426, Time spent: 9.75s\n",
      "Epoch 5, iter 40: avg. loss = 0.1421, Time spent: 2.72s\n",
      "Epoch 5, iter 50: avg. loss = 0.1407, Time spent: 4.66s\n",
      "Epoch 5, iter 60: avg. loss = 0.1393, Time spent: 2.57s\n",
      "Epoch 5, iter 70: avg. loss = 0.1392, Time spent: 2.47s\n",
      "Epoch 5, iter 80: avg. loss = 0.1374, Time spent: 2.45s\n",
      "Epoch 5, iter 90: avg. loss = 0.1351, Time spent: 4.66s\n",
      "Epoch 5, iter 100: avg. loss = 0.1354, Time spent: 2.69s\n",
      "Epoch 5, iter 110: avg. loss = 0.1349, Time spent: 2.49s\n",
      "Epoch 5, iter 120: avg. loss = 0.1347, Time spent: 2.57s\n",
      "Epoch 5, iter 130: avg. loss = 0.1345, Time spent: 2.78s\n",
      "Epoch 5, iter 140: avg. loss = 0.1334, Time spent: 2.62s\n",
      "Epoch 5, iter 150: avg. loss = 0.1328, Time spent: 2.63s\n",
      "Epoch 5, val loss = 1.4700, train loss = 0.1316; Time spent: 541.86s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 6, iter 10: avg. loss = 0.1233, Time spent: 4.85s\n",
      "Epoch 6, iter 20: avg. loss = 0.1237, Time spent: 2.62s\n",
      "Epoch 6, iter 30: avg. loss = 0.1230, Time spent: 9.76s\n",
      "Epoch 6, iter 40: avg. loss = 0.1217, Time spent: 2.71s\n",
      "Epoch 6, iter 50: avg. loss = 0.1220, Time spent: 4.65s\n",
      "Epoch 6, iter 60: avg. loss = 0.1228, Time spent: 2.56s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\aalto-snlp-project-spring-2024\\rnn.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m lstm(src_seqs, src_seq_lengths, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), tgt_labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm, valloader)\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(lstm, pad_src_seqs, src_seq_lengths):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "\n",
    "    Args:\n",
    "    lstm (LSTM): Trained lstm.\n",
    "    pad_src_seqs of shape (max_src_seq_length, batch_size): Padded source sequences.\n",
    "    src_seq_lengths: List of source sequence lengths.\n",
    "\n",
    "    Returns:\n",
    "    out_seqs of shape (batch_size, 1): LongTensor of word indices of the output sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        pad_src_seqs = pad_src_seqs.to(device)\n",
    "        lstm_hidden = lstm.init_hidden(pad_src_seqs.shape[1], device)\n",
    "        outputs = lstm(pad_src_seqs, src_seq_lengths, lstm_hidden)\n",
    "        out_seqs = outputs > 0.5\n",
    "        return out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    pad_src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths=[4, 2])\n",
    "    assert out_seqs.shape == torch.Size([2, 1]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify training data:\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-----------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m pad_src_seqs, src_seq_lengths, pad_tgt_seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(valloader))\n\u001b[0;32m----> 5\u001b[0m out_seqs \u001b[38;5;241m=\u001b[39m \u001b[43mclassify\u001b[49m(lstm, pad_src_seqs, src_seq_lengths)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSRC:\u001b[39m\u001b[38;5;124m'\u001b[39m, seq_to_tokens(pad_src_seqs[:,i], trainset\u001b[38;5;241m.\u001b[39mvocab))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classify' is not defined"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Classify training data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs = next(iter(valloader))\n",
    "out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_tokens(pad_src_seqs[:,i], trainset.vocab))\n",
    "    print('TGT:', pad_tgt_seqs[i].item())\n",
    "    print('OUT:', out_seqs[i].item())\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
