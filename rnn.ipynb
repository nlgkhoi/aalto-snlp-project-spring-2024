{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using MPS!\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Using the CPU!\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Except that Desmond played first base last nig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What i find funny is the loyalty and blindness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Read the article  not just the headline &amp; you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Speaking of a horses backside  is that where y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Michael Barone- gee are you dumb.  No other wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label\n",
       "0   0  Except that Desmond played first base last nig...      0\n",
       "1   1  What i find funny is the loyalty and blindness...      0\n",
       "2   2  Read the article  not just the headline & you ...      0\n",
       "3   3  Speaking of a horses backside  is that where y...      1\n",
       "4   4  Michael Barone- gee are you dumb.  No other wo...      1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_2024.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "EOS_token = 1\n",
    "lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "\tdef __init__(self, csv_path, dataset_type='train', vocab=None):\n",
    "\t\tdf = pd.read_csv(csv_path, quoting=3)\n",
    "\t\tprint(f'len df: {len(df)}')\n",
    "\t\tif dataset_type in ['train', 'val']:\n",
    "\t\t\tself.text, self.labels = zip(*[(text, label) for text, label in zip(df['text'], df['label'])])\n",
    "\t\telse:\n",
    "\t\t\tself.text = df['text'].tolist()\n",
    "\t\t\tself.labels = [0 for _ in range(len(self.text))]\n",
    "\t\tself.ids = df['id'].tolist()\n",
    "\t\tself.dataset_type = dataset_type\n",
    "\t\tself.tokenizer = get_tokenizer('basic_english')\n",
    "\t\tself._preprocess(vocab)\n",
    "\n",
    "\tdef _preprocess(self, vocab):\n",
    "\t\t# preprocess text\n",
    "\t\tself.text = [self._preprocess_sentence(text) for text in self.text]\n",
    "\n",
    "\t\tif vocab is None:\n",
    "\t\t\tself.vocab = build_vocab_from_iterator(self._yield_tokens(), specials=[\"<unk>\"])\n",
    "\t\t\tself.vocab.set_default_index(self.vocab['<unk>'])\n",
    "\t\t\tself.vocab.insert_token('<eos>', EOS_token)  # Insert <eos> token with index 1\n",
    "\t\telse:\n",
    "\t\t\tself.vocab = vocab\n",
    "\t\t\t\n",
    "\t\tself.vocab_size = len(self.vocab)\n",
    "\t\n",
    "\tdef _preprocess_sentence(self, sentence):\n",
    "\t\tsentence = normalizeString(sentence)\n",
    "\t\tsentence = self.tokenizer(sentence)\n",
    "\t\tsentence = lemmaString(sentence)\n",
    "\t\treturn sentence\n",
    "\n",
    "\tdef _yield_tokens(self):\n",
    "\t\tfor text_sample in self.text:\n",
    "\t\t\tyield text_sample\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tinput_seq = text_to_indices(self.vocab, self.text[idx])\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\treturn input_seq, label, self.ids[idx]\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "\treturn ''.join(\n",
    "\t\tc for c in unicodedata.normalize('NFD', s)\n",
    "\t\tif unicodedata.category(c) != 'Mn'\n",
    "\t)\n",
    "\n",
    "def normalizeString(s):\n",
    "\ts = unicodeToAscii(s.lower().strip())\n",
    "\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\treturn s\n",
    "\n",
    "def lemmaString(tokens):\n",
    "\treturn [token.lemma_ for token in lemmatizer(' '.join(tokens))]\n",
    "\n",
    "def text_to_indices(vocab, tokens):\n",
    "\tindices = [vocab[token] for token in tokens]\n",
    "\tindices.append(EOS_token)\n",
    "\treturn torch.tensor(indices, dtype=torch.long).view(-1)\n",
    "\n",
    "def seq_to_tokens(seq, vocab):\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[idx] for idx in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df: 10\n"
     ]
    }
   ],
   "source": [
    "trainset = TranslationDataset('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['except', 'that', 'desmond', 'play', 'first', 'base', 'last', 'night', '.', 'tapia', 'be', 'in', 'lf', 'and', 'reynolds', 'have', 'a', 'night', 'off', '.', '<eos>']\n",
      "tensor([109,  18,  95, 157, 110,  78, 128,  44,   2, 189,   3,  17, 130,   7,\n",
      "        172,  16,  25,  44, 145,   2,   1]) 0\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "src_sentence, label, id_ = trainset[0]\n",
    "print(seq_to_tokens(src_sentence, trainset.vocab))\n",
    "print(src_sentence, label)\n",
    "print(type(src_sentence), type(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "\t\"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "\tArgs:\n",
    "\tlist_of_samples is a list of tuples (src_seq, tgt_label, id):\n",
    "\t\tsrc_seq is of shape (src_seq_length,)\n",
    "\t\ttgt_label is of shape (1,)\n",
    "\t\tid is an int\n",
    "\n",
    "\tReturns:\n",
    "\tsrc_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "\t\tThe sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "\t\tthe longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "\tsrc_seq_lengths: List of lengths of source sequences.\n",
    "\ttgt_labels of shape (batch_size, 1): Tensor of labels for each sequence.\n",
    "\t\"\"\"\n",
    "\t# YOUR CODE HERE\n",
    "\tsrc_seqs = [s[0] for s in list_of_samples]\n",
    "\ttgt_labels = torch.LongTensor([s[1] for s in list_of_samples])\n",
    "\tsrc_seq_lengths = [len(s) for s in src_seqs]\n",
    "\tids = [s[2] for s in list_of_samples]\n",
    "\tsrc_seqs = pad_sequence(src_seqs, padding_value=PADDING_VALUE)\n",
    "\n",
    "\tsrc_seq_lengths, indices = torch.sort(torch.tensor(src_seq_lengths), descending=True)\n",
    "\tsrc_seqs = src_seqs[:, indices]\n",
    "\ttgt_labels = tgt_labels[indices]\n",
    "\tids = [ids[i] for i in indices]\n",
    "\n",
    "\treturn src_seqs, src_seq_lengths.tolist(), tgt_labels, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), 1, 0),\n",
    "        (torch.LongTensor([6, 7, 8]), 0, 1),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = collate(pairs)\n",
    "    assert type(src_seq_lengths) == list, \"src_seq_lengths should be a list.\"\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences combined:\n",
      "tensor([[11,  6,  1],\n",
      "        [12,  7,  2],\n",
      "        [13,  8,  0],\n",
      "        [14,  0,  0]])\n",
      "[4, 3, 2]\n",
      "Target sequences combined:\n",
      "tensor([0, 1, 0])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate() function\n",
    "\n",
    "def test_collate_fn():\n",
    "    pairs = [\n",
    "        (torch.tensor([1, 2]), 0, 0),\n",
    "        (torch.tensor([6, 7, 8]), 1, 1),\n",
    "        (torch.tensor([11, 12, 13, 14]), 0, 2),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = collate(pairs)\n",
    "    assert pad_src_seqs.shape == torch.Size([4, 3]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    print('Source sequences combined:')\n",
    "    print(pad_src_seqs)\n",
    "    expected = torch.tensor([\n",
    "      [11, 6, 1],\n",
    "      [12, 7, 2],\n",
    "      [13, 8, 0],\n",
    "      [14, 0, 0],\n",
    "    ])\n",
    "    assert (pad_src_seqs == expected).all(), \"pad_src_seqs does not match expected values\"\n",
    "\n",
    "    print(src_seq_lengths)\n",
    "    if isinstance(src_seq_lengths[0], torch.Size):\n",
    "        src_seq_lengths = sum((list(l) for l in src_seq_lengths), [])\n",
    "    else:\n",
    "        src_seq_lengths = [int(l) for l in src_seq_lengths]\n",
    "    assert src_seq_lengths == [4, 3, 2], f\"Bad src_seq_lengths: {src_seq_lengths}\"\n",
    "\n",
    "    print('Target sequences combined:')\n",
    "    print(pad_tgt_seqs)\n",
    "    expected = torch.tensor([\n",
    "      0, 1, 0\n",
    "    ])\n",
    "    assert (pad_tgt_seqs == expected).all(), \"pad_tgt_seqs0 does not match expected values\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We create custom DataLoader using the implemented collate function\n",
    "# # We are going to process 64 sequences at the same time (batch_size=64)\n",
    "# trainset = TranslationDataset('train_2024.csv')\n",
    "# trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data loader\n",
    "# for i, (src_seqs, src_seq_lengths, tgt_seqs, ids) in enumerate(trainloader):\n",
    "#     print(f\"Batch {i} src_seqs:\")\n",
    "#     print(src_seqs)\n",
    "#     print(f'src_seqs.shape: {src_seqs.shape}')\n",
    "#     print(f\"Batch {i} src_seq_lengths:\")\n",
    "#     print(src_seq_lengths)\n",
    "#     print(f\"Batch {i} tgt_seqs:\")\n",
    "#     print(tgt_seqs)\n",
    "#     print(f'tgt_seqs.shape: {tgt_seqs.shape}')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self, src_dictionary_size, embed_size, hidden_size, dropout=0.2, glove_embeddings=None):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tsrc_dictionary_size: The number of words in the source dictionary.\n",
    "\t\tembed_size: The number of dimensions in the word embeddings.\n",
    "\t\thidden_size: The number of features in the hidden state of GRU.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(LSTM, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "\t\tif glove_embeddings is not None:\n",
    "\t\t\tself.load_glove_embeddings(glove_embeddings, embed_size)\n",
    "\t\n",
    "\t\tself.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=1, batch_first=False, dropout=dropout, bidirectional=False)\n",
    "\t\tself.fc1 = nn.Linear(hidden_size, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef load_glove_embeddings(self, glove_embeddings, embed_size):\n",
    "\t\t\"\"\"Initialize the embedding layer with GloVe embeddings.\"\"\"\n",
    "\t\tweights_matrix = torch.zeros((self.embedding.num_embeddings, embed_size))\n",
    "\n",
    "\t\tfor i, word in enumerate(glove_embeddings):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tweights_matrix[i] = torch.FloatTensor(glove_embeddings[word])\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(e)\n",
    "\t\t\t\tprint(torch.FloatTensor(glove_embeddings[word]).size())\n",
    "\t\t\t\tprint(f'word: {word}, i: {i}')\n",
    "\n",
    "\t\tself.embedding.load_state_dict({'weight': weights_matrix})\n",
    "\n",
    "\tdef forward(self, pad_seqs, seq_lengths, hidden):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tpad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "\t\tseq_lengths: List of sequence lengths.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\toutputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "\t\t\"\"\"\n",
    "\t\t# YOUR CODE HERE\n",
    "\t\tembedded = self.embedding(pad_seqs) # shape: (max_seq_length, batch_size, embed_size)\n",
    "\t\tpacked = pack_padded_sequence(embedded, seq_lengths)\n",
    "\t\toutputs, hidden = self.lstm(packed, hidden) \n",
    "\t\toutputs, output_lengths = pad_packed_sequence(outputs, batch_first=False) # shape: (max_seq_length, batch_size, hidden_size)\n",
    "\t\tlast_timesteps = torch.stack([outputs[length-1, i] for i, length in enumerate(output_lengths)]) # shape: (batch_size, hidden_size)\n",
    "\t\t# feed through the fully connected layer\n",
    "\t\toutputs = self.fc1(last_timesteps)\n",
    "\t\toutputs = self.dropout(outputs)\n",
    "\t\toutputs = self.relu(outputs)\n",
    "\t\toutputs = self.fc2(outputs)\n",
    "\t\toutputs = self.sigmoid(outputs)\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef init_hidden(self, batch_size=1, device='cpu'):\n",
    "\t\tnum_directions = 1\n",
    "\t\treturn (\n",
    "\t\t\ttorch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "\t\t\ttorch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, src_dictionary_size, embed_size, hidden_size, num_filters=100, kernel_sizes= [3, 4, 5], dropout=0.2, glove_embeddings=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        src_dictionary_size: The number of words in the source dictionary.\n",
    "        embed_size: The number of dimensions in the word embeddings.\n",
    "        hidden_size: The number of features in the hidden state of GRU.\n",
    "        \"\"\"\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "        if glove_embeddings is not None:\n",
    "            self.load_glove_embeddings(glove_embeddings, embed_size)\n",
    "\n",
    "        # CNN layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_size, out_channels=num_filters, kernel_size=k, stride=1, padding=\"same\")\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.conv_output_size = num_filters * len(kernel_sizes)\n",
    "    \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=self.conv_output_size, hidden_size=hidden_size, num_layers=1, batch_first=False, dropout=dropout, bidirectional=False)\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def load_glove_embeddings(self, glove_embeddings, embed_size):\n",
    "        \"\"\"Initialize the embedding layer with GloVe embeddings.\"\"\"\n",
    "        weights_matrix = torch.zeros((self.embedding.num_embeddings, embed_size))\n",
    "\n",
    "        for i, word in enumerate(glove_embeddings):\n",
    "            try:\n",
    "                weights_matrix[i] = torch.FloatTensor(glove_embeddings[word])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(torch.FloatTensor(glove_embeddings[word]).size())\n",
    "                print(f'word: {word}, i: {i}')\n",
    "\n",
    "        self.embedding.load_state_dict({'weight': weights_matrix})\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        pad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "        seq_lengths: List of sequence lengths.\n",
    "        hidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "        Returns:\n",
    "        outputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "        hidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.embedding(pad_seqs) # shape: (max_seq_length, batch_size, embed_size)\n",
    "        # reshape before feeding through the CNN layers\n",
    "        embedded = embedded.permute(1, 2, 0)  # shape: (batch_size, embed_size, max_seq_length)\n",
    "        \n",
    "        # Apply CNN and ReLU activation\n",
    "        cnn_out = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        # for i, out in enumerate(cnn_out):\n",
    "        #     print(f'cnn_out[{i}] size: {out.size()}') # shape: (batch_size, num_filters, max_seq_length)\n",
    "        cnn_out = torch.cat(cnn_out, 1)  # concatenate along the channel dimension shape: (batch_size, num_filters * len(kernel_sizes), max_seq_length)\n",
    "        cnn_out = cnn_out.permute(2, 0, 1)  # shape: (max_seq_length, batch_size, conv_output_size)\n",
    "        embedded = cnn_out\n",
    "        # feed through the LSTM layer\n",
    "        packed = pack_padded_sequence(embedded, seq_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden) \n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=False) # shape: (max_seq_length, batch_size, hidden_size)\n",
    "        last_timesteps = torch.stack([outputs[length-1, i] for i, length in enumerate(output_lengths)]) # shape: (batch_size, hidden_size)\n",
    "        # feed through the fully connected layer\n",
    "        outputs = self.fc1(last_timesteps)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.fc2(outputs)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, batch_size=1, device='cpu'):\n",
    "        num_directions = 1\n",
    "        return (\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "def test_LSTM_shapes():\n",
    "    hidden_size = 3\n",
    "    lstm = LSTM(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = lstm.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])\n",
    "\n",
    "    outputs = lstm.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([batch_size, 1]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_LSTM_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, val_loader):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0\n",
    "\tcriterion = nn.BCELoss()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(val_loader):\n",
    "\t\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\t\thidden = model.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\t\toutputs = model(src_seqs, src_seq_lengths, hidden)\n",
    "\t\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\treturn total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 640 sequences at the same time (batch_size=640)\n",
    "# load vocab\n",
    "# vocab = torch.load('vocab.pth')\n",
    "# vocab = None\n",
    "# trainset = TranslationDataset('train_2024.csv', vocab=vocab)\n",
    "trainset = torch.load('datasets/lstm/trainset.pth')\n",
    "print(len(trainset.text))\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    }
   ],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "# valset = TranslationDataset('dev_2024.csv', vocab=trainset.vocab)\n",
    "valset = torch.load('datasets/lstm/valset.pth')\n",
    "print(len(valset.text))\n",
    "valloader = DataLoader(dataset=valset, batch_size=256, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trainset\n",
    "# torch.save(trainset, 'trainset.pth')\n",
    "\n",
    "# save valset\n",
    "# torch.save(valset, 'valset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          patience (int):    Maximum number of epochs with unsuccessful updates.\n",
    "          tolerance (float): We assume that the update is unsuccessful if the validation error is larger\n",
    "                              than the best validation error so far plus this tolerance.\n",
    "        \"\"\"\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    \n",
    "    def stop_criterion(self, val_errors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          val_errors (iterable): Validation errors after every update during training.\n",
    "        \n",
    "        Returns: True if training should be stopped: when the validation error is larger than the best\n",
    "                  validation error obtained so far (with given tolearance) for patience epochs (number of consecutive epochs for which the criterion is satisfied).\n",
    "                 \n",
    "                 Otherwise, False.\n",
    "        \"\"\"\n",
    "        if len(val_errors) <= self.patience:\n",
    "            return False\n",
    "\n",
    "        min_val_error = min(val_errors)\n",
    "        val_errors = np.array(val_errors[-self.patience:])\n",
    "        return all(val_errors > min_val_error + self.tolerance)\n",
    "\n",
    "early_stop = EarlyStopping(tolerance=0.001, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM model\n",
    "hidden_size = embed_size = 256\n",
    "lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "# Load pretrained LSTM\n",
    "# lstm.load_state_dict(torch.load('lstm_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue training\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.2052, Time spent: 4.75s\n",
      "Epoch 1, iter 20: avg. loss = 0.2008, Time spent: 5.09s\n",
      "Epoch 1, iter 30: avg. loss = 0.1968, Time spent: 8.31s\n",
      "Epoch 1, iter 40: avg. loss = 0.1959, Time spent: 5.94s\n",
      "Epoch 1, iter 50: avg. loss = 0.1912, Time spent: 5.70s\n",
      "Epoch 1, iter 60: avg. loss = 0.1879, Time spent: 5.42s\n",
      "Epoch 1, iter 70: avg. loss = 0.1865, Time spent: 5.26s\n",
      "Epoch 1, iter 80: avg. loss = 0.1839, Time spent: 5.56s\n",
      "Epoch 1, iter 90: avg. loss = 0.1822, Time spent: 10.76s\n",
      "Epoch 1, iter 100: avg. loss = 0.1820, Time spent: 5.45s\n",
      "Epoch 1, iter 110: avg. loss = 0.1828, Time spent: 5.78s\n",
      "Epoch 1, iter 120: avg. loss = 0.1833, Time spent: 5.45s\n",
      "Epoch 1, iter 130: avg. loss = 0.1823, Time spent: 6.67s\n",
      "Epoch 1, iter 140: avg. loss = 0.1820, Time spent: 4.60s\n",
      "Epoch 1, iter 150: avg. loss = 0.1830, Time spent: 4.72s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.2162\n",
      "Epoch 1, val loss = 0.2162, train loss = 0.1829; Time spent: 891.66s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.1783, Time spent: 5.31s\n",
      "Epoch 2, iter 20: avg. loss = 0.1817, Time spent: 4.65s\n",
      "Epoch 2, iter 30: avg. loss = 0.1790, Time spent: 8.16s\n",
      "Epoch 2, iter 40: avg. loss = 0.1756, Time spent: 5.39s\n",
      "Epoch 2, iter 50: avg. loss = 0.1711, Time spent: 4.87s\n",
      "Epoch 2, iter 60: avg. loss = 0.1681, Time spent: 5.34s\n",
      "Epoch 2, iter 70: avg. loss = 0.1674, Time spent: 5.24s\n",
      "Epoch 2, iter 80: avg. loss = 0.1660, Time spent: 5.68s\n",
      "Epoch 2, iter 90: avg. loss = 0.1644, Time spent: 10.02s\n",
      "Epoch 2, iter 100: avg. loss = 0.1642, Time spent: 5.02s\n",
      "Epoch 2, iter 110: avg. loss = 0.1651, Time spent: 6.07s\n",
      "Epoch 2, iter 120: avg. loss = 0.1664, Time spent: 5.07s\n",
      "Epoch 2, iter 130: avg. loss = 0.1660, Time spent: 5.74s\n",
      "Epoch 2, iter 140: avg. loss = 0.1655, Time spent: 4.67s\n",
      "Epoch 2, iter 150: avg. loss = 0.1654, Time spent: 5.71s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.2002\n",
      "Epoch 2, val loss = 0.2002, train loss = 0.1652; Time spent: 881.16s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.1646, Time spent: 4.70s\n",
      "Epoch 3, iter 20: avg. loss = 0.1663, Time spent: 5.74s\n",
      "Epoch 3, iter 30: avg. loss = 0.1643, Time spent: 8.70s\n",
      "Epoch 3, iter 40: avg. loss = 0.1616, Time spent: 6.00s\n",
      "Epoch 3, iter 50: avg. loss = 0.1574, Time spent: 5.02s\n",
      "Epoch 3, iter 60: avg. loss = 0.1543, Time spent: 5.72s\n",
      "Epoch 3, iter 70: avg. loss = 0.1527, Time spent: 4.79s\n",
      "Epoch 3, iter 80: avg. loss = 0.1511, Time spent: 5.48s\n",
      "Epoch 3, iter 90: avg. loss = 0.1497, Time spent: 8.52s\n",
      "Epoch 3, iter 100: avg. loss = 0.1496, Time spent: 4.83s\n",
      "Epoch 3, iter 110: avg. loss = 0.1501, Time spent: 4.88s\n",
      "Epoch 3, iter 120: avg. loss = 0.1517, Time spent: 4.72s\n",
      "Epoch 3, iter 130: avg. loss = 0.1548, Time spent: 5.41s\n",
      "Epoch 3, iter 140: avg. loss = 0.1566, Time spent: 4.68s\n",
      "Epoch 3, iter 150: avg. loss = 0.1587, Time spent: 4.87s\n",
      "Epoch 3, val loss = 0.2138, train loss = 0.1594; Time spent: 829.56s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.1786, Time spent: 4.73s\n",
      "Epoch 4, iter 20: avg. loss = 0.1773, Time spent: 5.00s\n",
      "Epoch 4, iter 30: avg. loss = 0.1742, Time spent: 7.09s\n",
      "Epoch 4, iter 40: avg. loss = 0.1702, Time spent: 4.48s\n",
      "Epoch 4, iter 50: avg. loss = 0.1651, Time spent: 5.37s\n",
      "Epoch 4, iter 60: avg. loss = 0.1605, Time spent: 4.32s\n",
      "Epoch 4, iter 70: avg. loss = 0.1582, Time spent: 4.10s\n",
      "Epoch 4, iter 80: avg. loss = 0.1554, Time spent: 4.08s\n",
      "Epoch 4, iter 90: avg. loss = 0.1537, Time spent: 7.58s\n",
      "Epoch 4, iter 100: avg. loss = 0.1529, Time spent: 4.31s\n",
      "Epoch 4, iter 110: avg. loss = 0.1562, Time spent: 4.86s\n",
      "Epoch 4, iter 120: avg. loss = 0.1587, Time spent: 6.38s\n",
      "Epoch 4, iter 130: avg. loss = 0.1596, Time spent: 6.91s\n",
      "Epoch 4, iter 140: avg. loss = 0.1596, Time spent: 5.87s\n",
      "Epoch 4, iter 150: avg. loss = 0.1598, Time spent: 4.71s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.1979\n",
      "Epoch 4, val loss = 0.1979, train loss = 0.1591; Time spent: 774.32s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.1544, Time spent: 4.45s\n",
      "Epoch 5, iter 20: avg. loss = 0.1527, Time spent: 4.49s\n",
      "Epoch 5, iter 30: avg. loss = 0.1498, Time spent: 6.69s\n",
      "Epoch 5, iter 40: avg. loss = 0.1463, Time spent: 4.62s\n",
      "Epoch 5, iter 50: avg. loss = 0.1434, Time spent: 4.44s\n",
      "Epoch 5, iter 60: avg. loss = 0.1407, Time spent: 5.10s\n",
      "Epoch 5, iter 70: avg. loss = 0.1391, Time spent: 4.87s\n",
      "Epoch 5, iter 80: avg. loss = 0.1368, Time spent: 4.51s\n",
      "Epoch 5, iter 90: avg. loss = 0.1354, Time spent: 8.38s\n",
      "Epoch 5, iter 100: avg. loss = 0.1351, Time spent: 5.34s\n",
      "Epoch 5, iter 110: avg. loss = 0.1353, Time spent: 5.28s\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "n_epochs = 20\n",
    "# prompt ask for continue training or not\n",
    "cont = input('Continue training? yes or no')\n",
    "if cont == 'no':\n",
    "\tprint('Fresh training')\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "else:\n",
    "\tprint(f'Continue training')\n",
    "\t\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm, valloader)\n",
    "\tif eval_loss < best_val_loss:\n",
    "\t\tbest_val_loss = eval_loss\n",
    "\t\tbest_model = lstm.state_dict()\n",
    "\t\tif best_model is not None:\n",
    "\t\t\tprint(f'find new best model, save to lstm.pth, eval_loss: {eval_loss:.4f}')\n",
    "\t\t\ttorch.save(best_model, 'models/lstm.pth')\n",
    "\n",
    "\tif early_stop.stop_criterion(val_losses):\n",
    "\t\tprint(f'Early stopping on epoch {epoch + 1}')\n",
    "\t\tbreak\n",
    "\t\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and vocab\n",
    "torch.save(lstm.state_dict(), 'lstm.pth')\n",
    "torch.save(trainset.vocab, 'vocab.pth')\n",
    "\n",
    "# # Load model\n",
    "# lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "# lstm.load_state_dict(torch.load('lstm.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assume you have a function to load GloVe embeddings\n",
    "# def load_glove_embeddings(filepath):\n",
    "#     glove_embeddings = {}\n",
    "#     with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             vector = np.asarray(values[1:], dtype='float32')\n",
    "#             glove_embeddings[word] = vector\n",
    "#     return glove_embeddings\n",
    "\n",
    "# # Load GloVe embeddings from a file\n",
    "# glove_embeddings = load_glove_embeddings('glove_embeddings/glove.twitter.27B.100d.txt')\n",
    "\n",
    "# # Create a dictionary with your src_dictionary and GloVe embeddings\n",
    "# src_dictionary = trainset.vocab  # Add your vocabulary here\n",
    "# src_dictionary_size = len(src_dictionary)\n",
    "# embed_size = 100  # Example embedding size\n",
    "\n",
    "# # Map GloVe vectors to your src_dictionary index\n",
    "# mapped_glove_embeddings = {}\n",
    "# for word, index in src_dictionary.get_stoi().items():\n",
    "#     if word in glove_embeddings:\n",
    "#         mapped_glove_embeddings[index] = glove_embeddings[word]\n",
    "#     else:\n",
    "#         # If word is not in GloVe, initialize a random vector\n",
    "#         mapped_glove_embeddings[index] = np.random.normal(scale=0.6, size=(embed_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save mapped_glove_embeddings\n",
      "load mapped_glove_embeddings\n"
     ]
    }
   ],
   "source": [
    "# save and load mapped_glove_embeddings\n",
    "# print('save mapped_glove_embeddings')\n",
    "# torch.save(mapped_glove_embeddings, 'models/mapped_glove_embeddings.pth')\n",
    "print('load mapped_glove_embeddings')\n",
    "mapped_glove_embeddings = torch.load('models/mapped_glove_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "embed_size = 100\n",
    "lstm_glove = LSTM(trainset.vocab_size, embed_size, hidden_size, dropout=0.2, glove_embeddings=mapped_glove_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mlstm_glove\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m      5\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# prompt ask for continue training or not\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_glove' is not defined"
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_glove.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "n_epochs = 40\n",
    "# prompt ask for continue training or not\n",
    "cont = input('Continue training? yes or no')\n",
    "if cont == 'no':\n",
    "\tprint('Fresh training')\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "else:\n",
    "\tprint(f'Continue training')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm_glove.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm_glove.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm_glove(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm_glove, valloader)\n",
    "\n",
    "\tif eval_loss < best_val_loss:\n",
    "\t\tbest_val_loss = eval_loss\n",
    "\t\tbest_model = lstm_glove.state_dict()\n",
    "\t\tif best_model is not None:\n",
    "\t\t\tprint(f'find new best model, save to models/lstm_glove.pth, eval_loss: {eval_loss:.4f}')\n",
    "\t\t\ttorch.save(best_model, os.path.join('models', 'lstm_glove.pth'))\n",
    "\n",
    "\t# if early_stop.stop_criterion(val_losses):\n",
    "\t# \tprint(f'Early stopping at epoch {epoch + 1}')\n",
    "\t# \tbreak\n",
    "\t\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load mapped_glove_embeddings\n"
     ]
    }
   ],
   "source": [
    "# load mapped_glove_embeddings\n",
    "print('load mapped_glove_embeddings')\n",
    "mapped_glove_embeddings = torch.load('models/mapped_glove_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "embed_size = 100\n",
    "lstm_cnn_glove = LSTM_CNN(trainset.vocab_size, embed_size, hidden_size, num_filters=100, kernel_sizes= [3, 4, 5], dropout=0.2, glove_embeddings=mapped_glove_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh training\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.6649, Time spent: 2.26s\n",
      "Epoch 1, iter 20: avg. loss = 0.6588, Time spent: 2.31s\n",
      "Epoch 1, iter 30: avg. loss = 0.6575, Time spent: 3.82s\n",
      "Epoch 1, iter 40: avg. loss = 0.6536, Time spent: 2.23s\n",
      "Epoch 1, iter 50: avg. loss = 0.6504, Time spent: 2.24s\n",
      "Epoch 1, iter 60: avg. loss = 0.6446, Time spent: 2.34s\n",
      "Epoch 1, iter 70: avg. loss = 0.6374, Time spent: 2.26s\n",
      "Epoch 1, iter 80: avg. loss = 0.6299, Time spent: 2.23s\n",
      "Epoch 1, iter 90: avg. loss = 0.6236, Time spent: 4.37s\n",
      "Epoch 1, iter 100: avg. loss = 0.6213, Time spent: 2.34s\n",
      "Epoch 1, iter 110: avg. loss = 0.6164, Time spent: 2.36s\n",
      "Epoch 1, iter 120: avg. loss = 0.6104, Time spent: 2.26s\n",
      "Epoch 1, iter 130: avg. loss = 0.6020, Time spent: 2.61s\n",
      "Epoch 1, iter 140: avg. loss = 0.5917, Time spent: 2.30s\n",
      "Epoch 1, iter 150: avg. loss = 0.5810, Time spent: 2.39s\n",
      "find new best model, save to models/lstm_cnn_glove.pth, eval_loss: 0.3839\n",
      "Epoch 1, val loss = 0.3839, train loss = 0.5750; Time spent: 393.24s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.3562, Time spent: 2.27s\n",
      "Epoch 2, iter 20: avg. loss = 0.3421, Time spent: 2.33s\n",
      "Epoch 2, iter 30: avg. loss = 0.3323, Time spent: 3.50s\n",
      "Epoch 2, iter 40: avg. loss = 0.3204, Time spent: 2.24s\n",
      "Epoch 2, iter 50: avg. loss = 0.3096, Time spent: 2.26s\n",
      "Epoch 2, iter 60: avg. loss = 0.3004, Time spent: 2.35s\n",
      "Epoch 2, iter 70: avg. loss = 0.2932, Time spent: 2.25s\n",
      "Epoch 2, iter 80: avg. loss = 0.2865, Time spent: 2.25s\n",
      "Epoch 2, iter 90: avg. loss = 0.2796, Time spent: 4.24s\n",
      "Epoch 2, iter 100: avg. loss = 0.2743, Time spent: 2.33s\n",
      "Epoch 2, iter 110: avg. loss = 0.2698, Time spent: 2.34s\n",
      "Epoch 2, iter 120: avg. loss = 0.2671, Time spent: 3.72s\n",
      "Epoch 2, iter 130: avg. loss = 0.2641, Time spent: 2.60s\n",
      "Epoch 2, iter 140: avg. loss = 0.2599, Time spent: 2.29s\n",
      "Epoch 2, iter 150: avg. loss = 0.2560, Time spent: 2.39s\n",
      "find new best model, save to models/lstm_cnn_glove.pth, eval_loss: 0.1956\n",
      "Epoch 2, val loss = 0.1956, train loss = 0.2539; Time spent: 399.77s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.1857, Time spent: 2.28s\n",
      "Epoch 3, iter 20: avg. loss = 0.1875, Time spent: 2.32s\n",
      "Epoch 3, iter 30: avg. loss = 0.1864, Time spent: 3.58s\n",
      "Epoch 3, iter 40: avg. loss = 0.1866, Time spent: 2.24s\n",
      "Epoch 3, iter 50: avg. loss = 0.1837, Time spent: 2.25s\n",
      "Epoch 3, iter 60: avg. loss = 0.1818, Time spent: 2.44s\n",
      "Epoch 3, iter 70: avg. loss = 0.1804, Time spent: 2.25s\n",
      "Epoch 3, iter 80: avg. loss = 0.1794, Time spent: 2.23s\n",
      "Epoch 3, iter 90: avg. loss = 0.1781, Time spent: 4.25s\n",
      "Epoch 3, iter 100: avg. loss = 0.1773, Time spent: 2.34s\n",
      "Epoch 3, iter 110: avg. loss = 0.1766, Time spent: 9.06s\n",
      "Epoch 3, iter 120: avg. loss = 0.1759, Time spent: 2.26s\n",
      "Epoch 3, iter 130: avg. loss = 0.1752, Time spent: 2.60s\n",
      "Epoch 3, iter 140: avg. loss = 0.1739, Time spent: 2.31s\n",
      "Epoch 3, iter 150: avg. loss = 0.1729, Time spent: 2.40s\n",
      "find new best model, save to models/lstm_cnn_glove.pth, eval_loss: 0.1787\n",
      "Epoch 3, val loss = 0.1787, train loss = 0.1723; Time spent: 407.18s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.1599, Time spent: 2.27s\n",
      "Epoch 4, iter 20: avg. loss = 0.1622, Time spent: 2.33s\n",
      "Epoch 4, iter 30: avg. loss = 0.1597, Time spent: 3.50s\n",
      "Epoch 4, iter 40: avg. loss = 0.1603, Time spent: 2.24s\n",
      "Epoch 4, iter 50: avg. loss = 0.1587, Time spent: 2.25s\n",
      "Epoch 4, iter 60: avg. loss = 0.1571, Time spent: 2.35s\n",
      "Epoch 4, iter 70: avg. loss = 0.1552, Time spent: 2.26s\n",
      "Epoch 4, iter 80: avg. loss = 0.1540, Time spent: 2.23s\n",
      "Epoch 4, iter 90: avg. loss = 0.1526, Time spent: 4.24s\n",
      "Epoch 4, iter 100: avg. loss = 0.1516, Time spent: 2.34s\n",
      "Epoch 4, iter 110: avg. loss = 0.1511, Time spent: 2.35s\n",
      "Epoch 4, iter 120: avg. loss = 0.1518, Time spent: 2.26s\n",
      "Epoch 4, iter 130: avg. loss = 0.1520, Time spent: 2.61s\n",
      "Epoch 4, iter 140: avg. loss = 0.1511, Time spent: 2.31s\n",
      "Epoch 4, iter 150: avg. loss = 0.1504, Time spent: 2.39s\n",
      "find new best model, save to models/lstm_cnn_glove.pth, eval_loss: 0.1696\n",
      "Epoch 4, val loss = 0.1696, train loss = 0.1500; Time spent: 380.76s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.1365, Time spent: 2.27s\n",
      "Epoch 5, iter 20: avg. loss = 0.1384, Time spent: 2.32s\n",
      "Epoch 5, iter 30: avg. loss = 0.1366, Time spent: 3.50s\n",
      "Epoch 5, iter 40: avg. loss = 0.1356, Time spent: 2.23s\n",
      "Epoch 5, iter 50: avg. loss = 0.1340, Time spent: 2.25s\n",
      "Epoch 5, iter 60: avg. loss = 0.1326, Time spent: 2.35s\n",
      "Epoch 5, iter 70: avg. loss = 0.1309, Time spent: 2.26s\n",
      "Epoch 5, iter 80: avg. loss = 0.1298, Time spent: 2.24s\n",
      "Epoch 5, iter 90: avg. loss = 0.1291, Time spent: 4.24s\n",
      "Epoch 5, iter 100: avg. loss = 0.1291, Time spent: 2.34s\n",
      "Epoch 5, iter 110: avg. loss = 0.1295, Time spent: 2.35s\n",
      "Epoch 5, iter 120: avg. loss = 0.1300, Time spent: 2.26s\n",
      "Epoch 5, iter 130: avg. loss = 0.1309, Time spent: 2.61s\n",
      "Epoch 5, iter 140: avg. loss = 0.1307, Time spent: 2.30s\n",
      "Epoch 5, iter 150: avg. loss = 0.1309, Time spent: 2.39s\n",
      "Epoch 5, val loss = 0.1901, train loss = 0.1308; Time spent: 379.49s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 6, iter 10: avg. loss = 0.1295, Time spent: 2.28s\n",
      "Epoch 6, iter 20: avg. loss = 0.1312, Time spent: 2.32s\n",
      "Epoch 6, iter 30: avg. loss = 0.1274, Time spent: 3.51s\n",
      "Epoch 6, iter 40: avg. loss = 0.1269, Time spent: 2.23s\n",
      "Epoch 6, iter 50: avg. loss = 0.1258, Time spent: 2.25s\n",
      "Epoch 6, iter 60: avg. loss = 0.1244, Time spent: 2.35s\n",
      "Epoch 6, iter 70: avg. loss = 0.1227, Time spent: 2.26s\n",
      "Epoch 6, iter 80: avg. loss = 0.1210, Time spent: 2.23s\n",
      "Epoch 6, iter 90: avg. loss = 0.1194, Time spent: 4.24s\n",
      "Epoch 6, iter 100: avg. loss = 0.1189, Time spent: 2.34s\n",
      "Epoch 6, iter 110: avg. loss = 0.1184, Time spent: 2.34s\n",
      "Epoch 6, iter 120: avg. loss = 0.1181, Time spent: 2.27s\n",
      "Epoch 6, iter 130: avg. loss = 0.1192, Time spent: 2.60s\n",
      "Epoch 6, iter 140: avg. loss = 0.1194, Time spent: 2.30s\n",
      "Epoch 6, iter 150: avg. loss = 0.1195, Time spent: 2.39s\n",
      "Epoch 6, val loss = 0.1798, train loss = 0.1191; Time spent: 379.26s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 7, iter 10: avg. loss = 0.1092, Time spent: 2.28s\n",
      "Epoch 7, iter 20: avg. loss = 0.1116, Time spent: 2.32s\n",
      "Epoch 7, iter 30: avg. loss = 0.1091, Time spent: 3.50s\n",
      "Epoch 7, iter 40: avg. loss = 0.1070, Time spent: 2.23s\n",
      "Epoch 7, iter 50: avg. loss = 0.1065, Time spent: 2.26s\n",
      "Epoch 7, iter 60: avg. loss = 0.1068, Time spent: 2.35s\n",
      "Epoch 7, iter 70: avg. loss = 0.1064, Time spent: 2.26s\n",
      "Epoch 7, iter 80: avg. loss = 0.1066, Time spent: 2.23s\n",
      "Epoch 7, iter 90: avg. loss = 0.1057, Time spent: 4.25s\n",
      "Epoch 7, iter 100: avg. loss = 0.1052, Time spent: 2.34s\n",
      "Epoch 7, iter 110: avg. loss = 0.1046, Time spent: 2.34s\n",
      "Epoch 7, iter 120: avg. loss = 0.1043, Time spent: 2.27s\n",
      "Epoch 7, iter 130: avg. loss = 0.1056, Time spent: 2.60s\n",
      "Epoch 7, iter 140: avg. loss = 0.1061, Time spent: 2.31s\n",
      "Epoch 7, iter 150: avg. loss = 0.1063, Time spent: 2.38s\n",
      "Epoch 7, val loss = 0.2214, train loss = 0.1062; Time spent: 379.34s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 8, iter 10: avg. loss = 0.1094, Time spent: 2.27s\n",
      "Epoch 8, iter 20: avg. loss = 0.1099, Time spent: 2.33s\n",
      "Epoch 8, iter 30: avg. loss = 0.1077, Time spent: 3.51s\n",
      "Epoch 8, iter 40: avg. loss = 0.1063, Time spent: 2.24s\n",
      "Epoch 8, iter 50: avg. loss = 0.1051, Time spent: 2.25s\n",
      "Epoch 8, iter 60: avg. loss = 0.1085, Time spent: 2.35s\n",
      "Epoch 8, iter 70: avg. loss = 0.1115, Time spent: 2.26s\n",
      "Epoch 8, iter 80: avg. loss = 0.1126, Time spent: 2.23s\n",
      "Epoch 8, iter 90: avg. loss = 0.1122, Time spent: 4.24s\n",
      "Epoch 8, iter 100: avg. loss = 0.1108, Time spent: 2.34s\n",
      "Epoch 8, iter 110: avg. loss = 0.1092, Time spent: 2.34s\n",
      "Epoch 8, iter 120: avg. loss = 0.1074, Time spent: 2.26s\n",
      "Epoch 8, iter 130: avg. loss = 0.1073, Time spent: 2.61s\n",
      "Epoch 8, iter 140: avg. loss = 0.1065, Time spent: 2.30s\n",
      "Epoch 8, iter 150: avg. loss = 0.1060, Time spent: 2.39s\n",
      "Epoch 8, val loss = 0.1765, train loss = 0.1059; Time spent: 379.62s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 9, iter 10: avg. loss = 0.0951, Time spent: 2.27s\n",
      "Epoch 9, iter 20: avg. loss = 0.0961, Time spent: 2.32s\n",
      "Epoch 9, iter 30: avg. loss = 0.0935, Time spent: 3.51s\n",
      "Epoch 9, iter 40: avg. loss = 0.0906, Time spent: 2.24s\n",
      "Epoch 9, iter 50: avg. loss = 0.0938, Time spent: 2.25s\n",
      "Epoch 9, iter 60: avg. loss = 0.0995, Time spent: 2.35s\n",
      "Epoch 9, iter 70: avg. loss = 0.1048, Time spent: 2.26s\n",
      "Epoch 9, iter 80: avg. loss = 0.1078, Time spent: 2.23s\n",
      "Epoch 9, iter 90: avg. loss = 0.1073, Time spent: 4.24s\n",
      "Epoch 9, iter 100: avg. loss = 0.1057, Time spent: 2.34s\n",
      "Epoch 9, iter 110: avg. loss = 0.1036, Time spent: 2.60s\n",
      "Epoch 9, iter 120: avg. loss = 0.1014, Time spent: 2.26s\n",
      "Epoch 9, iter 130: avg. loss = 0.1004, Time spent: 2.60s\n",
      "Epoch 9, iter 140: avg. loss = 0.0995, Time spent: 2.30s\n",
      "Epoch 9, iter 150: avg. loss = 0.0985, Time spent: 2.40s\n",
      "Epoch 9, val loss = 0.1948, train loss = 0.0980; Time spent: 379.97s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 10, iter 10: avg. loss = 0.0873, Time spent: 2.28s\n",
      "Epoch 10, iter 20: avg. loss = 0.0859, Time spent: 2.32s\n",
      "Epoch 10, iter 30: avg. loss = 0.0839, Time spent: 3.51s\n",
      "Epoch 10, iter 40: avg. loss = 0.0790, Time spent: 2.23s\n",
      "Epoch 10, iter 50: avg. loss = 0.0815, Time spent: 2.25s\n",
      "Epoch 10, iter 60: avg. loss = 0.0852, Time spent: 2.35s\n",
      "Epoch 10, iter 70: avg. loss = 0.0924, Time spent: 2.26s\n",
      "Epoch 10, iter 80: avg. loss = 0.0951, Time spent: 2.23s\n",
      "Epoch 10, iter 90: avg. loss = 0.0949, Time spent: 4.24s\n",
      "Epoch 10, iter 100: avg. loss = 0.0941, Time spent: 2.34s\n",
      "Epoch 10, iter 110: avg. loss = 0.0926, Time spent: 2.34s\n",
      "Epoch 10, iter 120: avg. loss = 0.0905, Time spent: 2.27s\n",
      "Epoch 10, iter 130: avg. loss = 0.0887, Time spent: 2.61s\n",
      "Epoch 10, iter 140: avg. loss = 0.0872, Time spent: 2.30s\n",
      "Epoch 10, iter 150: avg. loss = 0.0863, Time spent: 2.39s\n",
      "Epoch 10, val loss = 0.2218, train loss = 0.0857; Time spent: 379.45s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 11, iter 10: avg. loss = 0.0761, Time spent: 2.28s\n",
      "Epoch 11, iter 20: avg. loss = 0.0763, Time spent: 2.32s\n",
      "Epoch 11, iter 30: avg. loss = 0.0793, Time spent: 3.51s\n",
      "Epoch 11, iter 40: avg. loss = 0.0760, Time spent: 2.24s\n",
      "Epoch 11, iter 50: avg. loss = 0.0773, Time spent: 2.26s\n",
      "Epoch 11, iter 60: avg. loss = 0.0794, Time spent: 2.35s\n",
      "Epoch 11, iter 70: avg. loss = 0.0856, Time spent: 2.26s\n",
      "Epoch 11, iter 80: avg. loss = 0.0875, Time spent: 2.23s\n",
      "Epoch 11, iter 90: avg. loss = 0.0880, Time spent: 4.25s\n",
      "Epoch 11, iter 100: avg. loss = 0.0867, Time spent: 2.35s\n",
      "Epoch 11, iter 110: avg. loss = 0.0859, Time spent: 2.35s\n",
      "Epoch 11, iter 120: avg. loss = 0.0848, Time spent: 2.26s\n",
      "Epoch 11, iter 130: avg. loss = 0.0829, Time spent: 2.60s\n",
      "Epoch 11, iter 140: avg. loss = 0.0808, Time spent: 2.30s\n",
      "Epoch 11, iter 150: avg. loss = 0.0795, Time spent: 2.39s\n",
      "Epoch 11, val loss = 0.2969, train loss = 0.0787; Time spent: 379.46s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 12, iter 10: avg. loss = 0.0712, Time spent: 2.28s\n",
      "Epoch 12, iter 20: avg. loss = 0.0751, Time spent: 2.32s\n",
      "Epoch 12, iter 30: avg. loss = 0.0749, Time spent: 3.50s\n",
      "Epoch 12, iter 40: avg. loss = 0.0744, Time spent: 2.23s\n",
      "Epoch 12, iter 50: avg. loss = 0.0725, Time spent: 2.25s\n",
      "Epoch 12, iter 60: avg. loss = 0.0748, Time spent: 2.35s\n",
      "Epoch 12, iter 70: avg. loss = 0.0747, Time spent: 2.26s\n",
      "Epoch 12, iter 80: avg. loss = 0.0752, Time spent: 2.23s\n",
      "Epoch 12, iter 90: avg. loss = 0.0752, Time spent: 4.24s\n",
      "Epoch 12, iter 100: avg. loss = 0.0742, Time spent: 2.35s\n",
      "Epoch 12, iter 110: avg. loss = 0.0739, Time spent: 2.35s\n",
      "Epoch 12, iter 120: avg. loss = 0.0734, Time spent: 2.26s\n",
      "Epoch 12, iter 130: avg. loss = 0.0728, Time spent: 2.61s\n",
      "Epoch 12, iter 140: avg. loss = 0.0720, Time spent: 2.30s\n",
      "Epoch 12, iter 150: avg. loss = 0.0716, Time spent: 2.38s\n",
      "Epoch 12, val loss = 0.2956, train loss = 0.0708; Time spent: 379.43s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 13, iter 10: avg. loss = 0.0842, Time spent: 2.28s\n",
      "Epoch 13, iter 20: avg. loss = 0.0822, Time spent: 2.32s\n",
      "Epoch 13, iter 30: avg. loss = 0.0798, Time spent: 3.50s\n",
      "Epoch 13, iter 40: avg. loss = 0.0741, Time spent: 2.24s\n",
      "Epoch 13, iter 50: avg. loss = 0.0687, Time spent: 2.25s\n",
      "Epoch 13, iter 60: avg. loss = 0.0649, Time spent: 2.35s\n",
      "Epoch 13, iter 70: avg. loss = 0.0631, Time spent: 2.26s\n",
      "Epoch 13, iter 80: avg. loss = 0.0627, Time spent: 2.23s\n",
      "Epoch 13, iter 90: avg. loss = 0.0620, Time spent: 4.24s\n",
      "Epoch 13, iter 100: avg. loss = 0.0608, Time spent: 2.34s\n",
      "Epoch 13, iter 110: avg. loss = 0.0601, Time spent: 2.34s\n",
      "Epoch 13, iter 120: avg. loss = 0.0599, Time spent: 2.27s\n",
      "Epoch 13, iter 130: avg. loss = 0.0598, Time spent: 2.61s\n",
      "Epoch 13, iter 140: avg. loss = 0.0615, Time spent: 2.30s\n",
      "Epoch 13, iter 150: avg. loss = 0.0626, Time spent: 2.39s\n",
      "Epoch 13, val loss = 0.2062, train loss = 0.0634; Time spent: 379.28s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 14, iter 10: avg. loss = 0.0768, Time spent: 2.27s\n",
      "Epoch 14, iter 20: avg. loss = 0.0761, Time spent: 2.32s\n",
      "Epoch 14, iter 30: avg. loss = 0.0716, Time spent: 3.50s\n",
      "Epoch 14, iter 40: avg. loss = 0.0651, Time spent: 2.23s\n",
      "Epoch 14, iter 50: avg. loss = 0.0587, Time spent: 2.25s\n",
      "Epoch 14, iter 60: avg. loss = 0.0546, Time spent: 2.35s\n",
      "Epoch 14, iter 70: avg. loss = 0.0530, Time spent: 2.26s\n",
      "Epoch 14, iter 80: avg. loss = 0.0526, Time spent: 2.23s\n",
      "Epoch 14, iter 90: avg. loss = 0.0520, Time spent: 4.24s\n",
      "Epoch 14, iter 100: avg. loss = 0.0516, Time spent: 2.34s\n",
      "Epoch 14, iter 110: avg. loss = 0.0512, Time spent: 2.35s\n",
      "Epoch 14, iter 120: avg. loss = 0.0513, Time spent: 2.26s\n",
      "Epoch 14, iter 130: avg. loss = 0.0520, Time spent: 2.61s\n",
      "Epoch 14, iter 140: avg. loss = 0.0550, Time spent: 2.31s\n",
      "Epoch 14, iter 150: avg. loss = 0.0568, Time spent: 2.38s\n",
      "Epoch 14, val loss = 0.2871, train loss = 0.0573; Time spent: 379.44s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 15, iter 10: avg. loss = 0.0910, Time spent: 2.27s\n",
      "Epoch 15, iter 20: avg. loss = 0.0821, Time spent: 2.32s\n",
      "Epoch 15, iter 30: avg. loss = 0.0747, Time spent: 3.51s\n",
      "Epoch 15, iter 40: avg. loss = 0.0689, Time spent: 2.24s\n",
      "Epoch 15, iter 50: avg. loss = 0.0624, Time spent: 2.25s\n",
      "Epoch 15, iter 60: avg. loss = 0.0584, Time spent: 2.35s\n",
      "Epoch 15, iter 70: avg. loss = 0.0552, Time spent: 2.27s\n",
      "Epoch 15, iter 80: avg. loss = 0.0538, Time spent: 2.23s\n",
      "Epoch 15, iter 90: avg. loss = 0.0526, Time spent: 4.24s\n",
      "Epoch 15, iter 100: avg. loss = 0.0516, Time spent: 2.34s\n",
      "Epoch 15, iter 110: avg. loss = 0.0525, Time spent: 2.34s\n",
      "Epoch 15, iter 120: avg. loss = 0.0536, Time spent: 2.26s\n",
      "Epoch 15, iter 130: avg. loss = 0.0563, Time spent: 2.61s\n",
      "Epoch 15, iter 140: avg. loss = 0.0582, Time spent: 2.31s\n",
      "Epoch 15, iter 150: avg. loss = 0.0589, Time spent: 2.39s\n",
      "Epoch 15, val loss = 0.2619, train loss = 0.0585; Time spent: 379.19s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 16, iter 10: avg. loss = 0.0441, Time spent: 2.27s\n",
      "Epoch 16, iter 20: avg. loss = 0.0434, Time spent: 2.32s\n",
      "Epoch 16, iter 30: avg. loss = 0.0411, Time spent: 3.51s\n",
      "Epoch 16, iter 40: avg. loss = 0.0406, Time spent: 2.24s\n",
      "Epoch 16, iter 50: avg. loss = 0.0388, Time spent: 2.25s\n",
      "Epoch 16, iter 60: avg. loss = 0.0388, Time spent: 2.35s\n",
      "Epoch 16, iter 70: avg. loss = 0.0385, Time spent: 2.26s\n",
      "Epoch 16, iter 80: avg. loss = 0.0379, Time spent: 2.23s\n",
      "Epoch 16, iter 90: avg. loss = 0.0374, Time spent: 4.24s\n",
      "Epoch 16, iter 100: avg. loss = 0.0378, Time spent: 2.34s\n",
      "Epoch 16, iter 110: avg. loss = 0.0395, Time spent: 2.35s\n",
      "Epoch 16, iter 120: avg. loss = 0.0427, Time spent: 2.26s\n",
      "Epoch 16, iter 130: avg. loss = 0.0455, Time spent: 2.61s\n",
      "Epoch 16, iter 140: avg. loss = 0.0467, Time spent: 2.30s\n",
      "Epoch 16, iter 150: avg. loss = 0.0467, Time spent: 2.39s\n",
      "Epoch 16, val loss = 0.2592, train loss = 0.0462; Time spent: 379.72s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 17, iter 10: avg. loss = 0.0283, Time spent: 2.27s\n",
      "Epoch 17, iter 20: avg. loss = 0.0313, Time spent: 2.32s\n",
      "Epoch 17, iter 30: avg. loss = 0.0301, Time spent: 3.51s\n",
      "Epoch 17, iter 40: avg. loss = 0.0285, Time spent: 2.24s\n",
      "Epoch 17, iter 50: avg. loss = 0.0278, Time spent: 2.25s\n",
      "Epoch 17, iter 60: avg. loss = 0.0290, Time spent: 2.35s\n",
      "Epoch 17, iter 70: avg. loss = 0.0305, Time spent: 2.26s\n",
      "Epoch 17, iter 80: avg. loss = 0.0318, Time spent: 2.23s\n",
      "Epoch 17, iter 90: avg. loss = 0.0319, Time spent: 4.24s\n",
      "Epoch 17, iter 100: avg. loss = 0.0317, Time spent: 2.34s\n",
      "Epoch 17, iter 110: avg. loss = 0.0333, Time spent: 2.35s\n",
      "Epoch 17, iter 120: avg. loss = 0.0358, Time spent: 2.26s\n",
      "Epoch 17, iter 130: avg. loss = 0.0379, Time spent: 2.86s\n",
      "Epoch 17, iter 140: avg. loss = 0.0393, Time spent: 2.31s\n",
      "Epoch 17, iter 150: avg. loss = 0.0398, Time spent: 2.38s\n",
      "Epoch 17, val loss = 0.2876, train loss = 0.0394; Time spent: 379.91s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 18, iter 10: avg. loss = 0.0368, Time spent: 2.28s\n",
      "Epoch 18, iter 20: avg. loss = 0.0348, Time spent: 2.32s\n",
      "Epoch 18, iter 30: avg. loss = 0.0301, Time spent: 3.51s\n",
      "Epoch 18, iter 40: avg. loss = 0.0288, Time spent: 2.24s\n",
      "Epoch 18, iter 50: avg. loss = 0.0273, Time spent: 2.25s\n",
      "Epoch 18, iter 60: avg. loss = 0.0280, Time spent: 2.35s\n",
      "Epoch 18, iter 70: avg. loss = 0.0296, Time spent: 2.27s\n",
      "Epoch 18, iter 80: avg. loss = 0.0312, Time spent: 2.22s\n",
      "Epoch 18, iter 90: avg. loss = 0.0318, Time spent: 4.24s\n",
      "Epoch 18, iter 100: avg. loss = 0.0314, Time spent: 2.34s\n",
      "Epoch 18, iter 110: avg. loss = 0.0303, Time spent: 2.35s\n",
      "Epoch 18, iter 120: avg. loss = 0.0305, Time spent: 2.27s\n",
      "Epoch 18, iter 130: avg. loss = 0.0309, Time spent: 2.61s\n",
      "Epoch 18, iter 140: avg. loss = 0.0311, Time spent: 2.31s\n",
      "Epoch 18, iter 150: avg. loss = 0.0311, Time spent: 2.39s\n",
      "Epoch 18, val loss = 0.3184, train loss = 0.0307; Time spent: 379.52s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 19, iter 10: avg. loss = 0.0248, Time spent: 2.27s\n",
      "Epoch 19, iter 20: avg. loss = 0.0299, Time spent: 2.33s\n",
      "Epoch 19, iter 30: avg. loss = 0.0292, Time spent: 3.51s\n",
      "Epoch 19, iter 40: avg. loss = 0.0285, Time spent: 2.24s\n",
      "Epoch 19, iter 50: avg. loss = 0.0289, Time spent: 2.26s\n",
      "Epoch 19, iter 60: avg. loss = 0.0306, Time spent: 2.35s\n",
      "Epoch 19, iter 70: avg. loss = 0.0325, Time spent: 2.26s\n",
      "Epoch 19, iter 80: avg. loss = 0.0357, Time spent: 2.23s\n",
      "Epoch 19, iter 90: avg. loss = 0.0369, Time spent: 4.24s\n",
      "Epoch 19, iter 100: avg. loss = 0.0371, Time spent: 2.34s\n",
      "Epoch 19, iter 110: avg. loss = 0.0362, Time spent: 2.35s\n",
      "Epoch 19, iter 120: avg. loss = 0.0353, Time spent: 2.26s\n",
      "Epoch 19, iter 130: avg. loss = 0.0345, Time spent: 2.61s\n",
      "Epoch 19, iter 140: avg. loss = 0.0343, Time spent: 2.30s\n",
      "Epoch 19, iter 150: avg. loss = 0.0341, Time spent: 2.38s\n",
      "Epoch 19, val loss = 0.3162, train loss = 0.0337; Time spent: 379.50s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 20, iter 10: avg. loss = 0.0278, Time spent: 2.27s\n",
      "Epoch 20, iter 20: avg. loss = 0.0335, Time spent: 2.32s\n",
      "Epoch 20, iter 30: avg. loss = 0.0332, Time spent: 3.51s\n",
      "Epoch 20, iter 40: avg. loss = 0.0320, Time spent: 2.24s\n",
      "Epoch 20, iter 50: avg. loss = 0.0318, Time spent: 2.25s\n",
      "Epoch 20, iter 60: avg. loss = 0.0347, Time spent: 2.35s\n",
      "Epoch 20, iter 70: avg. loss = 0.0381, Time spent: 2.26s\n",
      "Epoch 20, iter 80: avg. loss = 0.0411, Time spent: 2.23s\n",
      "Epoch 20, iter 90: avg. loss = 0.0437, Time spent: 4.24s\n",
      "Epoch 20, iter 100: avg. loss = 0.0440, Time spent: 2.34s\n",
      "Epoch 20, iter 110: avg. loss = 0.0431, Time spent: 2.34s\n",
      "Epoch 20, iter 120: avg. loss = 0.0428, Time spent: 2.27s\n",
      "Epoch 20, iter 130: avg. loss = 0.0418, Time spent: 2.61s\n",
      "Epoch 20, iter 140: avg. loss = 0.0412, Time spent: 2.31s\n",
      "Epoch 20, iter 150: avg. loss = 0.0412, Time spent: 2.38s\n",
      "Epoch 20, val loss = 0.3130, train loss = 0.0409; Time spent: 379.49s\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_cnn_glove.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "n_epochs = 20\n",
    "# prompt ask for continue training or not\n",
    "cont = input('Continue training? yes or no')\n",
    "if cont == 'no':\n",
    "\tprint('Fresh training')\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "else:\n",
    "\tprint(f'Continue training')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm_cnn_glove.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm_cnn_glove.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm_cnn_glove(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm_cnn_glove, valloader)\n",
    "\n",
    "\tif eval_loss < best_val_loss:\n",
    "\t\tbest_val_loss = eval_loss\n",
    "\t\tbest_model = lstm_cnn_glove.state_dict()\n",
    "\t\tif best_model is not None:\n",
    "\t\t\tprint(f'find new best model, save to models/lstm_cnn_glove.pth, eval_loss: {eval_loss:.4f}')\n",
    "\t\t\ttorch.save(best_model, os.path.join('models', 'lstm_cnn_glove.pth'))\n",
    "\n",
    "\t# if early_stop.stop_criterion(val_losses):\n",
    "\t# \tprint(f'Early stopping at epoch {epoch + 1}')\n",
    "\t# \tbreak\n",
    "\t\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla LSTM\n",
    "# hidden_size = embed_size = 256\n",
    "# lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "# lstm.load_state_dict(torch.load('models/lstm.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM with GloVe embeddings\n",
    "hidden_size = 256\n",
    "embed_size = 100\n",
    "lstm_glove = LSTM(trainset.vocab_size, embed_size, hidden_size, dropout=0.2, glove_embeddings=None).to(device)\n",
    "lstm_glove.load_state_dict(torch.load('models/lstm_glove.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-CNN with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "embed_size = 100\n",
    "lstm_cnn_glove = LSTM_CNN(trainset.vocab_size, embed_size, hidden_size, num_filters=100, kernel_sizes= [3, 4, 5], dropout=0.2, glove_embeddings=None).to(device)\n",
    "lstm_cnn_glove.load_state_dict(torch.load('models/lstm_cnn_glove.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(lstm, pad_src_seqs, src_seq_lengths):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "\n",
    "    Args:\n",
    "    lstm (LSTM): Trained lstm.\n",
    "    pad_src_seqs of shape (max_src_seq_length, batch_size): Padded source sequences.\n",
    "    src_seq_lengths: List of source sequence lengths.\n",
    "\n",
    "    Returns:\n",
    "    out_seqs of shape (batch_size, 1): LongTensor of word indices of the output sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        pad_src_seqs = pad_src_seqs.to(device)\n",
    "        lstm_hidden = lstm.init_hidden(pad_src_seqs.shape[1], device)\n",
    "        outputs = lstm(pad_src_seqs, src_seq_lengths, lstm_hidden)\n",
    "        out_seqs = outputs > 0.5\n",
    "        return out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes(lstm):\n",
    "    pad_src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths=[4, 2])\n",
    "    assert out_seqs.shape == torch.Size([2, 1]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes(lstm_cnn_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify training data:\n",
      "-----------------------------\n",
      "SRC: ['the', 'benign', 'effect', 'of', 'the', 'financial', 'crisis', 'in', 'canada', 'that', 'reduce', 'consumer', 'borrowing', 'in', 'the', 'us', 'increase', 'it', 'here', '.', 'low', 'rate', 'but', 'a', 'healthy', 'gap', 'between', 'funding', 'level', 'for', 'bank', 'and', 'prime', 'rate', 'mean', 'huge', 'profit', 'form', 'retail', '.', 'the', 'bank', 'push', 'we', 'all', 'to', 'borrow', 'with', 'slogan', 'like', 'you', 'be', 'wealthy', 'than', 'you', 'think', '.', 'I', 'once', 'hear', 'a', 'bank', 'treasurer', 'imply', 'these', 'spread', 'represent', 'over', 'of', 'their', 'profit', 'the', 'prior', 'year', '.', 'and', 'so', 'it', 'be', 'ironic', 'that', 'the', 'bank', 'be', 'now', 'among', 'those', 'heed', 'the', 'debt', 'level', 'warning', '.', 'there', 'can', 'be', 'no', 'explanation', 'other', 'than', 'that', 'they', 'can', 'not', 'wean', 'themselves', 'off', 'it', 'and', 'need', 'osfi', 'to', 'help', '.', 'it', 'be', 'game', 'theory', 'run', 'amok', 'when', 'an', 'oligopoly', 'need', 'someone', 'outside', 'the', 'game', 'to', 'impel', 'they', 'to', 'change', 'the', 'way', 'they', 'play', '.', 'with', 'house', 'price', 'up', 'as', 'much', 'as', 'they', 'be', 'over', 'the', 'year', 'even', 'those', 'that', 'in', 'a', 'basis', 'point', 'rise', 'may', 'have', 'a', 'little', 'wiggle', 'room', '.', 'I', 'note', 'that', 'no', 'one', 'be', 'estimate', 'the', 'price', 'sensitivity', 'of', 'the', 'real', 'estate', 'market', 'to', 'the', 'move', '.', 'that', 'would', 'be', 'a', 'good', 'piece', 'of', 'work', '.', '<eos>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['thank', 'you', '<unk>', 'for', 'your', 'wise', 'word', '.', 'you', 'be', 'just', 'a', 'little', 'young', 'than', 'my', 'parent', 'bear', 'in', 'and', '.', 'yes', 'I', 'agree', 'with', 'your', 'statement', 'if', 'die', 'be', 'just', 'a', 'part', 'of', 'the', 'process', 'of', 'life', 'and', 'that', 'be', 'the', 'way', 'god', 'arrange', 'it', 'it', 'must', 'somehow', 'be', 'a', 'good', 'thing', '.', 'but', 'another', 'aspect', 'of', 'death', 'be', 'how', 'it', 'affect', 'our', 'relationship', 'with', 'other', 'living', 'creature', '.', 'we', 'be', 'so', 'easily', 'make', 'to', 'overlook', 'our', 'complicity', 'in', 'the', 'death', 'of', 'other', 'who', 'have', 'as', 'much', 'right', 'to', 'live', 'as', 'we', 'do', 'it', 'be', 'the', 'way', 'of', 'wisdom', 'to', 'pay', 'attention', 'to', 'the', 'life', 'of', 'these', 'countless', 'cousin', 'of', 'ours', 'and', 'never', 'do', 'anything', 'to', 'make', 'their', 'death', 'more', 'painful', 'than', 'they', 'be', '.', 'so', 'on', 'the', 'matter', 'of', 'assisted', 'suicide', 'I', 'be', 'confident', 'that', 'sick', 'people', 'with', 'no', 'hope', 'of', 'live', 'much', 'long', 'do', 'a', 'good', 'thing', 'to', 'spare', 'themselves', 'more', 'pain', 'and', 'their', 'love', 'one', 'inconvenience', '.', 'and', 'those', 'who', 'assist', 'they', 'also', 'do', 'a', 'good', 'thing', 'by', 'be', 'their', 'minister', '.', 'it', 's', 'much', 'well', 'than', 'the', 'unreal', 'inhumane', 'demand', 'of', 'modern', 'medical', 'life', 'save', 'practice', '.', '<eos>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['to', 'all', 'you', 'idiotic', 'naysayer', 'ok', 'let', 's', 'play', '.', '.', '.', 'say', 'climate', 'change', 'isn', 't', 'be', 'cause', 'by', 'stupid', 'human', 'activity', '<unk>', 'do', 'it', 'not', 'behoove', 'we', 'to', 'do', 'everything', 'we', 'can', 'to', 'mitigate', 'any', 'possibility', 'that', 'it', 'might', 'people', '?', 'I', 'm', 'not', 'say', 'we', 'can', 'fix', 'it', 'overnight', 'but', 'we', 're', 'one', 'of', 'the', 'early', 'generation', 'of', 'a', 'huge', 'industrial', 'revolution', 'and', 'can', 'set', 'the', 'tone', 'for', 'environmental', 'stewardship', 'for', 'future', 'one', '.', 'or', 'be', 'all', 'you', 'lazy', 'minded', 'moron', 'content', 'that', 'we', 'can', 'just', 'happily', 'continue', 'to', 'pollute', 'the', 'atmosphere', 'greedily', 'mine', 'and', 'drill', 'massively', 'over', 'fish', 'the', 'ocean', 'and', 'dump', 'million', 'of', 'metric', 'ton', 'of', 'trash', 'into', 'it', 'and', 'landfill', 'on', 'a', 'daily', 'basis', '?', 'we', 're', 'already', 'tragically', 'late', '!', 'ever', 'hear', 'the', 'saying', 'leave', 'a', 'place', 'in', 'well', 'condition', 'than', 'the', 'way', 'you', 'find', 'it', '.', 'you', 're', 'the', 'type', 'that', 'go', 'to', 'house', 'party', 'and', 'don', 't', 'stick', 'around', 'to', 'help', 'clean', 'up', 'your', 'mess', '.', 'your', 'unbelievably', 'stupid', 'selfish', 'and', 'dismissive', 'attitude', 'towards', 'the', 'environment', 'and', 'even', 'your', 'own', 'family', 's', 'future', 'be', 'just', 'beyond', 'stunning', '.', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 1\n",
      "OUT: True\n",
      "\n",
      "SRC: ['oh', 'bullshit', '.', 'go', 'look', 'at', 'the', 'most', 'gender', 'equal', 'country', 'on', 'earth', 'that', 'would', 'be', 'iceland', 'and', 'you', 'll', 'find', 'that', 'the', '<unk>', 'have', 'not', 'be', 'replace', 'by', 'the', 'state', 'it', 'have', 'not', 'be', 'replace', 'by', 'anything', '.', 'people', 'just', 'fail', 'to', 'expect', 'anything', 'from', 'each', 'other', 'base', 'on', 'gender', 'and', 'double', 'standard', 'don', 't', 'exist', '.', 'and', 'contrary', 'to', 'stupid', 'claim', 'elsewhere', 'in', 'the', 'comment', 'guess', 'what', 'else', 'have', 'cease', 'to', 'exist', 'any', 'kind', 'of', '<unk>', '.', 'the', 'big', 'downside', 'iceland', 'have', 'face', 'be', 'that', 'not', 'have', 'any', 'double', 'standard', 'or', 'judgement', 'of', 'people', 's', 'behaviour', 'their', 'enjoyment', 'of', 'daily', 'casual', 'sex', 'have', 'give', 'they', 'a', 'rather', 'high', 'than', 'average', 'rate', 'of', '<unk>', 'know', 'locally', 'as', 'the', '<unk>', 'handshake', 'one', 'of', 'the', 'few', 'stds', 'you', 'can', 'fairly', 'easily', 'catch', 'even', 'when', 'use', 'condom', '.', 'but', 'then', 'it', 'be', 'also', 'one', 'of', 'the', 'easy', 'to', 'cure', 'and', 'with', 'an', 'excellent', 'free', 'for', 'all', 'healthcare', 'system', 'it', 's', 'really', 'not', 'a', 'major', 'problem', '.', 'if', 'that', 's', 'the', 'big', 'price', 'for', 'a', 'society', 'where', 'rape', 'be', 'basically', 'non', 'existent', 'I', 'would', 'pay', 'it', 'with', 'glee', '.', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 1\n",
      "OUT: True\n",
      "\n",
      "SRC: ['these', 'poor', 'people', 'a', 'pawn', 'in', 'a', 'cruel', 'political', 'game', '.', 'obama', 'open', 'the', 'door', 'invite', 'their', 'parent', 'in', 'by', 'promise', 'to', 'make', 'their', 'lawbreaking', 'legal', 'and', 'later', 'reward', 'their', 'child', 'by', 'make', 'dream', 'act', 'previously', 'reject', 'by', 'congress', 'into', 'executive', 'order', 'daca', '.', 'he', 'dem', 'have', 'nothing', 'to', 'lose', 'in', 'this', 'cruel', 'game', 'that', 'have', 'nothing', 'to', 'do', 'with', 'compassion', 'and', 'here', 's', 'why', '.', 'if', 'hillary', 'would', 've', 'be', 'elect', 'she', 'would', 'continue', 'it', 'and', 'keep', 'flood', 'the', 'country', 'with', 'democratic', 'voter', '.', 'if', 'a', 'republican', 'win', 'as', 'he', 'do', 'he', 'would', 'have', 'a', 'mess', 'on', 'his', 'hand', 'have', 'to', 'cancel', 'daca', 'which', 'he', 'do', 'and', 'face', 'the', 'backlash', 'of', 'the', 'big', 'chunk', 'of', 'compassionate', 'american', 'voter', 'and', 'endure', 'riot', 'by', 'angry', 'who', 'can', 'blame', 'they', '?', 'the', 'be', 'lie', 'to', 'dreamer', '.', 'a', 'win', 'win', '.', 'it', 'be', 'all', 'plan', '.', 'now', 'obama', 'and', 'his', 'party', 'can', 'sit', 'back', 'pretend', 'to', 'care', 'and', 'enjoy', 'the', 'scenery', 'of', 'angry', 'youth', 'hate', 'on', 'current', 'president', 'who', 'have', 'nothing', 'to', 'do', 'with', 'false', 'illegal', 'promise', 'of', 'the', 'previous', 'administration', '.', 'dreamer', 'should', 'take', 'obama', 'to', 'court', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Classify training data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = next(iter(valloader))\n",
    "out_seqs = classify(lstm_cnn_glove, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_tokens(pad_src_seqs[:,i], trainset.vocab))\n",
    "    print('TGT:', pad_tgt_seqs[i].item())\n",
    "    print('OUT:', out_seqs[i].item())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(lstm, dataloader=valloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(dataloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        correct += (out_seqs.squeeze().cpu().long() == tgt_labels).sum().item()\n",
    "        total += tgt_labels.shape[0]\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def test_f1_score(lstm, dataloader=valloader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(dataloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        y_true.extend(tgt_labels.cpu().numpy())\n",
    "        y_pred.extend(out_seqs.squeeze().cpu().numpy())\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9140214946263434\n"
     ]
    }
   ],
   "source": [
    "# print(test_accuracy(lstm_glove, valloader))\n",
    "print(test_f1_score(lstm_cnn_glove, valloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset = TranslationDataset('test_2024.csv', vocab=trainset.vocab, dataset_type='test')\n",
    "# testloader = DataLoader(dataset=testset, batch_size=1, shuffle=False, collate_fn=collate, pin_memory=True)\n",
    "# # save testset\n",
    "# torch.save(testset, 'testset.pth')    \n",
    "\n",
    "# load testset\n",
    "testset = torch.load('datasets/lstm/testset.pth')\n",
    "testloader = DataLoader(dataset=testset, batch_size=32, shuffle=False, collate_fn=collate, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11974</th>\n",
       "      <td>11996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11969</th>\n",
       "      <td>11997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11980</th>\n",
       "      <td>11999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>12000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12001 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label\n",
       "20         0      1\n",
       "9          1      0\n",
       "4          2      0\n",
       "5          3      1\n",
       "1          4      1\n",
       "...      ...    ...\n",
       "11974  11996      1\n",
       "11969  11997      0\n",
       "11997  11998      1\n",
       "11980  11999      1\n",
       "12000  12000      0\n",
       "\n",
       "[12001 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do inference on test set and save the results into csv file\n",
    "def test_inference(lstm, output_filename='submission.csv', testset=testset):\n",
    "    testloader = DataLoader(dataset=testset, batch_size=32, shuffle=False, collate_fn=collate, pin_memory=True)\n",
    "    out = []\n",
    "    indices = []\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(testloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        try:\n",
    "            out.extend(out_seqs.squeeze().cpu().numpy())\n",
    "        except:\n",
    "            out.append(out_seqs.squeeze().cpu().numpy())\n",
    "        indices.extend(ids)\n",
    "    df = pd.DataFrame({'id': indices, 'label': out})\n",
    "    # convert label to int\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    # sort by id\n",
    "    df = df.sort_values(by='id')\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    return df\n",
    "\n",
    "# test_inference(lstm_cnn_glove, output_filename='dev_inference.csv', testset=valset)\n",
    "test_inference(lstm_cnn_glove, output_filename='submission.csv', testset=testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9311761312856803\n",
      "Recall: 0.9331633815975648\n",
      "F1: 0.9321520869749367\n"
     ]
    }
   ],
   "source": [
    "# import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Compute Precision, Recall, and F1 Score of the imported predicted csv and the validation df\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Load the predicted csv\n",
    "y_pred = pd.read_csv('dev_inference.csv', index_col=0)\n",
    "y_pred = y_pred['label'].tolist()\n",
    "\n",
    "# Load the validation df\n",
    "y_true = pd.read_csv('dev_2024.csv', quoting=3)\n",
    "y_true = y_true['label'].tolist()\n",
    "\n",
    "# Compute the metrics\n",
    "precision, recall, f1 = compute_metrics(y_true, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
