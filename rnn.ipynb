{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Except that Desmond played first base last nig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What i find funny is the loyalty and blindness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Read the article  not just the headline &amp; you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Speaking of a horses backside  is that where y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Michael Barone- gee are you dumb.  No other wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label\n",
       "0   0  Except that Desmond played first base last nig...      0\n",
       "1   1  What i find funny is the loyalty and blindness...      0\n",
       "2   2  Read the article  not just the headline & you ...      0\n",
       "3   3  Speaking of a horses backside  is that where y...      1\n",
       "4   4  Michael Barone- gee are you dumb.  No other wo...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_2024.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "EOS_token = 1\n",
    "lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "\tdef __init__(self, csv_path, dataset_type='train', vocab=None):\n",
    "\t\tdf = pd.read_csv(csv_path, quoting=3)\n",
    "\t\tprint(f'len df: {len(df)}')\n",
    "\t\tif dataset_type in ['train', 'val']:\n",
    "\t\t\tself.text, self.labels = zip(*[(text, label) for text, label in zip(df['text'], df['label'])])\n",
    "\t\telse:\n",
    "\t\t\tself.text = df['text'].tolist()\n",
    "\t\t\tself.labels = [0 for _ in range(len(self.text))]\n",
    "\t\tself.ids = df['id'].tolist()\n",
    "\t\tself.dataset_type = dataset_type\n",
    "\t\tself.tokenizer = get_tokenizer('basic_english')\n",
    "\t\tself._preprocess(vocab)\n",
    "\n",
    "\tdef _preprocess(self, vocab):\n",
    "\t\t# preprocess text\n",
    "\t\tself.text = [self._preprocess_sentence(text) for text in self.text]\n",
    "\n",
    "\t\tif vocab is None:\n",
    "\t\t\tself.vocab = build_vocab_from_iterator(self._yield_tokens(), specials=[\"<unk>\"])\n",
    "\t\t\tself.vocab.set_default_index(self.vocab['<unk>'])\n",
    "\t\t\tself.vocab.insert_token('<eos>', EOS_token)  # Insert <eos> token with index 1\n",
    "\t\telse:\n",
    "\t\t\tself.vocab = vocab\n",
    "\t\t\t\n",
    "\t\tself.vocab_size = len(self.vocab)\n",
    "\t\n",
    "\tdef _preprocess_sentence(self, sentence):\n",
    "\t\tsentence = normalizeString(sentence)\n",
    "\t\tsentence = self.tokenizer(sentence)\n",
    "\t\tsentence = lemmaString(sentence)\n",
    "\t\treturn sentence\n",
    "\n",
    "\tdef _yield_tokens(self):\n",
    "\t\tfor text_sample in self.text:\n",
    "\t\t\tyield text_sample\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tinput_seq = text_to_indices(self.vocab, self.text[idx])\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\treturn input_seq, label, self.ids[idx]\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "\treturn ''.join(\n",
    "\t\tc for c in unicodedata.normalize('NFD', s)\n",
    "\t\tif unicodedata.category(c) != 'Mn'\n",
    "\t)\n",
    "\n",
    "def normalizeString(s):\n",
    "\ts = unicodeToAscii(s.lower().strip())\n",
    "\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\treturn s\n",
    "\n",
    "def lemmaString(tokens):\n",
    "\treturn [token.lemma_ for token in lemmatizer(' '.join(tokens))]\n",
    "\n",
    "def text_to_indices(vocab, tokens):\n",
    "\tindices = [vocab[token] for token in tokens]\n",
    "\tindices.append(EOS_token)\n",
    "\treturn torch.tensor(indices, dtype=torch.long).view(-1)\n",
    "\n",
    "def seq_to_tokens(seq, vocab):\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[idx] for idx in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df: 10\n"
     ]
    }
   ],
   "source": [
    "trainset = TranslationDataset('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['except', 'that', 'desmond', 'play', 'first', 'base', 'last', 'night', '.', 'tapia', 'be', 'in', 'lf', 'and', 'reynolds', 'have', 'a', 'night', 'off', '.', '<eos>']\n",
      "tensor([109,  18,  95, 157, 110,  78, 128,  44,   2, 189,   3,  17, 130,   7,\n",
      "        172,  16,  25,  44, 145,   2,   1]) 0\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "src_sentence, label, id_ = trainset[0]\n",
    "print(seq_to_tokens(src_sentence, trainset.vocab))\n",
    "print(src_sentence, label)\n",
    "print(type(src_sentence), type(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "\t\"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "\tArgs:\n",
    "\tlist_of_samples is a list of tuples (src_seq, tgt_label, id):\n",
    "\t\tsrc_seq is of shape (src_seq_length,)\n",
    "\t\ttgt_label is of shape (1,)\n",
    "\t\tid is an int\n",
    "\n",
    "\tReturns:\n",
    "\tsrc_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "\t\tThe sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "\t\tthe longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "\tsrc_seq_lengths: List of lengths of source sequences.\n",
    "\ttgt_labels of shape (batch_size, 1): Tensor of labels for each sequence.\n",
    "\t\"\"\"\n",
    "\t# YOUR CODE HERE\n",
    "\tsrc_seqs = [s[0] for s in list_of_samples]\n",
    "\ttgt_labels = torch.LongTensor([s[1] for s in list_of_samples])\n",
    "\tsrc_seq_lengths = [len(s) for s in src_seqs]\n",
    "\tids = [s[2] for s in list_of_samples]\n",
    "\tsrc_seqs = pad_sequence(src_seqs, padding_value=PADDING_VALUE)\n",
    "\n",
    "\tsrc_seq_lengths, indices = torch.sort(torch.tensor(src_seq_lengths), descending=True)\n",
    "\tsrc_seqs = src_seqs[:, indices]\n",
    "\ttgt_labels = tgt_labels[indices]\n",
    "\n",
    "\treturn src_seqs, src_seq_lengths.tolist(), tgt_labels, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), 1, 0),\n",
    "        (torch.LongTensor([6, 7, 8]), 0, 1),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = collate(pairs)\n",
    "    assert type(src_seq_lengths) == list, \"src_seq_lengths should be a list.\"\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences combined:\n",
      "tensor([[11,  6,  1],\n",
      "        [12,  7,  2],\n",
      "        [13,  8,  0],\n",
      "        [14,  0,  0]])\n",
      "[4, 3, 2]\n",
      "Target sequences combined:\n",
      "tensor([0, 1, 0])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate() function\n",
    "\n",
    "def test_collate_fn():\n",
    "    pairs = [\n",
    "        (torch.tensor([1, 2]), 0, 0),\n",
    "        (torch.tensor([6, 7, 8]), 1, 1),\n",
    "        (torch.tensor([11, 12, 13, 14]), 0, 2),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = collate(pairs)\n",
    "    assert pad_src_seqs.shape == torch.Size([4, 3]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    print('Source sequences combined:')\n",
    "    print(pad_src_seqs)\n",
    "    expected = torch.tensor([\n",
    "      [11, 6, 1],\n",
    "      [12, 7, 2],\n",
    "      [13, 8, 0],\n",
    "      [14, 0, 0],\n",
    "    ])\n",
    "    assert (pad_src_seqs == expected).all(), \"pad_src_seqs does not match expected values\"\n",
    "\n",
    "    print(src_seq_lengths)\n",
    "    if isinstance(src_seq_lengths[0], torch.Size):\n",
    "        src_seq_lengths = sum((list(l) for l in src_seq_lengths), [])\n",
    "    else:\n",
    "        src_seq_lengths = [int(l) for l in src_seq_lengths]\n",
    "    assert src_seq_lengths == [4, 3, 2], f\"Bad src_seq_lengths: {src_seq_lengths}\"\n",
    "\n",
    "    print('Target sequences combined:')\n",
    "    print(pad_tgt_seqs)\n",
    "    expected = torch.tensor([\n",
    "      0, 1, 0\n",
    "    ])\n",
    "    assert (pad_tgt_seqs == expected).all(), \"pad_tgt_seqs0 does not match expected values\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We create custom DataLoader using the implemented collate function\n",
    "# # We are going to process 64 sequences at the same time (batch_size=64)\n",
    "# trainset = TranslationDataset('train_2024.csv')\n",
    "# trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data loader\n",
    "# for i, (src_seqs, src_seq_lengths, tgt_seqs, ids) in enumerate(trainloader):\n",
    "#     print(f\"Batch {i} src_seqs:\")\n",
    "#     print(src_seqs)\n",
    "#     print(f'src_seqs.shape: {src_seqs.shape}')\n",
    "#     print(f\"Batch {i} src_seq_lengths:\")\n",
    "#     print(src_seq_lengths)\n",
    "#     print(f\"Batch {i} tgt_seqs:\")\n",
    "#     print(tgt_seqs)\n",
    "#     print(f'tgt_seqs.shape: {tgt_seqs.shape}')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self, src_dictionary_size, embed_size, hidden_size, dropout=0.2, glove_embeddings=None):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tsrc_dictionary_size: The number of words in the source dictionary.\n",
    "\t\tembed_size: The number of dimensions in the word embeddings.\n",
    "\t\thidden_size: The number of features in the hidden state of GRU.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(LSTM, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "\t\tif glove_embeddings is not None:\n",
    "\t\t\tself.load_glove_embeddings(glove_embeddings, embed_size)\n",
    "\t\n",
    "\t\tself.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=1, batch_first=False, dropout=dropout, bidirectional=False)\n",
    "\t\tself.fc1 = nn.Linear(hidden_size, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef load_glove_embeddings(self, glove_embeddings, embed_size):\n",
    "\t\t\"\"\"Initialize the embedding layer with GloVe embeddings.\"\"\"\n",
    "\t\tweights_matrix = torch.zeros((self.embedding.num_embeddings, embed_size))\n",
    "\n",
    "\t\tfor i, word in enumerate(glove_embeddings):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tweights_matrix[i] = torch.FloatTensor(glove_embeddings[word])\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(e)\n",
    "\t\t\t\tprint(torch.FloatTensor(glove_embeddings[word]).size())\n",
    "\t\t\t\tprint(f'word: {word}, i: {i}')\n",
    "\n",
    "\t\tself.embedding.load_state_dict({'weight': weights_matrix})\n",
    "\n",
    "\tdef forward(self, pad_seqs, seq_lengths, hidden):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tpad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "\t\tseq_lengths: List of sequence lengths.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\toutputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "\t\t\"\"\"\n",
    "\t\t# YOUR CODE HERE\n",
    "\t\tembedded = self.embedding(pad_seqs) # shape: (max_seq_length, batch_size, embed_size)\n",
    "\t\tpacked = pack_padded_sequence(embedded, seq_lengths)\n",
    "\t\toutputs, hidden = self.lstm(packed, hidden) \n",
    "\t\toutputs, output_lengths = pad_packed_sequence(outputs, batch_first=False) # shape: (max_seq_length, batch_size, hidden_size)\n",
    "\t\tlast_timesteps = torch.stack([outputs[length-1, i] for i, length in enumerate(output_lengths)]) # shape: (batch_size, hidden_size)\n",
    "\t\t# feed through the fully connected layer\n",
    "\t\toutputs = self.fc1(last_timesteps)\n",
    "\t\toutputs = self.dropout(outputs)\n",
    "\t\toutputs = self.relu(outputs)\n",
    "\t\toutputs = self.fc2(outputs)\n",
    "\t\toutputs = self.sigmoid(outputs)\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef init_hidden(self, batch_size=1, device='cpu'):\n",
    "\t\tnum_directions = 1\n",
    "\t\treturn (\n",
    "\t\t\ttorch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "\t\t\ttorch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, src_dictionary_size, embed_size, hidden_size, num_filters=100, kernel_sizes= [3, 4, 5], dropout=0.2, glove_embeddings=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        src_dictionary_size: The number of words in the source dictionary.\n",
    "        embed_size: The number of dimensions in the word embeddings.\n",
    "        hidden_size: The number of features in the hidden state of GRU.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "        if glove_embeddings is not None:\n",
    "            self.load_glove_embeddings(glove_embeddings, embed_size)\n",
    "\n",
    "        # CNN layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_size, out_channels=num_filters, kernel_size=k, stride=1, padding=k // 2)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.conv_output_size = num_filters * len(kernel_sizes)\n",
    "    \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=1, batch_first=False, dropout=dropout, bidirectional=False)\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def load_glove_embeddings(self, glove_embeddings, embed_size):\n",
    "        \"\"\"Initialize the embedding layer with GloVe embeddings.\"\"\"\n",
    "        weights_matrix = torch.zeros((self.embedding.num_embeddings, embed_size))\n",
    "\n",
    "        for i, word in enumerate(glove_embeddings):\n",
    "            try:\n",
    "                weights_matrix[i] = torch.FloatTensor(glove_embeddings[word])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(torch.FloatTensor(glove_embeddings[word]).size())\n",
    "                print(f'word: {word}, i: {i}')\n",
    "\n",
    "        self.embedding.load_state_dict({'weight': weights_matrix})\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        pad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "        seq_lengths: List of sequence lengths.\n",
    "        hidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "        Returns:\n",
    "        outputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "        hidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.embedding(pad_seqs) # shape: (max_seq_length, batch_size, embed_size)\n",
    "        \n",
    "        # reshape before feeding through the CNN layers\n",
    "        embedded = embedded.permute(1, 2, 0)  # shape: (batch_size, embed_size, max_seq_length)\n",
    "\n",
    "        # Apply CNN and ReLU activation\n",
    "        cnn_out = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        cnn_out = torch.cat(cnn_out, 1)  # concatenate along the channel dimension shape: (batch_size, num_filters * len(kernel_sizes), max_seq_length)\n",
    "        cnn_out = cnn_out.permute(2, 0, 1)  # shape: (max_seq_length, batch_size, conv_output_size)\n",
    "        embedded = cnn_out\n",
    "\n",
    "        # feed through the LSTM layer\n",
    "        packed = pack_padded_sequence(embedded, seq_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden) \n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=False) # shape: (max_seq_length, batch_size, hidden_size)\n",
    "        last_timesteps = torch.stack([outputs[length-1, i] for i, length in enumerate(output_lengths)]) # shape: (batch_size, hidden_size)\n",
    "        # feed through the fully connected layer\n",
    "        outputs = self.fc1(last_timesteps)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.fc2(outputs)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, batch_size=1, device='cpu'):\n",
    "        num_directions = 1\n",
    "        return (\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "def test_LSTM_shapes():\n",
    "    hidden_size = 3\n",
    "    lstm = LSTM(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = lstm.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])\n",
    "\n",
    "    outputs = lstm.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([batch_size, 1]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_LSTM_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, val_loader):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0\n",
    "\tcriterion = nn.BCELoss()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(val_loader):\n",
    "\t\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\t\thidden = model.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\t\toutputs = model(src_seqs, src_seq_lengths, hidden)\n",
    "\t\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\treturn total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 640 sequences at the same time (batch_size=640)\n",
    "# load vocab\n",
    "# vocab = torch.load('vocab.pth')\n",
    "# vocab = None\n",
    "# trainset = TranslationDataset('train_2024.csv', vocab=vocab)\n",
    "trainset = torch.load('dataloaders/trainset.pth')\n",
    "print(len(trainset.text))\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    }
   ],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "# valset = TranslationDataset('dev_2024.csv', vocab=trainset.vocab)\n",
    "valset = torch.load('dataloaders/valset.pth')\n",
    "print(len(valset.text))\n",
    "valloader = DataLoader(dataset=valset, batch_size=256, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trainset\n",
    "# torch.save(trainset, 'trainset.pth')\n",
    "\n",
    "# save valset\n",
    "# torch.save(valset, 'valset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          patience (int):    Maximum number of epochs with unsuccessful updates.\n",
    "          tolerance (float): We assume that the update is unsuccessful if the validation error is larger\n",
    "                              than the best validation error so far plus this tolerance.\n",
    "        \"\"\"\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    \n",
    "    def stop_criterion(self, val_errors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          val_errors (iterable): Validation errors after every update during training.\n",
    "        \n",
    "        Returns: True if training should be stopped: when the validation error is larger than the best\n",
    "                  validation error obtained so far (with given tolearance) for patience epochs (number of consecutive epochs for which the criterion is satisfied).\n",
    "                 \n",
    "                 Otherwise, False.\n",
    "        \"\"\"\n",
    "        if len(val_errors) <= self.patience:\n",
    "            return False\n",
    "\n",
    "        min_val_error = min(val_errors)\n",
    "        val_errors = np.array(val_errors[-self.patience:])\n",
    "        return all(val_errors > min_val_error + self.tolerance)\n",
    "\n",
    "early_stop = EarlyStopping(tolerance=0.001, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM model\n",
    "hidden_size = embed_size = 256\n",
    "lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "# Load pretrained LSTM\n",
    "# lstm.load_state_dict(torch.load('lstm_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.3508, Time spent: 2.22s\n",
      "Epoch 1, iter 20: avg. loss = 0.3380, Time spent: 2.27s\n",
      "Epoch 1, iter 30: avg. loss = 0.3257, Time spent: 3.42s\n",
      "Epoch 1, iter 40: avg. loss = 0.3182, Time spent: 2.18s\n",
      "Epoch 1, iter 50: avg. loss = 0.3095, Time spent: 2.19s\n",
      "Epoch 1, iter 60: avg. loss = 0.3007, Time spent: 2.29s\n",
      "Epoch 1, iter 70: avg. loss = 0.2944, Time spent: 2.21s\n",
      "Epoch 1, iter 80: avg. loss = 0.2888, Time spent: 2.17s\n",
      "Epoch 1, iter 90: avg. loss = 0.2835, Time spent: 4.14s\n",
      "Epoch 1, iter 100: avg. loss = 0.2781, Time spent: 2.28s\n",
      "Epoch 1, iter 110: avg. loss = 0.2745, Time spent: 2.29s\n",
      "Epoch 1, iter 120: avg. loss = 0.2728, Time spent: 2.21s\n",
      "Epoch 1, iter 130: avg. loss = 0.2700, Time spent: 2.54s\n",
      "Epoch 1, iter 140: avg. loss = 0.2675, Time spent: 2.25s\n",
      "Epoch 1, iter 150: avg. loss = 0.2652, Time spent: 2.32s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.2042\n",
      "Epoch 1, val loss = 0.2042, train loss = 0.2637; Time spent: 382.34s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.2239, Time spent: 2.22s\n",
      "Epoch 2, iter 20: avg. loss = 0.2218, Time spent: 2.27s\n",
      "Epoch 2, iter 30: avg. loss = 0.2166, Time spent: 3.42s\n",
      "Epoch 2, iter 40: avg. loss = 0.2139, Time spent: 2.18s\n",
      "Epoch 2, iter 50: avg. loss = 0.2094, Time spent: 2.20s\n",
      "Epoch 2, iter 60: avg. loss = 0.2052, Time spent: 2.30s\n",
      "Epoch 2, iter 70: avg. loss = 0.2030, Time spent: 2.21s\n",
      "Epoch 2, iter 80: avg. loss = 0.1999, Time spent: 2.18s\n",
      "Epoch 2, iter 90: avg. loss = 0.1974, Time spent: 4.12s\n",
      "Epoch 2, iter 100: avg. loss = 0.1955, Time spent: 2.28s\n",
      "Epoch 2, iter 110: avg. loss = 0.1947, Time spent: 2.29s\n",
      "Epoch 2, iter 120: avg. loss = 0.1938, Time spent: 2.20s\n",
      "Epoch 2, iter 130: avg. loss = 0.1938, Time spent: 2.53s\n",
      "Epoch 2, iter 140: avg. loss = 0.1935, Time spent: 2.25s\n",
      "Epoch 2, iter 150: avg. loss = 0.1934, Time spent: 2.32s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.1623\n",
      "Epoch 2, val loss = 0.1623, train loss = 0.1928; Time spent: 382.62s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.1836, Time spent: 2.22s\n",
      "Epoch 3, iter 20: avg. loss = 0.1844, Time spent: 2.27s\n",
      "Epoch 3, iter 30: avg. loss = 0.1819, Time spent: 3.42s\n",
      "Epoch 3, iter 40: avg. loss = 0.1809, Time spent: 2.19s\n",
      "Epoch 3, iter 50: avg. loss = 0.1776, Time spent: 2.20s\n",
      "Epoch 3, iter 60: avg. loss = 0.1747, Time spent: 2.31s\n",
      "Epoch 3, iter 70: avg. loss = 0.1724, Time spent: 2.21s\n",
      "Epoch 3, iter 80: avg. loss = 0.1695, Time spent: 2.19s\n",
      "Epoch 3, iter 90: avg. loss = 0.1669, Time spent: 4.15s\n",
      "Epoch 3, iter 100: avg. loss = 0.1655, Time spent: 2.29s\n",
      "Epoch 3, iter 110: avg. loss = 0.1652, Time spent: 2.29s\n",
      "Epoch 3, iter 120: avg. loss = 0.1641, Time spent: 2.21s\n",
      "Epoch 3, iter 130: avg. loss = 0.1637, Time spent: 2.55s\n",
      "Epoch 3, iter 140: avg. loss = 0.1627, Time spent: 2.25s\n",
      "Epoch 3, iter 150: avg. loss = 0.1624, Time spent: 2.34s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.1312\n",
      "Epoch 3, val loss = 0.1312, train loss = 0.1618; Time spent: 385.12s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.1483, Time spent: 2.22s\n",
      "Epoch 4, iter 20: avg. loss = 0.1503, Time spent: 2.28s\n",
      "Epoch 4, iter 30: avg. loss = 0.1480, Time spent: 3.41s\n",
      "Epoch 4, iter 40: avg. loss = 0.1460, Time spent: 2.18s\n",
      "Epoch 4, iter 50: avg. loss = 0.1438, Time spent: 2.21s\n",
      "Epoch 4, iter 60: avg. loss = 0.1416, Time spent: 2.30s\n",
      "Epoch 4, iter 70: avg. loss = 0.1398, Time spent: 2.21s\n",
      "Epoch 4, iter 80: avg. loss = 0.1377, Time spent: 2.19s\n",
      "Epoch 4, iter 90: avg. loss = 0.1368, Time spent: 4.15s\n",
      "Epoch 4, iter 100: avg. loss = 0.1368, Time spent: 2.28s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\aalto-snlp-project-spring-2024\\rnn.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#Y111sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m lstm(src_seqs, src_seq_lengths, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#Y111sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), tgt_labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#Y111sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#Y111sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#Y111sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 20\n",
    "# prompt ask for continue training or not\n",
    "cont = input('Continue training? yes or no')\n",
    "if cont == 'no':\n",
    "\tprint('Fresh training')\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "else:\n",
    "\tprint(f'Continue training')\n",
    "\t\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm, valloader)\n",
    "\tif eval_loss < best_val_loss:\n",
    "\t\tbest_val_loss = eval_loss\n",
    "\t\tbest_model = lstm.state_dict()\n",
    "\t\tif best_model is not None:\n",
    "\t\t\tprint(f'find new best model, save to lstm.pth, eval_loss: {eval_loss:.4f}')\n",
    "\t\t\ttorch.save(best_model, 'models/lstm.pth')\n",
    "\n",
    "\tif early_stop.stop_criterion(val_losses):\n",
    "\t\tprint(f'Early stopping on epoch {epoch + 1}')\n",
    "\t\tbreak\n",
    "\t\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and vocab\n",
    "torch.save(lstm.state_dict(), 'lstm.pth')\n",
    "torch.save(trainset.vocab, 'vocab.pth')\n",
    "\n",
    "# # Load model\n",
    "# lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "# lstm.load_state_dict(torch.load('lstm.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a function to load GloVe embeddings\n",
    "def load_glove_embeddings(filepath):\n",
    "    glove_embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_embeddings[word] = vector\n",
    "    return glove_embeddings\n",
    "\n",
    "# Load GloVe embeddings from a file\n",
    "glove_embeddings = load_glove_embeddings('glove_embeddings/glove.twitter.27B.100d.txt')\n",
    "\n",
    "# Create a dictionary with your src_dictionary and GloVe embeddings\n",
    "src_dictionary = trainset.vocab  # Add your vocabulary here\n",
    "src_dictionary_size = len(src_dictionary)\n",
    "embed_size = 100  # Example embedding size\n",
    "\n",
    "# Map GloVe vectors to your src_dictionary index\n",
    "mapped_glove_embeddings = {}\n",
    "for word, index in src_dictionary.get_stoi().items():\n",
    "    if word in glove_embeddings:\n",
    "        mapped_glove_embeddings[index] = glove_embeddings[word]\n",
    "    else:\n",
    "        # If word is not in GloVe, initialize a random vector\n",
    "        mapped_glove_embeddings[index] = np.random.normal(scale=0.6, size=(embed_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save mapped_glove_embeddings\n",
      "load mapped_glove_embeddings\n"
     ]
    }
   ],
   "source": [
    "# save and load mapped_glove_embeddings\n",
    "# print('save mapped_glove_embeddings')\n",
    "# torch.save(mapped_glove_embeddings, 'models/mapped_glove_embeddings.pth')\n",
    "print('load mapped_glove_embeddings')\n",
    "mapped_glove_embeddings = torch.load('models/mapped_glove_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "embed_size = 100\n",
    "lstm_glove = LSTM(trainset.vocab_size, embed_size, hidden_size, dropout=0.2, glove_embeddings=mapped_glove_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh training\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.6676, Time spent: 2.21s\n",
      "Epoch 1, iter 20: avg. loss = 0.6592, Time spent: 2.26s\n",
      "Epoch 1, iter 30: avg. loss = 0.6570, Time spent: 3.43s\n",
      "Epoch 1, iter 40: avg. loss = 0.6505, Time spent: 2.19s\n",
      "Epoch 1, iter 50: avg. loss = 0.6440, Time spent: 2.19s\n",
      "Epoch 1, iter 60: avg. loss = 0.6357, Time spent: 2.28s\n",
      "Epoch 1, iter 70: avg. loss = 0.6256, Time spent: 2.21s\n",
      "Epoch 1, iter 80: avg. loss = 0.6161, Time spent: 2.17s\n",
      "Epoch 1, iter 90: avg. loss = 0.6049, Time spent: 4.13s\n",
      "Epoch 1, iter 100: avg. loss = 0.5934, Time spent: 2.27s\n",
      "Epoch 1, iter 110: avg. loss = 0.5801, Time spent: 2.28s\n",
      "Epoch 1, iter 120: avg. loss = 0.5691, Time spent: 2.21s\n",
      "Epoch 1, iter 130: avg. loss = 0.5638, Time spent: 2.53s\n",
      "Epoch 1, iter 140: avg. loss = 0.5520, Time spent: 2.25s\n",
      "Epoch 1, iter 150: avg. loss = 0.5383, Time spent: 2.33s\n",
      "find new best model, save to models/lstm_glove.pth, eval_loss: 0.3278\n",
      "Epoch 1, val loss = 0.3278, train loss = 0.5318; Time spent: 369.93s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.3042, Time spent: 2.21s\n",
      "Epoch 2, iter 20: avg. loss = 0.3221, Time spent: 2.26s\n",
      "Epoch 2, iter 30: avg. loss = 0.3140, Time spent: 3.41s\n",
      "Epoch 2, iter 40: avg. loss = 0.3103, Time spent: 2.17s\n",
      "Epoch 2, iter 50: avg. loss = 0.3023, Time spent: 2.19s\n",
      "Epoch 2, iter 60: avg. loss = 0.2950, Time spent: 2.29s\n",
      "Epoch 2, iter 70: avg. loss = 0.2885, Time spent: 2.21s\n",
      "Epoch 2, iter 80: avg. loss = 0.2817, Time spent: 2.18s\n",
      "Epoch 2, iter 90: avg. loss = 0.2755, Time spent: 4.12s\n",
      "Epoch 2, iter 100: avg. loss = 0.2705, Time spent: 2.28s\n",
      "Epoch 2, iter 110: avg. loss = 0.2658, Time spent: 2.28s\n",
      "Epoch 2, iter 120: avg. loss = 0.2637, Time spent: 2.20s\n",
      "Epoch 2, iter 130: avg. loss = 0.2614, Time spent: 2.54s\n",
      "Epoch 2, iter 140: avg. loss = 0.2576, Time spent: 2.25s\n",
      "Epoch 2, iter 150: avg. loss = 0.2542, Time spent: 2.32s\n",
      "find new best model, save to models/lstm_glove.pth, eval_loss: 0.2054\n",
      "Epoch 2, val loss = 0.2054, train loss = 0.2522; Time spent: 369.84s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.1971, Time spent: 2.21s\n",
      "Epoch 3, iter 20: avg. loss = 0.1986, Time spent: 2.26s\n",
      "Epoch 3, iter 30: avg. loss = 0.1989, Time spent: 3.41s\n",
      "Epoch 3, iter 40: avg. loss = 0.2021, Time spent: 2.18s\n",
      "Epoch 3, iter 50: avg. loss = 0.2000, Time spent: 2.20s\n",
      "Epoch 3, iter 60: avg. loss = 0.1983, Time spent: 2.29s\n",
      "Epoch 3, iter 70: avg. loss = 0.1958, Time spent: 2.22s\n",
      "Epoch 3, iter 80: avg. loss = 0.1926, Time spent: 2.18s\n",
      "Epoch 3, iter 90: avg. loss = 0.1899, Time spent: 4.12s\n",
      "Epoch 3, iter 100: avg. loss = 0.1884, Time spent: 2.28s\n",
      "Epoch 3, iter 110: avg. loss = 0.1872, Time spent: 2.28s\n",
      "Epoch 3, iter 120: avg. loss = 0.1872, Time spent: 2.20s\n",
      "Epoch 3, iter 130: avg. loss = 0.1874, Time spent: 2.53s\n",
      "Epoch 3, iter 140: avg. loss = 0.1867, Time spent: 2.25s\n",
      "Epoch 3, iter 150: avg. loss = 0.1858, Time spent: 2.32s\n",
      "find new best model, save to models/lstm_glove.pth, eval_loss: 0.1931\n",
      "Epoch 3, val loss = 0.1931, train loss = 0.1850; Time spent: 369.51s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.1682, Time spent: 2.22s\n",
      "Epoch 4, iter 20: avg. loss = 0.1687, Time spent: 2.26s\n",
      "Epoch 4, iter 30: avg. loss = 0.1684, Time spent: 3.41s\n",
      "Epoch 4, iter 40: avg. loss = 0.1696, Time spent: 2.18s\n",
      "Epoch 4, iter 50: avg. loss = 0.1679, Time spent: 2.19s\n",
      "Epoch 4, iter 60: avg. loss = 0.1670, Time spent: 2.29s\n",
      "Epoch 4, iter 70: avg. loss = 0.1652, Time spent: 2.21s\n",
      "Epoch 4, iter 80: avg. loss = 0.1636, Time spent: 2.17s\n",
      "Epoch 4, iter 90: avg. loss = 0.1615, Time spent: 4.13s\n",
      "Epoch 4, iter 100: avg. loss = 0.1608, Time spent: 2.28s\n",
      "Epoch 4, iter 110: avg. loss = 0.1605, Time spent: 2.28s\n",
      "Epoch 4, iter 120: avg. loss = 0.1606, Time spent: 2.21s\n",
      "Epoch 4, iter 130: avg. loss = 0.1621, Time spent: 2.54s\n",
      "Epoch 4, iter 140: avg. loss = 0.1621, Time spent: 2.25s\n",
      "Epoch 4, iter 150: avg. loss = 0.1618, Time spent: 2.32s\n",
      "find new best model, save to models/lstm_glove.pth, eval_loss: 0.1843\n",
      "Epoch 4, val loss = 0.1843, train loss = 0.1611; Time spent: 369.71s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.1513, Time spent: 2.21s\n",
      "Epoch 5, iter 20: avg. loss = 0.1534, Time spent: 2.26s\n",
      "Epoch 5, iter 30: avg. loss = 0.1549, Time spent: 3.41s\n",
      "Epoch 5, iter 40: avg. loss = 0.1568, Time spent: 2.18s\n",
      "Epoch 5, iter 50: avg. loss = 0.1551, Time spent: 2.19s\n",
      "Epoch 5, iter 60: avg. loss = 0.1545, Time spent: 2.28s\n",
      "Epoch 5, iter 70: avg. loss = 0.1524, Time spent: 2.21s\n",
      "Epoch 5, iter 80: avg. loss = 0.1508, Time spent: 2.18s\n",
      "Epoch 5, iter 90: avg. loss = 0.1489, Time spent: 4.13s\n",
      "Epoch 5, iter 100: avg. loss = 0.1479, Time spent: 2.28s\n",
      "Epoch 5, iter 110: avg. loss = 0.1474, Time spent: 2.29s\n",
      "Epoch 5, iter 120: avg. loss = 0.1466, Time spent: 2.21s\n",
      "Epoch 5, iter 130: avg. loss = 0.1475, Time spent: 2.54s\n",
      "Epoch 5, iter 140: avg. loss = 0.1470, Time spent: 2.24s\n",
      "Epoch 5, iter 150: avg. loss = 0.1470, Time spent: 2.33s\n",
      "Epoch 5, val loss = 0.1861, train loss = 0.1468; Time spent: 370.06s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 6, iter 10: avg. loss = 0.1431, Time spent: 2.21s\n",
      "Epoch 6, iter 20: avg. loss = 0.1477, Time spent: 2.26s\n",
      "Epoch 6, iter 30: avg. loss = 0.1487, Time spent: 3.41s\n",
      "Epoch 6, iter 40: avg. loss = 0.1464, Time spent: 2.17s\n",
      "Epoch 6, iter 50: avg. loss = 0.1428, Time spent: 2.18s\n",
      "Epoch 6, iter 60: avg. loss = 0.1410, Time spent: 2.28s\n",
      "Epoch 6, iter 70: avg. loss = 0.1402, Time spent: 2.21s\n",
      "Epoch 6, iter 80: avg. loss = 0.1399, Time spent: 2.17s\n",
      "Epoch 6, iter 90: avg. loss = 0.1386, Time spent: 4.12s\n",
      "Epoch 6, iter 100: avg. loss = 0.1381, Time spent: 2.27s\n",
      "Epoch 6, iter 110: avg. loss = 0.1380, Time spent: 2.29s\n",
      "Epoch 6, iter 120: avg. loss = 0.1375, Time spent: 2.21s\n",
      "Epoch 6, iter 130: avg. loss = 0.1380, Time spent: 2.54s\n",
      "Epoch 6, iter 140: avg. loss = 0.1377, Time spent: 2.23s\n",
      "Epoch 6, iter 150: avg. loss = 0.1376, Time spent: 2.33s\n",
      "Epoch 6, val loss = 0.1869, train loss = 0.1372; Time spent: 368.61s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 7, iter 10: avg. loss = 0.1327, Time spent: 2.21s\n",
      "Epoch 7, iter 20: avg. loss = 0.1347, Time spent: 2.26s\n",
      "Epoch 7, iter 30: avg. loss = 0.1334, Time spent: 3.41s\n",
      "Epoch 7, iter 40: avg. loss = 0.1319, Time spent: 2.18s\n",
      "Epoch 7, iter 50: avg. loss = 0.1295, Time spent: 2.19s\n",
      "Epoch 7, iter 60: avg. loss = 0.1276, Time spent: 2.28s\n",
      "Epoch 7, iter 70: avg. loss = 0.1274, Time spent: 2.21s\n",
      "Epoch 7, iter 80: avg. loss = 0.1277, Time spent: 2.17s\n",
      "Epoch 7, iter 90: avg. loss = 0.1264, Time spent: 4.13s\n",
      "Epoch 7, iter 100: avg. loss = 0.1268, Time spent: 2.28s\n",
      "Epoch 7, iter 110: avg. loss = 0.1274, Time spent: 2.29s\n",
      "Epoch 7, iter 120: avg. loss = 0.1269, Time spent: 2.20s\n",
      "Epoch 7, iter 130: avg. loss = 0.1276, Time spent: 2.54s\n",
      "Epoch 7, iter 140: avg. loss = 0.1275, Time spent: 2.23s\n",
      "Epoch 7, iter 150: avg. loss = 0.1277, Time spent: 2.33s\n",
      "Epoch 7, val loss = 0.1848, train loss = 0.1276; Time spent: 368.51s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 8, iter 10: avg. loss = 0.1234, Time spent: 2.21s\n",
      "Epoch 8, iter 20: avg. loss = 0.1260, Time spent: 2.26s\n",
      "Epoch 8, iter 30: avg. loss = 0.1251, Time spent: 3.42s\n",
      "Epoch 8, iter 40: avg. loss = 0.1245, Time spent: 2.18s\n",
      "Epoch 8, iter 50: avg. loss = 0.1226, Time spent: 2.19s\n",
      "Epoch 8, iter 60: avg. loss = 0.1208, Time spent: 2.28s\n",
      "Epoch 8, iter 70: avg. loss = 0.1193, Time spent: 2.20s\n",
      "Epoch 8, iter 80: avg. loss = 0.1189, Time spent: 2.17s\n",
      "Epoch 8, iter 90: avg. loss = 0.1179, Time spent: 4.12s\n",
      "Epoch 8, iter 100: avg. loss = 0.1186, Time spent: 2.28s\n",
      "Epoch 8, iter 110: avg. loss = 0.1195, Time spent: 2.28s\n",
      "Epoch 8, iter 120: avg. loss = 0.1196, Time spent: 2.19s\n",
      "Epoch 8, iter 130: avg. loss = 0.1204, Time spent: 2.52s\n",
      "Epoch 8, iter 140: avg. loss = 0.1204, Time spent: 2.25s\n",
      "Epoch 8, iter 150: avg. loss = 0.1206, Time spent: 2.31s\n",
      "Epoch 8, val loss = 0.1898, train loss = 0.1204; Time spent: 368.67s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 9, iter 10: avg. loss = 0.1193, Time spent: 2.21s\n",
      "Epoch 9, iter 20: avg. loss = 0.1198, Time spent: 2.25s\n",
      "Epoch 9, iter 30: avg. loss = 0.1178, Time spent: 3.41s\n",
      "Epoch 9, iter 40: avg. loss = 0.1167, Time spent: 2.17s\n",
      "Epoch 9, iter 50: avg. loss = 0.1149, Time spent: 2.18s\n",
      "Epoch 9, iter 60: avg. loss = 0.1136, Time spent: 2.28s\n",
      "Epoch 9, iter 70: avg. loss = 0.1140, Time spent: 2.20s\n",
      "Epoch 9, iter 80: avg. loss = 0.1138, Time spent: 2.17s\n",
      "Epoch 9, iter 90: avg. loss = 0.1134, Time spent: 4.12s\n",
      "Epoch 9, iter 100: avg. loss = 0.1139, Time spent: 2.28s\n",
      "Epoch 9, iter 110: avg. loss = 0.1142, Time spent: 2.27s\n",
      "Epoch 9, iter 120: avg. loss = 0.1144, Time spent: 2.20s\n",
      "Epoch 9, iter 130: avg. loss = 0.1151, Time spent: 2.54s\n",
      "Epoch 9, iter 140: avg. loss = 0.1151, Time spent: 2.24s\n",
      "Epoch 9, iter 150: avg. loss = 0.1161, Time spent: 2.33s\n",
      "Epoch 9, val loss = 0.2005, train loss = 0.1162; Time spent: 368.10s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 10, iter 10: avg. loss = 0.1205, Time spent: 2.21s\n",
      "Epoch 10, iter 20: avg. loss = 0.1214, Time spent: 2.26s\n",
      "Epoch 10, iter 30: avg. loss = 0.1199, Time spent: 3.41s\n",
      "Epoch 10, iter 40: avg. loss = 0.1183, Time spent: 2.17s\n",
      "Epoch 10, iter 50: avg. loss = 0.1158, Time spent: 2.19s\n",
      "Epoch 10, iter 60: avg. loss = 0.1145, Time spent: 2.28s\n",
      "Epoch 10, iter 70: avg. loss = 0.1168, Time spent: 2.21s\n",
      "Epoch 10, iter 80: avg. loss = 0.1171, Time spent: 2.17s\n",
      "Epoch 10, iter 90: avg. loss = 0.1180, Time spent: 4.12s\n",
      "Epoch 10, iter 100: avg. loss = 0.1199, Time spent: 2.27s\n",
      "Epoch 10, iter 110: avg. loss = 0.1201, Time spent: 2.28s\n",
      "Epoch 10, iter 120: avg. loss = 0.1191, Time spent: 2.19s\n",
      "Epoch 10, iter 130: avg. loss = 0.1190, Time spent: 2.53s\n",
      "Epoch 10, iter 140: avg. loss = 0.1186, Time spent: 2.24s\n",
      "Epoch 10, iter 150: avg. loss = 0.1183, Time spent: 2.32s\n",
      "Epoch 10, val loss = 0.1989, train loss = 0.1183; Time spent: 368.33s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 11, iter 10: avg. loss = 0.1163, Time spent: 2.21s\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_glove.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "n_epochs = 40\n",
    "# prompt ask for continue training or not\n",
    "cont = input('Continue training? yes or no')\n",
    "if cont == 'no':\n",
    "\tprint('Fresh training')\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "else:\n",
    "\tprint(f'Continue training')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm_glove.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm_glove.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm_glove(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm_glove, valloader)\n",
    "\n",
    "\tif eval_loss < best_val_loss:\n",
    "\t\tbest_val_loss = eval_loss\n",
    "\t\tbest_model = lstm_glove.state_dict()\n",
    "\t\tif best_model is not None:\n",
    "\t\t\tprint(f'find new best model, save to models/lstm_glove.pth, eval_loss: {eval_loss:.4f}')\n",
    "\t\t\ttorch.save(best_model, os.path.join('models', 'lstm_glove.pth'))\n",
    "\n",
    "\t# if early_stop.stop_criterion(val_losses):\n",
    "\t# \tprint(f'Early stopping at epoch {epoch + 1}')\n",
    "\t# \tbreak\n",
    "\t\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\aalto-snlp-project-spring-2024\\rnn.ipynb Cell 34\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# plot learning curve\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(train_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(val_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/aalto-snlp-project-spring-2024/rnn.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "# plot learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vanilla LSTM\n",
    "# hidden_size = embed_size = 256\n",
    "# lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "# lstm.load_state_dict(torch.load('lstm.pth'))\n",
    "\n",
    "# LSTM with GloVe embeddings\n",
    "hidden_size = 256\n",
    "embed_size = 100\n",
    "lstm_glove = LSTM(trainset.vocab_size, embed_size, hidden_size, dropout=0.2, glove_embeddings=None).to(device)\n",
    "lstm_glove.load_state_dict(torch.load('models/lstm_glove.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(lstm, pad_src_seqs, src_seq_lengths):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "\n",
    "    Args:\n",
    "    lstm (LSTM): Trained lstm.\n",
    "    pad_src_seqs of shape (max_src_seq_length, batch_size): Padded source sequences.\n",
    "    src_seq_lengths: List of source sequence lengths.\n",
    "\n",
    "    Returns:\n",
    "    out_seqs of shape (batch_size, 1): LongTensor of word indices of the output sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        pad_src_seqs = pad_src_seqs.to(device)\n",
    "        lstm_hidden = lstm.init_hidden(pad_src_seqs.shape[1], device)\n",
    "        outputs = lstm(pad_src_seqs, src_seq_lengths, lstm_hidden)\n",
    "        out_seqs = outputs > 0.5\n",
    "        return out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes(lstm):\n",
    "    pad_src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths=[4, 2])\n",
    "    assert out_seqs.shape == torch.Size([2, 1]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes(lstm_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify training data:\n",
      "-----------------------------\n",
      "SRC: ['the', 'benign', 'effect', 'of', 'the', 'financial', 'crisis', 'in', 'canada', 'that', 'reduce', 'consumer', 'borrowing', 'in', 'the', 'us', 'increase', 'it', 'here', '.', 'low', 'rate', 'but', 'a', 'healthy', 'gap', 'between', 'funding', 'level', 'for', 'bank', 'and', 'prime', 'rate', 'mean', 'huge', 'profit', 'form', 'retail', '.', 'the', 'bank', 'push', 'we', 'all', 'to', 'borrow', 'with', 'slogan', 'like', 'you', 'be', 'wealthy', 'than', 'you', 'think', '.', 'I', 'once', 'hear', 'a', 'bank', 'treasurer', 'imply', 'these', 'spread', 'represent', 'over', 'of', 'their', 'profit', 'the', 'prior', 'year', '.', 'and', 'so', 'it', 'be', 'ironic', 'that', 'the', 'bank', 'be', 'now', 'among', 'those', 'heed', 'the', 'debt', 'level', 'warning', '.', 'there', 'can', 'be', 'no', 'explanation', 'other', 'than', 'that', 'they', 'can', 'not', 'wean', 'themselves', 'off', 'it', 'and', 'need', 'osfi', 'to', 'help', '.', 'it', 'be', 'game', 'theory', 'run', 'amok', 'when', 'an', 'oligopoly', 'need', 'someone', 'outside', 'the', 'game', 'to', 'impel', 'they', 'to', 'change', 'the', 'way', 'they', 'play', '.', 'with', 'house', 'price', 'up', 'as', 'much', 'as', 'they', 'be', 'over', 'the', 'year', 'even', 'those', 'that', 'in', 'a', 'basis', 'point', 'rise', 'may', 'have', 'a', 'little', 'wiggle', 'room', '.', 'I', 'note', 'that', 'no', 'one', 'be', 'estimate', 'the', 'price', 'sensitivity', 'of', 'the', 'real', 'estate', 'market', 'to', 'the', 'move', '.', 'that', 'would', 'be', 'a', 'good', 'piece', 'of', 'work', '.', '<eos>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['thank', 'you', '<unk>', 'for', 'your', 'wise', 'word', '.', 'you', 'be', 'just', 'a', 'little', 'young', 'than', 'my', 'parent', 'bear', 'in', 'and', '.', 'yes', 'I', 'agree', 'with', 'your', 'statement', 'if', 'die', 'be', 'just', 'a', 'part', 'of', 'the', 'process', 'of', 'life', 'and', 'that', 'be', 'the', 'way', 'god', 'arrange', 'it', 'it', 'must', 'somehow', 'be', 'a', 'good', 'thing', '.', 'but', 'another', 'aspect', 'of', 'death', 'be', 'how', 'it', 'affect', 'our', 'relationship', 'with', 'other', 'living', 'creature', '.', 'we', 'be', 'so', 'easily', 'make', 'to', 'overlook', 'our', 'complicity', 'in', 'the', 'death', 'of', 'other', 'who', 'have', 'as', 'much', 'right', 'to', 'live', 'as', 'we', 'do', 'it', 'be', 'the', 'way', 'of', 'wisdom', 'to', 'pay', 'attention', 'to', 'the', 'life', 'of', 'these', 'countless', 'cousin', 'of', 'ours', 'and', 'never', 'do', 'anything', 'to', 'make', 'their', 'death', 'more', 'painful', 'than', 'they', 'be', '.', 'so', 'on', 'the', 'matter', 'of', 'assisted', 'suicide', 'I', 'be', 'confident', 'that', 'sick', 'people', 'with', 'no', 'hope', 'of', 'live', 'much', 'long', 'do', 'a', 'good', 'thing', 'to', 'spare', 'themselves', 'more', 'pain', 'and', 'their', 'love', 'one', 'inconvenience', '.', 'and', 'those', 'who', 'assist', 'they', 'also', 'do', 'a', 'good', 'thing', 'by', 'be', 'their', 'minister', '.', 'it', 's', 'much', 'well', 'than', 'the', 'unreal', 'inhumane', 'demand', 'of', 'modern', 'medical', 'life', 'save', 'practice', '.', '<eos>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['to', 'all', 'you', 'idiotic', 'naysayer', 'ok', 'let', 's', 'play', '.', '.', '.', 'say', 'climate', 'change', 'isn', 't', 'be', 'cause', 'by', 'stupid', 'human', 'activity', '<unk>', 'do', 'it', 'not', 'behoove', 'we', 'to', 'do', 'everything', 'we', 'can', 'to', 'mitigate', 'any', 'possibility', 'that', 'it', 'might', 'people', '?', 'I', 'm', 'not', 'say', 'we', 'can', 'fix', 'it', 'overnight', 'but', 'we', 're', 'one', 'of', 'the', 'early', 'generation', 'of', 'a', 'huge', 'industrial', 'revolution', 'and', 'can', 'set', 'the', 'tone', 'for', 'environmental', 'stewardship', 'for', 'future', 'one', '.', 'or', 'be', 'all', 'you', 'lazy', 'minded', 'moron', 'content', 'that', 'we', 'can', 'just', 'happily', 'continue', 'to', 'pollute', 'the', 'atmosphere', 'greedily', 'mine', 'and', 'drill', 'massively', 'over', 'fish', 'the', 'ocean', 'and', 'dump', 'million', 'of', 'metric', 'ton', 'of', 'trash', 'into', 'it', 'and', 'landfill', 'on', 'a', 'daily', 'basis', '?', 'we', 're', 'already', 'tragically', 'late', '!', 'ever', 'hear', 'the', 'saying', 'leave', 'a', 'place', 'in', 'well', 'condition', 'than', 'the', 'way', 'you', 'find', 'it', '.', 'you', 're', 'the', 'type', 'that', 'go', 'to', 'house', 'party', 'and', 'don', 't', 'stick', 'around', 'to', 'help', 'clean', 'up', 'your', 'mess', '.', 'your', 'unbelievably', 'stupid', 'selfish', 'and', 'dismissive', 'attitude', 'towards', 'the', 'environment', 'and', 'even', 'your', 'own', 'family', 's', 'future', 'be', 'just', 'beyond', 'stunning', '.', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 1\n",
      "OUT: True\n",
      "\n",
      "SRC: ['oh', 'bullshit', '.', 'go', 'look', 'at', 'the', 'most', 'gender', 'equal', 'country', 'on', 'earth', 'that', 'would', 'be', 'iceland', 'and', 'you', 'll', 'find', 'that', 'the', '<unk>', 'have', 'not', 'be', 'replace', 'by', 'the', 'state', 'it', 'have', 'not', 'be', 'replace', 'by', 'anything', '.', 'people', 'just', 'fail', 'to', 'expect', 'anything', 'from', 'each', 'other', 'base', 'on', 'gender', 'and', 'double', 'standard', 'don', 't', 'exist', '.', 'and', 'contrary', 'to', 'stupid', 'claim', 'elsewhere', 'in', 'the', 'comment', 'guess', 'what', 'else', 'have', 'cease', 'to', 'exist', 'any', 'kind', 'of', '<unk>', '.', 'the', 'big', 'downside', 'iceland', 'have', 'face', 'be', 'that', 'not', 'have', 'any', 'double', 'standard', 'or', 'judgement', 'of', 'people', 's', 'behaviour', 'their', 'enjoyment', 'of', 'daily', 'casual', 'sex', 'have', 'give', 'they', 'a', 'rather', 'high', 'than', 'average', 'rate', 'of', '<unk>', 'know', 'locally', 'as', 'the', '<unk>', 'handshake', 'one', 'of', 'the', 'few', 'stds', 'you', 'can', 'fairly', 'easily', 'catch', 'even', 'when', 'use', 'condom', '.', 'but', 'then', 'it', 'be', 'also', 'one', 'of', 'the', 'easy', 'to', 'cure', 'and', 'with', 'an', 'excellent', 'free', 'for', 'all', 'healthcare', 'system', 'it', 's', 'really', 'not', 'a', 'major', 'problem', '.', 'if', 'that', 's', 'the', 'big', 'price', 'for', 'a', 'society', 'where', 'rape', 'be', 'basically', 'non', 'existent', 'I', 'would', 'pay', 'it', 'with', 'glee', '.', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 1\n",
      "OUT: True\n",
      "\n",
      "SRC: ['these', 'poor', 'people', 'a', 'pawn', 'in', 'a', 'cruel', 'political', 'game', '.', 'obama', 'open', 'the', 'door', 'invite', 'their', 'parent', 'in', 'by', 'promise', 'to', 'make', 'their', 'lawbreaking', 'legal', 'and', 'later', 'reward', 'their', 'child', 'by', 'make', 'dream', 'act', 'previously', 'reject', 'by', 'congress', 'into', 'executive', 'order', 'daca', '.', 'he', 'dem', 'have', 'nothing', 'to', 'lose', 'in', 'this', 'cruel', 'game', 'that', 'have', 'nothing', 'to', 'do', 'with', 'compassion', 'and', 'here', 's', 'why', '.', 'if', 'hillary', 'would', 've', 'be', 'elect', 'she', 'would', 'continue', 'it', 'and', 'keep', 'flood', 'the', 'country', 'with', 'democratic', 'voter', '.', 'if', 'a', 'republican', 'win', 'as', 'he', 'do', 'he', 'would', 'have', 'a', 'mess', 'on', 'his', 'hand', 'have', 'to', 'cancel', 'daca', 'which', 'he', 'do', 'and', 'face', 'the', 'backlash', 'of', 'the', 'big', 'chunk', 'of', 'compassionate', 'american', 'voter', 'and', 'endure', 'riot', 'by', 'angry', 'who', 'can', 'blame', 'they', '?', 'the', 'be', 'lie', 'to', 'dreamer', '.', 'a', 'win', 'win', '.', 'it', 'be', 'all', 'plan', '.', 'now', 'obama', 'and', 'his', 'party', 'can', 'sit', 'back', 'pretend', 'to', 'care', 'and', 'enjoy', 'the', 'scenery', 'of', 'angry', 'youth', 'hate', 'on', 'current', 'president', 'who', 'have', 'nothing', 'to', 'do', 'with', 'false', 'illegal', 'promise', 'of', 'the', 'previous', 'administration', '.', 'dreamer', 'should', 'take', 'obama', 'to', 'court', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Classify training data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = next(iter(valloader))\n",
    "out_seqs = classify(lstm_glove, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_tokens(pad_src_seqs[:,i], trainset.vocab))\n",
    "    print('TGT:', pad_tgt_seqs[i].item())\n",
    "    print('OUT:', out_seqs[i].item())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(lstm, dataloader=valloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(dataloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        correct += (out_seqs.squeeze().cpu().long() == tgt_labels).sum().item()\n",
    "        total += tgt_labels.shape[0]\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def test_f1_score(lstm, dataloader=valloader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(dataloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        y_true.extend(tgt_labels.cpu().numpy())\n",
    "        y_pred.extend(out_seqs.squeeze().cpu().numpy())\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9065940863540765\n"
     ]
    }
   ],
   "source": [
    "# print(test_accuracy(lstm_glove, valloader))\n",
    "print(test_f1_score(lstm_glove, valloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset = TranslationDataset('test_2024.csv', vocab=trainset.vocab, dataset_type='test')\n",
    "# testloader = DataLoader(dataset=testset, batch_size=1, shuffle=False, collate_fn=collate, pin_memory=True)\n",
    "# # save testset\n",
    "# torch.save(testset, 'testset.pth')    \n",
    "\n",
    "# load testset\n",
    "testset = torch.load('dataloaders/testset.pth')\n",
    "testloader = DataLoader(dataset=testset, batch_size=1, shuffle=False, collate_fn=collate, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>10995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>10996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>10997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>10998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label\n",
       "0          0      0\n",
       "1          1      0\n",
       "2          2      1\n",
       "3          3      1\n",
       "4          4      0\n",
       "...      ...    ...\n",
       "10995  10995      1\n",
       "10996  10996      1\n",
       "10997  10997      1\n",
       "10998  10998      0\n",
       "10999  10999      0\n",
       "\n",
       "[11000 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do inference on test set and save the results into csv file\n",
    "def test_inference(lstm, output_filename='submission.csv', testset=testset):\n",
    "    testloader = DataLoader(dataset=testset, batch_size=1, shuffle=False, collate_fn=collate, pin_memory=True)\n",
    "    out = []\n",
    "    indices = []\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(testloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        try:\n",
    "            out.extend(out_seqs.squeeze().cpu().numpy())\n",
    "        except:\n",
    "            out.append(out_seqs.squeeze().cpu().numpy())\n",
    "        indices.extend(ids)\n",
    "    df = pd.DataFrame({'id': indices, 'label': out})\n",
    "    # convert label to int\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    # sort by id\n",
    "    df = df.sort_values(by='id')\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    return df\n",
    "\n",
    "test_inference(lstm_glove, output_filename='dev_inference.csv', testset=valset)\n",
    "# test_inference(lstm_glove, output_filename='submission.csv', testset=testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5025261453410923\n",
      "Recall: 0.5025590639452486\n",
      "F1: 0.5024727094784845\n"
     ]
    }
   ],
   "source": [
    "# import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Compute Precision, Recall, and F1 Score of the imported predicted csv and the validation df\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Load the predicted csv\n",
    "y_pred = pd.read_csv('dev_inference.csv', index_col=0)\n",
    "y_pred = y_pred['label'].tolist()\n",
    "\n",
    "# Load the validation df\n",
    "y_true = pd.read_csv('dev_2024.csv', quoting=3)\n",
    "y_true = y_true['label'].tolist()\n",
    "\n",
    "# Compute the metrics\n",
    "precision, recall, f1 = compute_metrics(y_true, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
