{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Except that Desmond played first base last nig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What i find funny is the loyalty and blindness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Read the article  not just the headline &amp; you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Speaking of a horses backside  is that where y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Michael Barone- gee are you dumb.  No other wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label\n",
       "0   0  Except that Desmond played first base last nig...      0\n",
       "1   1  What i find funny is the loyalty and blindness...      0\n",
       "2   2  Read the article  not just the headline & you ...      0\n",
       "3   3  Speaking of a horses backside  is that where y...      1\n",
       "4   4  Michael Barone- gee are you dumb.  No other wo...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_2024.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "EOS_token = 1\n",
    "lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "\tdef __init__(self, csv_path, dataset_type='train', vocab=None):\n",
    "\t\tdf = pd.read_csv(csv_path, quoting=3)\n",
    "\t\tprint(f'len df: {len(df)}')\n",
    "\t\tif dataset_type in ['train', 'val']:\n",
    "\t\t\tself.text, self.labels = zip(*[(text, label) for text, label in zip(df['text'], df['label'])])\n",
    "\t\telse:\n",
    "\t\t\tself.text = df['text'].tolist()\n",
    "\t\t\tself.labels = [0 for _ in range(len(self.text))]\n",
    "\t\tself.ids = df['id'].tolist()\n",
    "\t\tself.dataset_type = dataset_type\n",
    "\t\tself.tokenizer = get_tokenizer('basic_english')\n",
    "\t\tif vocab is None:\n",
    "\t\t\tself._preprocess()\n",
    "\t\telse:\n",
    "\t\t\tself.vocab = vocab\n",
    "\t\t\tself.vocab_size = len(vocab)\n",
    "\n",
    "\tdef _preprocess(self):\n",
    "\t\tself.vocab = build_vocab_from_iterator(self._yield_tokens(), specials=[\"<unk>\"])\n",
    "\t\tself.vocab.set_default_index(self.vocab['<unk>'])\n",
    "\t\tself.vocab.insert_token('<eos>', EOS_token)  # Insert <eos> token with index 1\n",
    "\t\tself.vocab_size = len(self.vocab)\n",
    "\t\t\n",
    "\tdef _yield_tokens(self):\n",
    "\t\tfor text_sample in self.text:\n",
    "\t\t\t# preprocess text\n",
    "\t\t\ttext_sample = normalizeString(text_sample)\n",
    "\t\t\t# tokenize text\n",
    "\t\t\ttokens = self.tokenizer(text_sample)\n",
    "\t\t\t# lemma text\n",
    "\t\t\ttokens = lemmaString(tokens)\n",
    "\n",
    "\t\t\tyield tokens\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tinput_seq = text_to_indices(self.tokenizer, self.vocab, self.text[idx])\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\treturn input_seq, label, self.ids[idx]\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "\treturn ''.join(\n",
    "\t\tc for c in unicodedata.normalize('NFD', s)\n",
    "\t\tif unicodedata.category(c) != 'Mn'\n",
    "\t)\n",
    "\n",
    "def normalizeString(s):\n",
    "\ts = unicodeToAscii(s.lower().strip())\n",
    "\ts = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\ts = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\treturn s\n",
    "\n",
    "def lemmaString(tokens):\n",
    "\treturn [token.lemma_ for token in lemmatizer(' '.join(tokens))]\n",
    "\n",
    "def text_to_indices(tokenizer, vocab, text_sample):\n",
    "\ttokens = tokenizer(text_sample)\n",
    "\tindices = [vocab[token] for token in tokens]\n",
    "\tindices.append(EOS_token)\n",
    "\treturn torch.tensor(indices, dtype=torch.long).view(-1)\n",
    "\n",
    "def seq_to_tokens(seq, vocab):\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[idx] for idx in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df: 10\n"
     ]
    }
   ],
   "source": [
    "trainset = TranslationDataset('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([113,  14,  99, 167, 114,  81, 135,  45,   2, 200,  58,  13, 137,   5,\n",
      "        182, 122,  21,  45, 155,   2,   1]) 0\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "src_sentence, label, id_ = trainset[0]\n",
    "print(src_sentence, label)\n",
    "print(type(src_sentence), type(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "\t\"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "\tArgs:\n",
    "\tlist_of_samples is a list of tuples (src_seq, tgt_label, id):\n",
    "\t\tsrc_seq is of shape (src_seq_length,)\n",
    "\t\ttgt_label is of shape (1,)\n",
    "\t\tid is an int\n",
    "\n",
    "\tReturns:\n",
    "\tsrc_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "\t\tThe sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "\t\tthe longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "\tsrc_seq_lengths: List of lengths of source sequences.\n",
    "\ttgt_labels of shape (batch_size, 1): Tensor of labels for each sequence.\n",
    "\t\"\"\"\n",
    "\t# YOUR CODE HERE\n",
    "\tsrc_seqs = [s[0] for s in list_of_samples]\n",
    "\ttgt_labels = torch.LongTensor([s[1] for s in list_of_samples])\n",
    "\tsrc_seq_lengths = [len(s) for s in src_seqs]\n",
    "\tids = [s[2] for s in list_of_samples]\n",
    "\tsrc_seqs = pad_sequence(src_seqs, padding_value=PADDING_VALUE)\n",
    "\n",
    "\tsrc_seq_lengths, indices = torch.sort(torch.tensor(src_seq_lengths), descending=True)\n",
    "\tsrc_seqs = src_seqs[:, indices]\n",
    "\ttgt_labels = tgt_labels[indices]\n",
    "\n",
    "\treturn src_seqs, src_seq_lengths.tolist(), tgt_labels, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), 1, 0),\n",
    "        (torch.LongTensor([6, 7, 8]), 0, 1),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = collate(pairs)\n",
    "    assert type(src_seq_lengths) == list, \"src_seq_lengths should be a list.\"\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences combined:\n",
      "tensor([[11,  6,  1],\n",
      "        [12,  7,  2],\n",
      "        [13,  8,  0],\n",
      "        [14,  0,  0]])\n",
      "[4, 3, 2]\n",
      "Target sequences combined:\n",
      "tensor([0, 1, 0])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate() function\n",
    "\n",
    "def test_collate_fn():\n",
    "    pairs = [\n",
    "        (torch.tensor([1, 2]), 0, 0),\n",
    "        (torch.tensor([6, 7, 8]), 1, 1),\n",
    "        (torch.tensor([11, 12, 13, 14]), 0, 2),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = collate(pairs)\n",
    "    assert pad_src_seqs.shape == torch.Size([4, 3]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    print('Source sequences combined:')\n",
    "    print(pad_src_seqs)\n",
    "    expected = torch.tensor([\n",
    "      [11, 6, 1],\n",
    "      [12, 7, 2],\n",
    "      [13, 8, 0],\n",
    "      [14, 0, 0],\n",
    "    ])\n",
    "    assert (pad_src_seqs == expected).all(), \"pad_src_seqs does not match expected values\"\n",
    "\n",
    "    print(src_seq_lengths)\n",
    "    if isinstance(src_seq_lengths[0], torch.Size):\n",
    "        src_seq_lengths = sum((list(l) for l in src_seq_lengths), [])\n",
    "    else:\n",
    "        src_seq_lengths = [int(l) for l in src_seq_lengths]\n",
    "    assert src_seq_lengths == [4, 3, 2], f\"Bad src_seq_lengths: {src_seq_lengths}\"\n",
    "\n",
    "    print('Target sequences combined:')\n",
    "    print(pad_tgt_seqs)\n",
    "    expected = torch.tensor([\n",
    "      0, 1, 0\n",
    "    ])\n",
    "    assert (pad_tgt_seqs == expected).all(), \"pad_tgt_seqs0 does not match expected values\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We create custom DataLoader using the implemented collate function\n",
    "# # We are going to process 64 sequences at the same time (batch_size=64)\n",
    "# trainset = TranslationDataset('train_2024.csv')\n",
    "# trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data loader\n",
    "# for i, (src_seqs, src_seq_lengths, tgt_seqs, ids) in enumerate(trainloader):\n",
    "#     print(f\"Batch {i} src_seqs:\")\n",
    "#     print(src_seqs)\n",
    "#     print(f'src_seqs.shape: {src_seqs.shape}')\n",
    "#     print(f\"Batch {i} src_seq_lengths:\")\n",
    "#     print(src_seq_lengths)\n",
    "#     print(f\"Batch {i} tgt_seqs:\")\n",
    "#     print(tgt_seqs)\n",
    "#     print(f'tgt_seqs.shape: {tgt_seqs.shape}')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self, src_dictionary_size, embed_size, hidden_size, dropout=0.2):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tsrc_dictionary_size: The number of words in the source dictionary.\n",
    "\t\tembed_size: The number of dimensions in the word embeddings.\n",
    "\t\thidden_size: The number of features in the hidden state of GRU.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(LSTM, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "\t\tself.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=1, batch_first=False, dropout=dropout, bidirectional=False)\n",
    "\t\tself.fc1 = nn.Linear(hidden_size, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t\n",
    "\tdef forward(self, pad_seqs, seq_lengths, hidden):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tpad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "\t\tseq_lengths: List of sequence lengths.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\toutputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "\t\thidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "\t\t\"\"\"\n",
    "\t\t# YOUR CODE HERE\n",
    "\t\tembedded = self.embedding(pad_seqs)\n",
    "\t\tpacked = pack_padded_sequence(embedded, seq_lengths)\n",
    "\t\toutputs, hidden = self.lstm(packed, hidden)\n",
    "\t\toutputs, output_lengths = pad_packed_sequence(outputs, batch_first=False)\n",
    "\t\tlast_timesteps = torch.stack([outputs[length-1, i] for i, length in enumerate(output_lengths)]) # shape: (batch_size, hidden_size)\n",
    "\t\t# feed through the fully connected layer\n",
    "\t\toutputs = self.fc1(last_timesteps)\n",
    "\t\toutputs = self.dropout(outputs)\n",
    "\t\toutputs = self.relu(outputs)\n",
    "\t\toutputs = self.fc2(outputs)\n",
    "\t\toutputs = self.sigmoid(outputs)\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef init_hidden(self, batch_size=1, device='cpu'):\n",
    "\t\tnum_directions = 1\n",
    "\t\treturn (\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.hidden_size).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "def test_LSTM_shapes():\n",
    "    hidden_size = 3\n",
    "    lstm = LSTM(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = lstm.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])\n",
    "\n",
    "    outputs = lstm.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([batch_size, 1]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_LSTM_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, val_loader):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0\n",
    "\tcriterion = nn.BCELoss()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(val_loader):\n",
    "\t\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\t\thidden = model.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\t\toutputs = model(src_seqs, src_seq_lengths, hidden)\n",
    "\t\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\treturn total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df: 99000\n"
     ]
    }
   ],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 640 sequences at the same time (batch_size=640)\n",
    "# load vocab\n",
    "vocab = torch.load('vocab.pth')\n",
    "trainset = TranslationDataset('train_2024.csv', vocab=vocab)\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=640, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df: 11000\n"
     ]
    }
   ],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "valset = TranslationDataset('dev_2024.csv', vocab=trainset.vocab)\n",
    "valloader = DataLoader(dataset=trainset, batch_size=256, shuffle=False, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xuong\\.conda\\envs\\sl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM model\n",
    "hidden_size = embed_size = 256\n",
    "lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "# Load pretrained LSTM\n",
    "# lstm.load_state_dict(torch.load('lstm_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 1, iter 10: avg. loss = 0.0891, Time spent: 2.26s\n",
      "Epoch 1, iter 20: avg. loss = 0.0750, Time spent: 2.38s\n",
      "Epoch 1, iter 30: avg. loss = 0.0727, Time spent: 3.53s\n",
      "Epoch 1, iter 40: avg. loss = 0.0716, Time spent: 2.49s\n",
      "Epoch 1, iter 50: avg. loss = 0.0695, Time spent: 2.40s\n",
      "Epoch 1, iter 60: avg. loss = 0.0675, Time spent: 2.35s\n",
      "Epoch 1, iter 70: avg. loss = 0.0656, Time spent: 2.25s\n",
      "Epoch 1, iter 80: avg. loss = 0.0649, Time spent: 2.29s\n",
      "Epoch 1, iter 90: avg. loss = 0.0641, Time spent: 4.26s\n",
      "Epoch 1, iter 100: avg. loss = 0.0634, Time spent: 2.40s\n",
      "Epoch 1, iter 110: avg. loss = 0.0635, Time spent: 2.34s\n",
      "Epoch 1, iter 120: avg. loss = 0.0626, Time spent: 2.32s\n",
      "Epoch 1, iter 130: avg. loss = 0.0619, Time spent: 2.59s\n",
      "Epoch 1, iter 140: avg. loss = 0.0616, Time spent: 2.31s\n",
      "Epoch 1, iter 150: avg. loss = 0.0613, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0298\n",
      "Epoch 1, val loss = 0.0298, train loss = 0.0607; Time spent: 400.78s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 2, iter 10: avg. loss = 0.0304, Time spent: 2.26s\n",
      "Epoch 2, iter 20: avg. loss = 0.0305, Time spent: 2.38s\n",
      "Epoch 2, iter 30: avg. loss = 0.0303, Time spent: 3.53s\n",
      "Epoch 2, iter 40: avg. loss = 0.0317, Time spent: 2.49s\n",
      "Epoch 2, iter 50: avg. loss = 0.0315, Time spent: 2.40s\n",
      "Epoch 2, iter 60: avg. loss = 0.0311, Time spent: 2.37s\n",
      "Epoch 2, iter 70: avg. loss = 0.0307, Time spent: 2.25s\n",
      "Epoch 2, iter 80: avg. loss = 0.0301, Time spent: 2.29s\n",
      "Epoch 2, iter 90: avg. loss = 0.0298, Time spent: 4.26s\n",
      "Epoch 2, iter 100: avg. loss = 0.0305, Time spent: 2.39s\n",
      "Epoch 2, iter 110: avg. loss = 0.0315, Time spent: 2.35s\n",
      "Epoch 2, iter 120: avg. loss = 0.0315, Time spent: 2.32s\n",
      "Epoch 2, iter 130: avg. loss = 0.0313, Time spent: 2.61s\n",
      "Epoch 2, iter 140: avg. loss = 0.0314, Time spent: 2.30s\n",
      "Epoch 2, iter 150: avg. loss = 0.0314, Time spent: 2.38s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0206\n",
      "Epoch 2, val loss = 0.0206, train loss = 0.0311; Time spent: 403.72s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 3, iter 10: avg. loss = 0.0189, Time spent: 2.26s\n",
      "Epoch 3, iter 20: avg. loss = 0.0195, Time spent: 2.38s\n",
      "Epoch 3, iter 30: avg. loss = 0.0195, Time spent: 3.52s\n",
      "Epoch 3, iter 40: avg. loss = 0.0203, Time spent: 2.48s\n",
      "Epoch 3, iter 50: avg. loss = 0.0193, Time spent: 2.40s\n",
      "Epoch 3, iter 60: avg. loss = 0.0188, Time spent: 2.36s\n",
      "Epoch 3, iter 70: avg. loss = 0.0185, Time spent: 2.25s\n",
      "Epoch 3, iter 80: avg. loss = 0.0179, Time spent: 2.29s\n",
      "Epoch 3, iter 90: avg. loss = 0.0178, Time spent: 4.26s\n",
      "Epoch 3, iter 100: avg. loss = 0.0186, Time spent: 2.39s\n",
      "Epoch 3, iter 110: avg. loss = 0.0187, Time spent: 2.34s\n",
      "Epoch 3, iter 120: avg. loss = 0.0191, Time spent: 2.33s\n",
      "Epoch 3, iter 130: avg. loss = 0.0191, Time spent: 2.59s\n",
      "Epoch 3, iter 140: avg. loss = 0.0192, Time spent: 2.30s\n",
      "Epoch 3, iter 150: avg. loss = 0.0193, Time spent: 2.39s\n",
      "Epoch 3, val loss = 0.0241, train loss = 0.0192; Time spent: 400.58s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 4, iter 10: avg. loss = 0.0152, Time spent: 2.26s\n",
      "Epoch 4, iter 20: avg. loss = 0.0182, Time spent: 2.37s\n",
      "Epoch 4, iter 30: avg. loss = 0.0186, Time spent: 3.51s\n",
      "Epoch 4, iter 40: avg. loss = 0.0199, Time spent: 2.50s\n",
      "Epoch 4, iter 50: avg. loss = 0.0190, Time spent: 2.40s\n",
      "Epoch 4, iter 60: avg. loss = 0.0184, Time spent: 2.35s\n",
      "Epoch 4, iter 70: avg. loss = 0.0192, Time spent: 2.26s\n",
      "Epoch 4, iter 80: avg. loss = 0.0190, Time spent: 2.29s\n",
      "Epoch 4, iter 90: avg. loss = 0.0193, Time spent: 4.27s\n",
      "Epoch 4, iter 100: avg. loss = 0.0199, Time spent: 2.40s\n",
      "Epoch 4, iter 110: avg. loss = 0.0202, Time spent: 2.34s\n",
      "Epoch 4, iter 120: avg. loss = 0.0208, Time spent: 2.32s\n",
      "Epoch 4, iter 130: avg. loss = 0.0214, Time spent: 2.59s\n",
      "Epoch 4, iter 140: avg. loss = 0.0214, Time spent: 2.30s\n",
      "Epoch 4, iter 150: avg. loss = 0.0215, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0161\n",
      "Epoch 4, val loss = 0.0161, train loss = 0.0212; Time spent: 400.71s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 5, iter 10: avg. loss = 0.0188, Time spent: 2.27s\n",
      "Epoch 5, iter 20: avg. loss = 0.0187, Time spent: 2.37s\n",
      "Epoch 5, iter 30: avg. loss = 0.0195, Time spent: 3.51s\n",
      "Epoch 5, iter 40: avg. loss = 0.0198, Time spent: 2.52s\n",
      "Epoch 5, iter 50: avg. loss = 0.0203, Time spent: 2.39s\n",
      "Epoch 5, iter 60: avg. loss = 0.0201, Time spent: 2.37s\n",
      "Epoch 5, iter 70: avg. loss = 0.0200, Time spent: 2.25s\n",
      "Epoch 5, iter 80: avg. loss = 0.0193, Time spent: 2.29s\n",
      "Epoch 5, iter 90: avg. loss = 0.0188, Time spent: 4.25s\n",
      "Epoch 5, iter 100: avg. loss = 0.0188, Time spent: 2.39s\n",
      "Epoch 5, iter 110: avg. loss = 0.0190, Time spent: 2.34s\n",
      "Epoch 5, iter 120: avg. loss = 0.0186, Time spent: 2.33s\n",
      "Epoch 5, iter 130: avg. loss = 0.0183, Time spent: 2.59s\n",
      "Epoch 5, iter 140: avg. loss = 0.0179, Time spent: 2.30s\n",
      "Epoch 5, iter 150: avg. loss = 0.0174, Time spent: 2.39s\n",
      "Epoch 5, val loss = 0.0182, train loss = 0.0172; Time spent: 401.14s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 6, iter 10: avg. loss = 0.0153, Time spent: 2.25s\n",
      "Epoch 6, iter 20: avg. loss = 0.0144, Time spent: 2.37s\n",
      "Epoch 6, iter 30: avg. loss = 0.0156, Time spent: 3.52s\n",
      "Epoch 6, iter 40: avg. loss = 0.0160, Time spent: 2.48s\n",
      "Epoch 6, iter 50: avg. loss = 0.0152, Time spent: 2.39s\n",
      "Epoch 6, iter 60: avg. loss = 0.0150, Time spent: 2.36s\n",
      "Epoch 6, iter 70: avg. loss = 0.0146, Time spent: 2.25s\n",
      "Epoch 6, iter 80: avg. loss = 0.0143, Time spent: 2.29s\n",
      "Epoch 6, iter 90: avg. loss = 0.0138, Time spent: 4.27s\n",
      "Epoch 6, iter 100: avg. loss = 0.0140, Time spent: 2.40s\n",
      "Epoch 6, iter 110: avg. loss = 0.0140, Time spent: 2.34s\n",
      "Epoch 6, iter 120: avg. loss = 0.0139, Time spent: 2.33s\n",
      "Epoch 6, iter 130: avg. loss = 0.0137, Time spent: 2.59s\n",
      "Epoch 6, iter 140: avg. loss = 0.0138, Time spent: 2.32s\n",
      "Epoch 6, iter 150: avg. loss = 0.0137, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0102\n",
      "Epoch 6, val loss = 0.0102, train loss = 0.0135; Time spent: 400.36s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 7, iter 10: avg. loss = 0.0148, Time spent: 2.26s\n",
      "Epoch 7, iter 20: avg. loss = 0.0158, Time spent: 2.37s\n",
      "Epoch 7, iter 30: avg. loss = 0.0164, Time spent: 3.52s\n",
      "Epoch 7, iter 40: avg. loss = 0.0160, Time spent: 2.49s\n",
      "Epoch 7, iter 50: avg. loss = 0.0168, Time spent: 2.40s\n",
      "Epoch 7, iter 60: avg. loss = 0.0161, Time spent: 2.36s\n",
      "Epoch 7, iter 70: avg. loss = 0.0168, Time spent: 2.25s\n",
      "Epoch 7, iter 80: avg. loss = 0.0168, Time spent: 2.29s\n",
      "Epoch 7, iter 90: avg. loss = 0.0163, Time spent: 4.27s\n",
      "Epoch 7, iter 100: avg. loss = 0.0163, Time spent: 2.40s\n",
      "Epoch 7, iter 110: avg. loss = 0.0158, Time spent: 2.34s\n",
      "Epoch 7, iter 120: avg. loss = 0.0160, Time spent: 2.32s\n",
      "Epoch 7, iter 130: avg. loss = 0.0159, Time spent: 2.59s\n",
      "Epoch 7, iter 140: avg. loss = 0.0165, Time spent: 2.30s\n",
      "Epoch 7, iter 150: avg. loss = 0.0168, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0101\n",
      "Epoch 7, val loss = 0.0101, train loss = 0.0170; Time spent: 400.37s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 8, iter 10: avg. loss = 0.0193, Time spent: 2.25s\n",
      "Epoch 8, iter 20: avg. loss = 0.0183, Time spent: 2.37s\n",
      "Epoch 8, iter 30: avg. loss = 0.0160, Time spent: 3.52s\n",
      "Epoch 8, iter 40: avg. loss = 0.0152, Time spent: 2.48s\n",
      "Epoch 8, iter 50: avg. loss = 0.0135, Time spent: 2.40s\n",
      "Epoch 8, iter 60: avg. loss = 0.0122, Time spent: 2.36s\n",
      "Epoch 8, iter 70: avg. loss = 0.0115, Time spent: 2.25s\n",
      "Epoch 8, iter 80: avg. loss = 0.0113, Time spent: 2.28s\n",
      "Epoch 8, iter 90: avg. loss = 0.0113, Time spent: 4.26s\n",
      "Epoch 8, iter 100: avg. loss = 0.0118, Time spent: 2.39s\n",
      "Epoch 8, iter 110: avg. loss = 0.0114, Time spent: 2.33s\n",
      "Epoch 8, iter 120: avg. loss = 0.0114, Time spent: 2.32s\n",
      "Epoch 8, iter 130: avg. loss = 0.0115, Time spent: 2.58s\n",
      "Epoch 8, iter 140: avg. loss = 0.0118, Time spent: 2.29s\n",
      "Epoch 8, iter 150: avg. loss = 0.0117, Time spent: 2.38s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0058\n",
      "Epoch 8, val loss = 0.0058, train loss = 0.0115; Time spent: 400.13s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 9, iter 10: avg. loss = 0.0088, Time spent: 2.25s\n",
      "Epoch 9, iter 20: avg. loss = 0.0088, Time spent: 2.38s\n",
      "Epoch 9, iter 30: avg. loss = 0.0082, Time spent: 3.52s\n",
      "Epoch 9, iter 40: avg. loss = 0.0074, Time spent: 2.49s\n",
      "Epoch 9, iter 50: avg. loss = 0.0069, Time spent: 2.39s\n",
      "Epoch 9, iter 60: avg. loss = 0.0071, Time spent: 2.36s\n",
      "Epoch 9, iter 70: avg. loss = 0.0069, Time spent: 2.24s\n",
      "Epoch 9, iter 80: avg. loss = 0.0068, Time spent: 2.29s\n",
      "Epoch 9, iter 90: avg. loss = 0.0070, Time spent: 4.26s\n",
      "Epoch 9, iter 100: avg. loss = 0.0071, Time spent: 2.39s\n",
      "Epoch 9, iter 110: avg. loss = 0.0076, Time spent: 2.34s\n",
      "Epoch 9, iter 120: avg. loss = 0.0073, Time spent: 2.32s\n",
      "Epoch 9, iter 130: avg. loss = 0.0075, Time spent: 2.59s\n",
      "Epoch 9, iter 140: avg. loss = 0.0076, Time spent: 2.30s\n",
      "Epoch 9, iter 150: avg. loss = 0.0079, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0052\n",
      "Epoch 9, val loss = 0.0052, train loss = 0.0078; Time spent: 400.17s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 10, iter 10: avg. loss = 0.0030, Time spent: 2.26s\n",
      "Epoch 10, iter 20: avg. loss = 0.0039, Time spent: 2.38s\n",
      "Epoch 10, iter 30: avg. loss = 0.0051, Time spent: 3.52s\n",
      "Epoch 10, iter 40: avg. loss = 0.0054, Time spent: 2.49s\n",
      "Epoch 10, iter 50: avg. loss = 0.0057, Time spent: 2.40s\n",
      "Epoch 10, iter 60: avg. loss = 0.0056, Time spent: 2.36s\n",
      "Epoch 10, iter 70: avg. loss = 0.0057, Time spent: 2.25s\n",
      "Epoch 10, iter 80: avg. loss = 0.0058, Time spent: 2.29s\n",
      "Epoch 10, iter 90: avg. loss = 0.0060, Time spent: 4.26s\n",
      "Epoch 10, iter 100: avg. loss = 0.0061, Time spent: 2.40s\n",
      "Epoch 10, iter 110: avg. loss = 0.0062, Time spent: 2.34s\n",
      "Epoch 10, iter 120: avg. loss = 0.0063, Time spent: 2.32s\n",
      "Epoch 10, iter 130: avg. loss = 0.0063, Time spent: 2.59s\n",
      "Epoch 10, iter 140: avg. loss = 0.0061, Time spent: 2.30s\n",
      "Epoch 10, iter 150: avg. loss = 0.0061, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0041\n",
      "Epoch 10, val loss = 0.0041, train loss = 0.0062; Time spent: 400.86s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 11, iter 10: avg. loss = 0.0052, Time spent: 2.26s\n",
      "Epoch 11, iter 20: avg. loss = 0.0052, Time spent: 2.37s\n",
      "Epoch 11, iter 30: avg. loss = 0.0052, Time spent: 3.52s\n",
      "Epoch 11, iter 40: avg. loss = 0.0049, Time spent: 2.48s\n",
      "Epoch 11, iter 50: avg. loss = 0.0045, Time spent: 2.40s\n",
      "Epoch 11, iter 60: avg. loss = 0.0042, Time spent: 2.36s\n",
      "Epoch 11, iter 70: avg. loss = 0.0039, Time spent: 2.26s\n",
      "Epoch 11, iter 80: avg. loss = 0.0036, Time spent: 2.28s\n",
      "Epoch 11, iter 90: avg. loss = 0.0034, Time spent: 4.26s\n",
      "Epoch 11, iter 100: avg. loss = 0.0035, Time spent: 2.40s\n",
      "Epoch 11, iter 110: avg. loss = 0.0036, Time spent: 2.34s\n",
      "Epoch 11, iter 120: avg. loss = 0.0037, Time spent: 2.32s\n",
      "Epoch 11, iter 130: avg. loss = 0.0038, Time spent: 2.59s\n",
      "Epoch 11, iter 140: avg. loss = 0.0037, Time spent: 2.30s\n",
      "Epoch 11, iter 150: avg. loss = 0.0037, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0034\n",
      "Epoch 11, val loss = 0.0034, train loss = 0.0037; Time spent: 400.79s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 12, iter 10: avg. loss = 0.0027, Time spent: 2.26s\n",
      "Epoch 12, iter 20: avg. loss = 0.0047, Time spent: 2.37s\n",
      "Epoch 12, iter 30: avg. loss = 0.0045, Time spent: 3.51s\n",
      "Epoch 12, iter 40: avg. loss = 0.0045, Time spent: 2.49s\n",
      "Epoch 12, iter 50: avg. loss = 0.0042, Time spent: 2.39s\n",
      "Epoch 12, iter 60: avg. loss = 0.0039, Time spent: 2.36s\n",
      "Epoch 12, iter 70: avg. loss = 0.0043, Time spent: 2.25s\n",
      "Epoch 12, iter 80: avg. loss = 0.0051, Time spent: 2.28s\n",
      "Epoch 12, iter 90: avg. loss = 0.0054, Time spent: 4.26s\n",
      "Epoch 12, iter 100: avg. loss = 0.0058, Time spent: 2.40s\n",
      "Epoch 12, iter 110: avg. loss = 0.0062, Time spent: 2.34s\n",
      "Epoch 12, iter 120: avg. loss = 0.0060, Time spent: 2.33s\n",
      "Epoch 12, iter 130: avg. loss = 0.0059, Time spent: 2.60s\n",
      "Epoch 12, iter 140: avg. loss = 0.0057, Time spent: 2.30s\n",
      "Epoch 12, iter 150: avg. loss = 0.0056, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0031\n",
      "Epoch 12, val loss = 0.0031, train loss = 0.0056; Time spent: 400.63s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 13, iter 10: avg. loss = 0.0036, Time spent: 2.25s\n",
      "Epoch 13, iter 20: avg. loss = 0.0047, Time spent: 2.37s\n",
      "Epoch 13, iter 30: avg. loss = 0.0046, Time spent: 3.52s\n",
      "Epoch 13, iter 40: avg. loss = 0.0046, Time spent: 2.49s\n",
      "Epoch 13, iter 50: avg. loss = 0.0045, Time spent: 2.40s\n",
      "Epoch 13, iter 60: avg. loss = 0.0049, Time spent: 2.37s\n",
      "Epoch 13, iter 70: avg. loss = 0.0053, Time spent: 2.25s\n",
      "Epoch 13, iter 80: avg. loss = 0.0054, Time spent: 2.29s\n",
      "Epoch 13, iter 90: avg. loss = 0.0059, Time spent: 4.26s\n",
      "Epoch 13, iter 100: avg. loss = 0.0058, Time spent: 2.39s\n",
      "Epoch 13, iter 110: avg. loss = 0.0058, Time spent: 2.34s\n",
      "Epoch 13, iter 120: avg. loss = 0.0063, Time spent: 2.32s\n",
      "Epoch 13, iter 130: avg. loss = 0.0066, Time spent: 2.59s\n",
      "Epoch 13, iter 140: avg. loss = 0.0064, Time spent: 2.29s\n",
      "Epoch 13, iter 150: avg. loss = 0.0065, Time spent: 2.38s\n",
      "Epoch 13, val loss = 0.0090, train loss = 0.0067; Time spent: 400.61s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 14, iter 10: avg. loss = 0.0082, Time spent: 2.26s\n",
      "Epoch 14, iter 20: avg. loss = 0.0095, Time spent: 2.37s\n",
      "Epoch 14, iter 30: avg. loss = 0.0086, Time spent: 3.52s\n",
      "Epoch 14, iter 40: avg. loss = 0.0077, Time spent: 2.50s\n",
      "Epoch 14, iter 50: avg. loss = 0.0073, Time spent: 2.39s\n",
      "Epoch 14, iter 60: avg. loss = 0.0068, Time spent: 2.36s\n",
      "Epoch 14, iter 70: avg. loss = 0.0063, Time spent: 2.25s\n",
      "Epoch 14, iter 80: avg. loss = 0.0062, Time spent: 2.29s\n",
      "Epoch 14, iter 90: avg. loss = 0.0061, Time spent: 4.26s\n",
      "Epoch 14, iter 100: avg. loss = 0.0059, Time spent: 2.40s\n",
      "Epoch 14, iter 110: avg. loss = 0.0057, Time spent: 2.35s\n",
      "Epoch 14, iter 120: avg. loss = 0.0054, Time spent: 2.32s\n",
      "Epoch 14, iter 130: avg. loss = 0.0054, Time spent: 2.60s\n",
      "Epoch 14, iter 140: avg. loss = 0.0055, Time spent: 2.30s\n",
      "Epoch 14, iter 150: avg. loss = 0.0053, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0030\n",
      "Epoch 14, val loss = 0.0030, train loss = 0.0054; Time spent: 401.38s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 15, iter 10: avg. loss = 0.0026, Time spent: 2.25s\n",
      "Epoch 15, iter 20: avg. loss = 0.0032, Time spent: 2.38s\n",
      "Epoch 15, iter 30: avg. loss = 0.0032, Time spent: 3.52s\n",
      "Epoch 15, iter 40: avg. loss = 0.0036, Time spent: 2.49s\n",
      "Epoch 15, iter 50: avg. loss = 0.0031, Time spent: 2.40s\n",
      "Epoch 15, iter 60: avg. loss = 0.0031, Time spent: 2.35s\n",
      "Epoch 15, iter 70: avg. loss = 0.0030, Time spent: 2.26s\n",
      "Epoch 15, iter 80: avg. loss = 0.0029, Time spent: 2.28s\n",
      "Epoch 15, iter 90: avg. loss = 0.0029, Time spent: 4.27s\n",
      "Epoch 15, iter 100: avg. loss = 0.0029, Time spent: 2.40s\n",
      "Epoch 15, iter 110: avg. loss = 0.0029, Time spent: 2.34s\n",
      "Epoch 15, iter 120: avg. loss = 0.0027, Time spent: 2.32s\n",
      "Epoch 15, iter 130: avg. loss = 0.0026, Time spent: 2.59s\n",
      "Epoch 15, iter 140: avg. loss = 0.0025, Time spent: 2.30s\n",
      "Epoch 15, iter 150: avg. loss = 0.0025, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0019\n",
      "Epoch 15, val loss = 0.0019, train loss = 0.0026; Time spent: 401.31s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 16, iter 10: avg. loss = 0.0013, Time spent: 2.26s\n",
      "Epoch 16, iter 20: avg. loss = 0.0018, Time spent: 2.38s\n",
      "Epoch 16, iter 30: avg. loss = 0.0019, Time spent: 3.52s\n",
      "Epoch 16, iter 40: avg. loss = 0.0021, Time spent: 2.49s\n",
      "Epoch 16, iter 50: avg. loss = 0.0020, Time spent: 2.39s\n",
      "Epoch 16, iter 60: avg. loss = 0.0022, Time spent: 2.36s\n",
      "Epoch 16, iter 70: avg. loss = 0.0024, Time spent: 2.25s\n",
      "Epoch 16, iter 80: avg. loss = 0.0025, Time spent: 2.29s\n",
      "Epoch 16, iter 90: avg. loss = 0.0030, Time spent: 4.27s\n",
      "Epoch 16, iter 100: avg. loss = 0.0030, Time spent: 2.39s\n",
      "Epoch 16, iter 110: avg. loss = 0.0033, Time spent: 2.35s\n",
      "Epoch 16, iter 120: avg. loss = 0.0041, Time spent: 2.32s\n",
      "Epoch 16, iter 130: avg. loss = 0.0051, Time spent: 2.59s\n",
      "Epoch 16, iter 140: avg. loss = 0.0056, Time spent: 2.29s\n",
      "Epoch 16, iter 150: avg. loss = 0.0058, Time spent: 2.39s\n",
      "Epoch 16, val loss = 0.0070, train loss = 0.0059; Time spent: 400.36s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 17, iter 10: avg. loss = 0.0088, Time spent: 2.26s\n",
      "Epoch 17, iter 20: avg. loss = 0.0092, Time spent: 2.38s\n",
      "Epoch 17, iter 30: avg. loss = 0.0088, Time spent: 3.52s\n",
      "Epoch 17, iter 40: avg. loss = 0.0083, Time spent: 2.48s\n",
      "Epoch 17, iter 50: avg. loss = 0.0073, Time spent: 2.40s\n",
      "Epoch 17, iter 60: avg. loss = 0.0074, Time spent: 2.36s\n",
      "Epoch 17, iter 70: avg. loss = 0.0073, Time spent: 2.25s\n",
      "Epoch 17, iter 80: avg. loss = 0.0069, Time spent: 2.28s\n",
      "Epoch 17, iter 90: avg. loss = 0.0067, Time spent: 4.26s\n",
      "Epoch 17, iter 100: avg. loss = 0.0066, Time spent: 2.39s\n",
      "Epoch 17, iter 110: avg. loss = 0.0063, Time spent: 2.34s\n",
      "Epoch 17, iter 120: avg. loss = 0.0062, Time spent: 2.33s\n",
      "Epoch 17, iter 130: avg. loss = 0.0062, Time spent: 2.59s\n",
      "Epoch 17, iter 140: avg. loss = 0.0061, Time spent: 2.30s\n",
      "Epoch 17, iter 150: avg. loss = 0.0060, Time spent: 2.39s\n",
      "Epoch 17, val loss = 0.0036, train loss = 0.0059; Time spent: 400.46s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 18, iter 10: avg. loss = 0.0036, Time spent: 2.26s\n",
      "Epoch 18, iter 20: avg. loss = 0.0142, Time spent: 2.38s\n",
      "Epoch 18, iter 30: avg. loss = 0.0253, Time spent: 3.52s\n",
      "Epoch 18, iter 40: avg. loss = 0.0275, Time spent: 2.48s\n",
      "Epoch 18, iter 50: avg. loss = 0.0272, Time spent: 2.39s\n",
      "Epoch 18, iter 60: avg. loss = 0.0275, Time spent: 2.36s\n",
      "Epoch 18, iter 70: avg. loss = 0.0272, Time spent: 2.25s\n",
      "Epoch 18, iter 80: avg. loss = 0.0271, Time spent: 2.29s\n",
      "Epoch 18, iter 90: avg. loss = 0.0264, Time spent: 4.26s\n",
      "Epoch 18, iter 100: avg. loss = 0.0259, Time spent: 2.39s\n",
      "Epoch 18, iter 110: avg. loss = 0.0248, Time spent: 2.34s\n",
      "Epoch 18, iter 120: avg. loss = 0.0238, Time spent: 2.32s\n",
      "Epoch 18, iter 130: avg. loss = 0.0226, Time spent: 2.59s\n",
      "Epoch 18, iter 140: avg. loss = 0.0221, Time spent: 2.31s\n",
      "Epoch 18, iter 150: avg. loss = 0.0213, Time spent: 2.39s\n",
      "Epoch 18, val loss = 0.0045, train loss = 0.0209; Time spent: 400.14s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 19, iter 10: avg. loss = 0.0121, Time spent: 2.25s\n",
      "Epoch 19, iter 20: avg. loss = 0.0102, Time spent: 2.37s\n",
      "Epoch 19, iter 30: avg. loss = 0.0104, Time spent: 3.51s\n",
      "Epoch 19, iter 40: avg. loss = 0.0100, Time spent: 2.48s\n",
      "Epoch 19, iter 50: avg. loss = 0.0089, Time spent: 2.40s\n",
      "Epoch 19, iter 60: avg. loss = 0.0078, Time spent: 2.36s\n",
      "Epoch 19, iter 70: avg. loss = 0.0072, Time spent: 2.25s\n",
      "Epoch 19, iter 80: avg. loss = 0.0066, Time spent: 2.29s\n",
      "Epoch 19, iter 90: avg. loss = 0.0063, Time spent: 4.26s\n",
      "Epoch 19, iter 100: avg. loss = 0.0061, Time spent: 2.40s\n",
      "Epoch 19, iter 110: avg. loss = 0.0058, Time spent: 2.35s\n",
      "Epoch 19, iter 120: avg. loss = 0.0054, Time spent: 2.33s\n",
      "Epoch 19, iter 130: avg. loss = 0.0051, Time spent: 2.59s\n",
      "Epoch 19, iter 140: avg. loss = 0.0050, Time spent: 2.30s\n",
      "Epoch 19, iter 150: avg. loss = 0.0048, Time spent: 2.39s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0015\n",
      "Epoch 19, val loss = 0.0015, train loss = 0.0048; Time spent: 400.04s\n",
      "Number of batches: 155\n",
      "batch_size: 640\n",
      "Epoch 20, iter 10: avg. loss = 0.0019, Time spent: 2.26s\n",
      "Epoch 20, iter 20: avg. loss = 0.0021, Time spent: 2.37s\n",
      "Epoch 20, iter 30: avg. loss = 0.0020, Time spent: 3.52s\n",
      "Epoch 20, iter 40: avg. loss = 0.0021, Time spent: 2.47s\n",
      "Epoch 20, iter 50: avg. loss = 0.0021, Time spent: 2.39s\n",
      "Epoch 20, iter 60: avg. loss = 0.0020, Time spent: 2.36s\n",
      "Epoch 20, iter 70: avg. loss = 0.0018, Time spent: 2.25s\n",
      "Epoch 20, iter 80: avg. loss = 0.0017, Time spent: 2.29s\n",
      "Epoch 20, iter 90: avg. loss = 0.0018, Time spent: 4.27s\n",
      "Epoch 20, iter 100: avg. loss = 0.0017, Time spent: 2.39s\n",
      "Epoch 20, iter 110: avg. loss = 0.0017, Time spent: 2.36s\n",
      "Epoch 20, iter 120: avg. loss = 0.0016, Time spent: 2.33s\n",
      "Epoch 20, iter 130: avg. loss = 0.0016, Time spent: 2.59s\n",
      "Epoch 20, iter 140: avg. loss = 0.0016, Time spent: 2.30s\n",
      "Epoch 20, iter 150: avg. loss = 0.0016, Time spent: 2.40s\n",
      "find new best model, save to lstm.pth, eval_loss: 0.0013\n",
      "Epoch 20, val loss = 0.0013, train loss = 0.0016; Time spent: 400.28s\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(n_epochs):\n",
    "\tlstm.train()\n",
    "\trunning_loss = 0.0\n",
    "\tepoch_start_time = time.time()\n",
    "\tprint(f'Number of batches: {len(trainloader)}')\n",
    "\tprint(f'batch_size: {trainloader.batch_size}')\n",
    "\tfor i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(trainloader):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsrc_seqs, tgt_labels = src_seqs.to(device), tgt_labels.to(device)\n",
    "\t\thidden = lstm.init_hidden(src_seqs.shape[1], device=device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = lstm(src_seqs, src_seq_lengths, hidden)\n",
    "\t\tloss = criterion(outputs.squeeze(), tgt_labels.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 10 == 9:\n",
    "\t\t\tprint(f'Epoch {epoch + 1}, iter {i + 1}: avg. loss = {running_loss/(i + 1):.4f}, Time spent: {time.time()-start_time:.2f}s')\n",
    "\ttrain_losses.append(running_loss / len(trainloader))\n",
    "\teval_loss = val_loss(lstm, valloader)\n",
    "\tif eval_loss < best_val_loss:\n",
    "\t\tbest_val_loss = eval_loss\n",
    "\t\tbest_model = lstm.state_dict()\n",
    "\t\tif best_model is not None:\n",
    "\t\t\tprint(f'find new best model, save to lstm.pth, eval_loss: {eval_loss:.4f}')\n",
    "\t\t\ttorch.save(best_model, 'lstm.pth')\n",
    "\n",
    "\tval_losses.append(eval_loss)\n",
    "\tprint(f'Epoch {epoch + 1}, val loss = {eval_loss:.4f}, train loss = {train_losses[-1]:.4f}; Time spent: {time.time()-epoch_start_time:.2f}s')\n",
    "\trunning_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and vocab\n",
    "torch.save(lstm.state_dict(), 'lstm.pth')\n",
    "torch.save(trainset.vocab, 'vocab.pth')\n",
    "\n",
    "# # Load model\n",
    "# lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "# lstm.load_state_dict(torch.load('lstm.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = embed_size = 256\n",
    "lstm = LSTM(trainset.vocab_size, embed_size, hidden_size).to(device)\n",
    "lstm.load_state_dict(torch.load('lstm.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(lstm, pad_src_seqs, src_seq_lengths):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "\n",
    "    Args:\n",
    "    lstm (LSTM): Trained lstm.\n",
    "    pad_src_seqs of shape (max_src_seq_length, batch_size): Padded source sequences.\n",
    "    src_seq_lengths: List of source sequence lengths.\n",
    "\n",
    "    Returns:\n",
    "    out_seqs of shape (batch_size, 1): LongTensor of word indices of the output sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        pad_src_seqs = pad_src_seqs.to(device)\n",
    "        lstm_hidden = lstm.init_hidden(pad_src_seqs.shape[1], device)\n",
    "        outputs = lstm(pad_src_seqs, src_seq_lengths, lstm_hidden)\n",
    "        out_seqs = outputs > 0.5\n",
    "        return out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    pad_src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths=[4, 2])\n",
    "    assert out_seqs.shape == torch.Size([2, 1]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify training data:\n",
      "-----------------------------\n",
      "SRC: ['good', 'article', 'but', 'few', 'read', 'the', 'rg', '.', 'for', 'enforcement', 'folks', 'we', 'see', 'that', 'there', 'is', 'a', 'wide', 'degree', 'of', 'latitude', 'in', 'how', 'officers', 'apply', 'the', 'law', '.', 'only', '<unk>', 'tickets', 'issued', 'so', 'far', 'indicates', 'to', 'me', 'that', 'of', 'the', 'total', 'infractions', 'it', '<unk>', 's', 'a', 'drop', 'in', 'the', 'bucket', '.', 'and', 'then', 'there', 'are', 'judges', 'some', 'of', 'whom', 'will', 'reduce', 'the', 'fine', 'for', 'first', 'offense', 'or', 'mitigating', 'circumstances', '.', 'what', 'would', 'an', 'officer', 'do', 'if', 'someone', 'later', 'injured', 'or', 'killed', 'someone', 'while', 'texting', '?', 'i', 'am', 'inclined', 'to', 'issue', 'more', 'tickets', 'and', 'let', 'the', 'judge', 'make', 'the', 'call', '.', 'for', 'anyone', 'caught', 'lying', 'about', 'it', 'that', 'is', 'an', 'immediate', 'ticket', '.', 'these', 'people', 'know', '.', 'i', 'sent', 'a', 'text', 'to', 'a', 'friend', 'that', 'returned', 'an', 'automated', 'text', 'saying', 'that', 'she', 'was', 'driving', 'and', 'would', 'reply', 'later', '.', 'that', 'is', 'a', 'great', 'app', '.', 'guessing', 'i', '<unk>', 'd', 'say', 'half', 'the', 'drivers', 'think', 'it', 'is', 'ok', 'to', 'text', 'at', 'a', 'light', '.', 'you', 'can', 'see', 'many', 'furtively', 'texting', 'and', 'some', 'are', 'the', 'ones', 'in', 'front', 'of', 'you', 'that', 'seem', 'to', 'not', 'notice', 'the', 'light', '.', 'i', '<unk>', 'd', 'add', 'a', 'few', 'signs', 'at', 'street', 'lights', 'that', 'read', 'distracted', 'driving', 'laws', 'apply', 'while', 'stopped', 'at', 'light', '.', '<eos>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['whenever', 'abortion', 'comes', 'up', 'i', 'have', 'a', 'question', 'i', '<unk>', 've', 'been', 'asking', 'for', 'ten', 'years', 'now', 'of', 'the', 'life', 'begins', 'at', 'conception', 'crowd', '.', 'in', 'ten', 'years', 'no', 'one', 'has', 'ever', 'answered', 'it', 'honestly', '.', 'here', 'it', 'is', '.', 'you', '<unk>', 're', 'in', 'a', 'fertility', 'clinic', '.', 'why', 'isn', '<unk>', 't', 'important', '.', 'the', 'fire', 'alarm', 'goes', 'off', '.', 'you', 'run', 'for', 'the', 'exit', '.', 'as', 'you', 'run', 'down', 'this', 'hallway', 'you', 'hear', 'a', 'child', 'screaming', 'from', 'behind', 'a', 'door', '.', 'you', 'throw', 'open', 'the', 'door', 'and', 'find', 'a', '<unk>', 'child', 'crying', 'for', 'help', '.', 'they', '<unk>', 're', 'in', 'one', 'corner', 'of', 'the', 'room', '.', 'in', 'the', 'other', 'corner', 'you', 'spot', 'a', 'frozen', 'container', 'labeled', '<unk>', 'viable', 'human', 'embryos', '.', 'the', 'smoke', 'is', 'rising', '.', 'you', 'start', 'to', 'choke', '.', 'you', 'know', 'you', 'can', 'grab', 'one', 'or', 'the', 'other', 'but', 'not', 'both', 'before', 'you', 'succumb', 'to', 'smoke', 'inhalation', 'and', 'die', 'saving', 'no', 'one', '.', 'do', 'you', 'a', '<unk>', 'save', 'the', 'child', 'or', 'b', '<unk>', 'save', 'the', 'thousand', 'embryos', '?', 'there', 'is', 'no', 'c', '.', 'c', 'means', 'you', 'all', 'die', '.', 'patrick', 's', '.', 'tomlinson', '<unk>', 'http', '<unk>', '.', 'patrickstomlinson', '.', 'com', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['it', '<unk>', 's', 'definitely', 'a', 'new', 'thing', '.', 'i', 'don', '<unk>', 't', 'mind', 'it', 'myself', '.', '.', '.', 'i', 'find', 'myself', 'being', 'more', 'conscious', 'of', 'the', 'difference', 'between', 'a', 'good', 'comment', 'vs', '.', 'a', 'comment', 'that', 'fails', 'the', 'civility', 'test', '<unk>', 'find', 'myself', 'rating', 'comments', 'good', 'even', 'if', 'i', 'disagree', 'with', 'them', 'when', 'they', 'are', 'civil', '<unk>', 'especially', 'if', 'they', 'bring', 'up', 'points', 'that', 'i', 'can', 'think', 'about', '.', 'it', 'makes', 'me', 'feel', 'better', 'about', 'people', 'i', 'disagree', 'with', '.', 'i', 'just', 'read', 'an', 'article', 'which', 'mentions', 'civil', 'comments', '<unk>', 'its', 'rationale', '.', 'the', 'entire', 'article', 'is', 'worth', 'a', 'read', 'the', 'social', 'impact', 'of', 'uncivil', 'discourse', 'is', 'one', 'thing', 'but', 'there', '<unk>', 's', 'also', 'economic', '<unk>', 'world', 'political', 'implications', '<unk>', 'among', 'others', '<unk>', '.', 'i', 'think', 'we', '<unk>', 're', 'likely', 'to', 'see', 'tech', 'companies', 'roll', 'a', 'lot', 'more', 'products', 'to', 'encourage', 'civility', 'in', 'social', 'media', 'in', 'the', 'coming', 'years', '.', 'play', 'nice', '!', 'how', 'the', 'internet', 'is', 'trying', 'to', 'design', 'out', 'toxic', 'behaviour', 'http', '<unk>', '.', 'theguardian', '.', '<unk>', 'have', 'a', 'great', 'night', 'tom', 'good', 'talking', 'with', 'you', '.', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['when', 'you', 'come', 'from', 'a', 'totalitarian', 'society', 'that', 'is', 'very', 'tribal', 'and', 'that', 'tribalism', 'is', 'expressed', 'through', 'the', 'glue', 'of', 'religion', 'a', 'free', 'secular', 'society', 'like', 'that', 'in', 'the', 'west', 'is', 'viewed', 'quite', 'differently', '.', 'that', 'is', 'especially', 'so', 'when', 'the', 'prescripts', 'of', 'that', 'religion', 'deny', 'many', 'of', 'the', 'basic', 'human', 'rights', 'that', 'we', 'in', 'the', 'west', 'take', 'for', 'granted', '.', 'hence', 'we', 'are', 'subject', 'to', 'these', 'incomprehensible', 'terrorist', 'attacks', 'both', 'thru', 'new', 'immigrants', 'from', 'those', 'cultures', 'and', 'from', 'homegrown', 'terrorists', 'who', 'romantizes', 'the', 'whiff', 'of', '<unk>', 'century', 'cultures', '.', 'yet', 'here', 'in', 'the', 'west', 'we', 'are', 'being', 'brainwashed', 'by', 'those', 'who', 'profess', 'the', 'benefits', 'of', 'open', 'borders', '<unk>', 'but', 'not', 'open', 'societies', '<unk>', 'by', 'allowing', 'these', 'alien', 'cultures', 'to', 'profusely', 'populate', 'our', 'lands', '.', 'its', 'insidious', 'and', 'subversive', 'and', 'is', 'now', 'accompanied', 'by', 'suppression', 'of', 'free', 'speech', 'as', 'we', 'are', 'now', 'seeing', 'thrown', 'at', 'us', 'by', 'the', 'msm', 'and', 'their', 'bosses', 'in', 'the', 'wealthy', '<unk>', '<unk>', '.', 'it', 'must', 'be', 'countered', 'and', 'snuffed', 'out', '.', 'else', 'our', 'basic', 'human', 'rights', 'will', 'be', 'eroded', 'from', 'within', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n",
      "SRC: ['what', 'i', 'find', 'funny', 'is', 'the', 'loyalty', 'and', 'blindness', 'of', 'english', 'community', '.', 'the', 'worst', 'possible', 'choice', 'for', 'them', 'is', 'liberal', 'and', 'yet', 'they', 'keep', 'voting', 'for', 'them', 'every', 'time', '.', 'they', 'keep', 'renewing', 'hope', 'every', 'election', '<unk>', 'year', 'prior', 'to', 'it', 'just', 'to', 'ignore', 'them', 'at', 'the', 'winning', 'speach', 'already', '.', 'honestly', 'pq', 'have', 'more', 'respect', 'for', 'english', 'community', 'then', 'liberal', 'at', 'least', 'they', 'dont', 'lie', 'to', 'you', 'just', 'to', 'get', 'your', 'vote', '.', 'that', 'being', 'said', 'i', 'dont', 'vote', 'pq', 'either', 'tired', 'of', 'those', 'old', 'man', 'but', 'that', 'is', 'another', 'story', '.', 'i', 'mostly', 'vote', 'local', 'candidate', 'regardless', 'of', 'party', 'even', 'voted', 'liberal', 'once', '.', '.', 'outch', 'that', 'was', 'hard', 'to', 'admit', '.', 'but', 'seriously', 'guy', '<unk>', 's', 'drop', 'the', 'act', 'anti', 'pq', 'anti', 'qs', 'dont', 'vote', 'for', 'caq', 'cause', 'they', 'dont', 'win', 'etc', '.', '.', 'any', 'of', 'those', 'will', 'at', 'least', 'respect', 'you', 'when', 'they', 'say', 'no', '.', 'and', 'most', 'of', 'time', 'they', 'will', 'say', 'yes', 'and', 'act', 'on', 'it', 'not', 'just', 'saying', 'it', 'like', 'liberals', 'do', '.', '<eos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "TGT: 0\n",
      "OUT: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Classify training data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs, ids = next(iter(valloader))\n",
    "out_seqs = classify(lstm, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_tokens(pad_src_seqs[:,i], trainset.vocab))\n",
    "    print('TGT:', pad_tgt_seqs[i].item())\n",
    "    print('OUT:', out_seqs[i].item())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(dataloader=valloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(dataloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        correct += (out_seqs.squeeze().cpu().long() == tgt_labels).sum().item()\n",
    "        total += tgt_labels.shape[0]\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def test_f1_score(dataloader=valloader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(dataloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        y_true.extend(tgt_labels.cpu().numpy())\n",
    "        y_pred.extend(out_seqs.squeeze().cpu().numpy())\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993123287671233\n"
     ]
    }
   ],
   "source": [
    "# print(test_accuracy(valloader))\n",
    "print(test_f1_score(valloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df: 12001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>11996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>11998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>11999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>12000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12001 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label\n",
       "0          0      1\n",
       "1          1      0\n",
       "2          2      0\n",
       "3          3      1\n",
       "4          4      1\n",
       "...      ...    ...\n",
       "11996  11996      1\n",
       "11997  11997      1\n",
       "11998  11998      1\n",
       "11999  11999      1\n",
       "12000  12000      0\n",
       "\n",
       "[12001 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do inference on test set and save the results into csv file\n",
    "def test_inference(output_filename='submission.csv', input_file='test_2024.csv'):\n",
    "    testset = TranslationDataset(input_file, vocab=trainset.vocab, dataset_type='test')\n",
    "    testloader = DataLoader(dataset=testset, batch_size=1, shuffle=False, collate_fn=collate, pin_memory=True)\n",
    "    out = []\n",
    "    indices = []\n",
    "    for i, (src_seqs, src_seq_lengths, tgt_labels, ids) in enumerate(testloader):\n",
    "        out_seqs = classify(lstm, src_seqs, src_seq_lengths)\n",
    "        try:\n",
    "            out.extend(out_seqs.squeeze().cpu().numpy())\n",
    "        except:\n",
    "            out.append(out_seqs.squeeze().cpu().numpy())\n",
    "        indices.extend(ids)\n",
    "    df = pd.DataFrame({'id': indices, 'label': out})\n",
    "    # convert label to int\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    # sort by id\n",
    "    df = df.sort_values(by='id')\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    return df\n",
    "\n",
    "# test_inference(output_filename='dev_inference.csv', input_file='dev_2024.csv')\n",
    "test_inference(output_filename='submission.csv', input_file='test_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9110569040350666\n",
      "Recall: 0.9107433446067854\n",
      "F1: 0.9108996650122765\n"
     ]
    }
   ],
   "source": [
    "# import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Compute Precision, Recall, and F1 Score of the imported predicted csv and the validation df\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Load the predicted csv\n",
    "y_pred = pd.read_csv('dev_inference.csv', index_col=0)\n",
    "y_pred = y_pred['label'].tolist()\n",
    "\n",
    "# Load the validation df\n",
    "y_true = pd.read_csv('dev_2024.csv', quoting=3)\n",
    "y_true = y_true['label'].tolist()\n",
    "\n",
    "# Compute the metrics\n",
    "precision, recall, f1 = compute_metrics(y_true, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
