{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comptact Convolutional Transformer BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Id des GPU disponibles : 0 et 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import Any, Union, Dict, List\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import nltk\n",
    "import sklearn\n",
    "import transformers\n",
    "import torchmetrics as tm\n",
    "from torchmetrics import MetricCollection, Metric, Accuracy, Precision, Recall, AUROC, HammingDistance, F1Score, ROC, AUC, PrecisionRecallCurve\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOME_NAME = \"glove-cct-bce\"\n",
    "\n",
    "# Dataset\n",
    "DATA_DIR_PATH = os.path.abspath(\"../../data\")\n",
    "TRAIN_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"jigsaw2019-train.csv\")\n",
    "TEST_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"jigsaw2019-test.csv\")\n",
    "LABEL_LIST = ['toxicity', 'obscene', 'sexual_explicit',\n",
    "            'identity_attack', 'insult', 'threat']\n",
    "IDENTITY_LIST = ['male', 'female', 'transgender', 'other_gender', 'heterosexual',\n",
    "                'homosexual_gay_or_lesbian', 'bisexual','other_sexual_orientation',\n",
    "                'christian', 'jewish', 'muslim', 'hindu','buddhist', 'atheist',\n",
    "                'other_religion', 'black', 'white', 'asian', 'latino',\n",
    "                'other_race_or_ethnicity', 'physical_disability',\n",
    "                'intellectual_or_learning_disability',\n",
    "                'psychiatric_or_mental_illness','other_disability']\n",
    "SELECTED_IDENTITY_LIST = ['male', 'female', 'black', 'white', 'homosexual_gay_or_lesbian',\n",
    "                    'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# Session\n",
    "SESSION_DIR_PATH = os.path.abspath(\"../../session\")\n",
    "SESSION_DATETIME = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S-%f\")\n",
    "SESSION_NAME = f\"{CUSTOME_NAME}_{SESSION_DATETIME}\"\n",
    "CURRENT_SESSION_DIR_PATH = os.path.join(SESSION_DIR_PATH, SESSION_NAME)\n",
    "# Créer le dossier de la session\n",
    "os.makedirs(CURRENT_SESSION_DIR_PATH, exist_ok=True)\n",
    "\n",
    "# Architecture de fichier dans `CURRENT_SESSION_DIR_PATH`\n",
    "LOG_FILE_NAME = f\"{SESSION_NAME}.loguru.log\"\n",
    "MODEL_FILE_NAME = f\"{SESSION_NAME}.model\"\n",
    "TEST_FILE_NAME = f\"{SESSION_NAME}.test.csv\"\n",
    "VALIDATION_DATASET_NAME = f\"{SESSION_NAME}.jigsaw2019-validation.csv\"\n",
    "VALIDATION_FILE_NAME = f\"{SESSION_NAME}.validation.csv\"\n",
    "METRIC_FILE_NAME = f\"{SESSION_NAME}.metric.json\"\n",
    "LOG_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, LOG_FILE_NAME)\n",
    "MODEL_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, MODEL_FILE_NAME)\n",
    "TEST_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, TEST_FILE_NAME)\n",
    "VALIDATION_DATASET_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, VALIDATION_DATASET_NAME)\n",
    "VALIDATION_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, VALIDATION_FILE_NAME)\n",
    "METRIC_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, METRIC_FILE_NAME)\n",
    "\n",
    "# CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:19:52.478 | INFO     | __main__:<cell line: 2>:2 - SESSION_NAME='test-freeze-glove-cct-bce_2022-03-30T23-19-52-188703'\n",
      "2022-03-30 23:19:52.532 | INFO     | __main__:<cell line: 3>:3 - TRAIN_DATASET_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/data/jigsaw2019-train.csv'\n",
      "2022-03-30 23:19:52.534 | INFO     | __main__:<cell line: 4>:4 - TEST_DATASET_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/data/jigsaw2019-test.csv'\n",
      "2022-03-30 23:19:52.537 | INFO     | __main__:<cell line: 5>:5 - CURRENT_SESSION_DIR_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/session/test-freeze-glove-cct-bce_2022-03-30T23-19-52-188703'\n",
      "2022-03-30 23:19:52.539 | INFO     | __main__:<cell line: 6>:6 - LABEL_LIST=['toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat']\n",
      "2022-03-30 23:19:52.541 | INFO     | __main__:<cell line: 7>:7 - IDENTITY_LIST=['male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']\n",
      "2022-03-30 23:19:52.543 | INFO     | __main__:<cell line: 8>:8 - SELECTED_IDENTITY_LIST=['male', 'female', 'black', 'white', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness']\n"
     ]
    }
   ],
   "source": [
    "logger.add(LOG_FILE_PATH, level=\"TRACE\")\n",
    "logger.info(f\"{SESSION_NAME=}\")\n",
    "logger.info(f\"{TRAIN_DATASET_PATH=}\")\n",
    "logger.info(f\"{TEST_DATASET_PATH=}\")\n",
    "logger.info(f\"{CURRENT_SESSION_DIR_PATH=}\")\n",
    "logger.info(f\"{LABEL_LIST=}\")\n",
    "logger.info(f\"{IDENTITY_LIST=}\")\n",
    "logger.info(f\"{SELECTED_IDENTITY_LIST=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifier la cohérence de l'architecture et l'accès aux ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:19:52.843 | INFO     | __main__:<cell line: 1>:1 - Checking consistency...\n",
      "2022-03-30 23:19:52.847 | SUCCESS  | __main__:<cell line: 10>:10 - Datasets are reachable\n",
      "2022-03-30 23:19:52.849 | INFO     | __main__:<cell line: 15>:15 - GPU_IS_AVAILABLE=True\n",
      "2022-03-30 23:19:52.852 | INFO     | __main__:<cell line: 16>:16 - GPU_COUNT=1\n",
      "2022-03-30 23:19:52.853 | SUCCESS  | __main__:<cell line: 20>:20 - GPU and CUDA are available\n",
      "2022-03-30 23:19:52.855 | INFO     | __main__:<cell line: 21>:21 - device=device(type='cuda')\n",
      "2022-03-30 23:19:52.857 | INFO     | __main__:<cell line: 22>:24 - GPU 0 : NVIDIA TITAN X (Pascal)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Checking consistency...\")\n",
    "\n",
    "# Vérifier l'accès aux datasets\n",
    "if not os.path.exists(TRAIN_DATASET_PATH):\n",
    "    logger.critical(f\"Train dataset does not exist !\")\n",
    "    raise RuntimeError(\"Train dataset does not exist !\")\n",
    "if not os.path.exists(TEST_DATASET_PATH):\n",
    "    logger.critical(f\"Test dataset does not exist !\")\n",
    "    raise RuntimeError(\"Test dataset does not exist !\")\n",
    "logger.success(\"Datasets are reachable\")\n",
    "\n",
    "# Vérifier l'accès aux GPU\n",
    "GPU_IS_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_COUNT = torch.cuda.device_count()\n",
    "logger.info(f\"{GPU_IS_AVAILABLE=}\")\n",
    "logger.info(f\"{GPU_COUNT=}\")\n",
    "if not GPU_IS_AVAILABLE:\n",
    "    logger.critical(\"GPU and CUDA are not available !\")\n",
    "    raise RuntimeError(\"GPU and CUDA are not available !\")\n",
    "logger.success(\"GPU and CUDA are available\")\n",
    "logger.info(f\"{device=}\")\n",
    "for gpu_id in range(GPU_COUNT):\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    logger.info(f\"GPU {gpu_id} : {gpu_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:20:01.872 | SUCCESS  | __main__:<cell line: 2>:2 - Dataset loaded !\n"
     ]
    }
   ],
   "source": [
    "all_train_df = pd.read_csv(TRAIN_DATASET_PATH, index_col=0)\n",
    "logger.success(\"Dataset loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:20:02.242 | DEBUG    | __main__:<cell line: 4>:6 - Mode test is enabled. The training set has been truncated to 20 000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Pour réduire le nombre d'exemple et savoir sur quel groupes d'identités\n",
    "# le modèle est entrainé, on prend un sous ensemble du jeu de données\n",
    "train_df = all_train_df[~all_train_df.white.isna()]\n",
    "if CUSTOME_NAME.startswith(\"test\"):\n",
    "    # Si c'est juste une session pour tester le notebook\n",
    "    logger.debug(\"Mode test is enabled. The training set has been truncated to 20 000 samples.\")\n",
    "    train_df = train_df[:20_000]\n",
    "# Pour le jeu de validation, on veut juste avoir des indications de perf\n",
    "# Peu importe, c'est juste pour voir si l'entraînement s'est bien passé\n",
    "# Sans pour autant regarder les biais\n",
    "validation_df = all_train_df[all_train_df.white.isna()].sample(n=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer toutes les colonnes correspondantes aux labels par 1 ou 0\n",
    "# si la probabilité est supérieure ou égale à 0.5 ou non\n",
    "train_df[LABEL_LIST] = (train_df[LABEL_LIST]>=0.5).astype(int)\n",
    "validation_df[LABEL_LIST] = (validation_df[LABEL_LIST]>=0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:20:03.148 | INFO     | __main__:<cell line: 5>:5 - Tokenizer : basic_english\n",
      "2022-03-30 23:20:03.151 | INFO     | __main__:<cell line: 8>:8 - Tokenizer : min_freq=5\n",
      "2022-03-30 23:20:03.153 | INFO     | __main__:<cell line: 10>:10 - Tokenizer : special_tokens=['<unk>', '<pad>']\n",
      "2022-03-30 23:20:03.465 | INFO     | __main__:<cell line: 14>:14 - Tokens are stored in column of train_df\n",
      "2022-03-30 23:20:03.641 | INFO     | __main__:<cell line: 19>:19 - Vocab is built\n"
     ]
    }
   ],
   "source": [
    "# For GloVe tokenizer and vocabulary\n",
    "# Get the tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "max_len = 70\n",
    "logger.info(f\"Tokenizer : basic_english\")\n",
    "# Build vocab\n",
    "min_freq = 5\n",
    "logger.info(f\"Tokenizer : {min_freq=}\")\n",
    "special_tokens = ['<unk>', '<pad>']\n",
    "logger.info(f\"Tokenizer : {special_tokens=}\")\n",
    "\n",
    "# Create tokenized column\n",
    "train_df['tokens'] = train_df['comment_text'].apply(lambda x: tokenizer(x)[:max_len])\n",
    "logger.info(f\"Tokens are stored in column of train_df\")\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(train_df['tokens'],\n",
    "                                                  min_freq=min_freq,\n",
    "                                                  specials=special_tokens)\n",
    "logger.info(f\"Vocab is built\")\n",
    "\n",
    "unk_index = vocab['<unk>']\n",
    "pad_index = vocab['<pad>']\n",
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, vocab, max_len=64):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.data.iloc[index][\"comment_text\"]\n",
    "        label = torch.tensor(self.data.iloc[index][LABEL_LIST].tolist(), dtype=torch.float)\n",
    "        \n",
    "        # Tokenize the sentence\n",
    "        tokenized_sentence = self.tokenizer(comment)[:self.max_len]\n",
    "\n",
    "        # Get ids from tokens\n",
    "        ids = torch.tensor([self.vocab[token] for token in tokenized_sentence])\n",
    "\n",
    "        # Pad the sequence\n",
    "        padding = torch.full((self.max_len - len(ids), ), self.vocab['<pad>'])\n",
    "        padded_ids = torch.cat([ids, padding])\n",
    "\n",
    "        return dict(index=index, ids=padded_ids, labels=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation from:\n",
    "# https://github.com/rishikksh20/compact-convolution-transformer/blob/master/cct.py\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "# from module import Attention, PreNorm, FeedForward\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=7, stride=2, padding=3, pool_kernel_size=3, pool_stride=2,\n",
    "                 pool_padding=1):\n",
    "        super(ConvEmbed, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,\n",
    "                    padding=padding, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride, padding=pool_padding),\n",
    "            Rearrange('b d h w -> b (h w) d')\n",
    "            )\n",
    "\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
    "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class CompactTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim=768, depth=12, heads=12, pool='cls', in_channels=3,\n",
    "                 dim_head=64, dropout=0.1, emb_dropout=0.1, scale_dim=4, conv_embed=False):\n",
    "        super().__init__()\n",
    "\n",
    "        width, height = image_size\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        assert width % patch_size == 0, 'Image dimensions (width) must be divisible by the patch size.'\n",
    "        assert height % patch_size == 0, 'Image dimensions (height) must be divisible by the patch size.'\n",
    "        # num_patches = (image_size // patch_size) ** 2\n",
    "        num_patches = (width // patch_size) * (height // patch_size)\n",
    "        patch_dim = in_channels * patch_size ** 2\n",
    "\n",
    "        if conv_embed:\n",
    "            self.to_patch_embedding = ConvEmbed(in_channels, dim)\n",
    "            num_patches = self.to_patch_embedding.sequence_length(n_channels=in_channels,\n",
    "                                                                  height=height,\n",
    "                                                                  width=width)\n",
    "        else:\n",
    "            self.to_patch_embedding = nn.Sequential(\n",
    "                Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "                nn.Linear(patch_dim, dim),\n",
    "            )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.2)\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, dim*scale_dim, dropout)\n",
    "\n",
    "        self.pool = nn.Linear(dim, 1)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        g = self.pool(x)\n",
    "        xl = F.softmax(g, dim=1)\n",
    "        x = einsum('b n l, b n d -> b l d', xl, x)\n",
    "\n",
    "        return self.mlp_head(x.squeeze(-2))\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "class CCTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nb_labels: int = 6,\n",
    "                 in_channels: int = 3,\n",
    "                 pretrained_embedding: str = \"glove\",\n",
    "                 embedding_dim: int = 300,\n",
    "                 freeze_embedding: bool = False,\n",
    "                 vocab = None,\n",
    "                 img_size: tuple = (60,60),\n",
    "                 patch_size: int = 16,\n",
    "                 cct_dim: int = 768,\n",
    "                 cct_depth: int = 12,\n",
    "                 cct_heads: int = 12,\n",
    "                 cct_dim_head: int = 64,\n",
    "                 conv_embed: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        pad_index = vocab['<pad>']\n",
    "\n",
    "        # Embedding type\n",
    "        if pretrained_embedding == \"glove\":\n",
    "            self.embedding = nn.Embedding(vocab_size, 300, padding_idx=pad_index)\n",
    "\n",
    "            # Download pretrained glove embedding\n",
    "            vectors = torchtext.vocab.GloVe()\n",
    "\n",
    "            # Get the pretrained embedding vectors according to the vocabulary\n",
    "            pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
    "            self.embedding.weight.data = pretrained_embedding\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "\n",
    "        self.cct = CompactTransformer(img_size,\n",
    "                                      patch_size,\n",
    "                                      nb_labels,\n",
    "                                      in_channels=in_channels,\n",
    "                                      dim=cct_dim,\n",
    "                                      depth=cct_depth,\n",
    "                                      heads=cct_heads,\n",
    "                                      dim_head=cct_dim_head,\n",
    "                                      conv_embed=conv_embed)\n",
    "\n",
    "        # freeze all the embedding parameters if necessary\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = not freeze_embedding\n",
    "\n",
    "    def forward(self, ids):\n",
    "        # ids = [batch_size, padded_seq_len]\n",
    "        x = self.embedding(ids)\n",
    "\n",
    "        # x = [batch_size, padded_seq_len, embedding_dim]\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "\n",
    "        # x = [batch_size, 1, padded_seq_len, embedding_dim]\n",
    "        x = self.cct(x)\n",
    "\n",
    "        # x = [batch_size, nb_labels]\n",
    "        return x\n",
    "\n",
    "def preds_fn(batch, model, device):\n",
    "    '''\n",
    "    Get the predictions for one batch according to the model\n",
    "    '''\n",
    "    ids = batch['ids'].int()\n",
    "    b_input = ids.to(device, non_blocking=True)\n",
    "    return model(b_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 70, 300\n",
    "\n",
    "model = CCTClassifier(img_size=img_size,\n",
    "                     vocab=vocab,\n",
    "                     in_channels=1,\n",
    "                     pretrained_embedding=\"glove\",\n",
    "                     patch_size=10,\n",
    "                     freeze_embedding=False,\n",
    "                     cct_depth=6,\n",
    "                     cct_heads=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:20:09.945 | INFO     | __main__:<cell line: 7>:7 - BATCH_SIZE=8\n",
      "2022-03-30 23:20:09.948 | INFO     | __main__:<cell line: 8>:8 - LR=0.0001\n",
      "2022-03-30 23:20:09.949 | INFO     | __main__:<cell line: 9>:9 - PIN_MEMORY=True\n",
      "2022-03-30 23:20:09.951 | INFO     | __main__:<cell line: 10>:10 - NUM_WORKERS=0\n",
      "2022-03-30 23:20:09.952 | INFO     | __main__:<cell line: 11>:11 - PREFETCH_FACTOR=2\n",
      "2022-03-30 23:20:09.953 | INFO     | __main__:<cell line: 12>:12 - NUM_EPOCHS=1\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "LR=1e-4\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 0\n",
    "PREFETCH_FACTOR = 2\n",
    "NUM_EPOCHS = 1\n",
    "logger.info(f\"{BATCH_SIZE=}\")\n",
    "logger.info(f\"{LR=}\")\n",
    "logger.info(f\"{PIN_MEMORY=}\")\n",
    "logger.info(f\"{NUM_WORKERS=}\")\n",
    "logger.info(f\"{PREFETCH_FACTOR=}\")\n",
    "logger.info(f\"{NUM_EPOCHS=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 gamma: float = 2,\n",
    "                 reduction: str = \"mean\",\n",
    "                 pos_weight: torch.Tensor = None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma= gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor,\n",
    "                targets: torch.Tensor):\n",
    "        p = torch.sigmoid(inputs)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction=\"none\", pos_weight=self.pos_weight\n",
    "        )\n",
    "        p_t =  p * targets + (1 - p) * (1 - targets)\n",
    "        loss = ce_loss * ((1 - p_t) ** self.gamma)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from: https://github.com/Roche/BalancedLossNLP\n",
    "\n",
    "class ResampleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 use_sigmoid=True, partial=False,\n",
    "                 loss_weight=1.0, reduction='mean',\n",
    "                 reweight_func=None,  # None, 'inv', 'sqrt_inv', 'rebalance', 'CB'\n",
    "                 weight_norm=None, # None, 'by_instance', 'by_batch'\n",
    "                 focal=dict(\n",
    "                     focal=True,\n",
    "                     alpha=0.5,\n",
    "                     gamma=2,\n",
    "                 ),\n",
    "                 map_param=dict(\n",
    "                     alpha=10.0,\n",
    "                     beta=0.2,\n",
    "                     gamma=0.1\n",
    "                 ),\n",
    "                 CB_loss=dict(\n",
    "                     CB_beta=0.9,\n",
    "                     CB_mode='average_w'  # 'by_class', 'average_n', 'average_w', 'min_n'\n",
    "                 ),\n",
    "                 logit_reg=dict(\n",
    "                     neg_scale=5.0,\n",
    "                     init_bias=0.1\n",
    "                 ),\n",
    "                 class_freq=None,\n",
    "                 train_num=None):\n",
    "        super(ResampleLoss, self).__init__()\n",
    "\n",
    "        assert (use_sigmoid is True) or (partial is False)\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.partial = partial\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "        if self.use_sigmoid:\n",
    "            if self.partial:\n",
    "                raise RuntimeError(\"Not defined here\")\n",
    "                self.cls_criterion = partial_cross_entropy\n",
    "            else:\n",
    "                self.cls_criterion = binary_cross_entropy\n",
    "        else:\n",
    "            raise RuntimeError(\"Not defined here\")\n",
    "            self.cls_criterion = cross_entropy\n",
    "\n",
    "        # reweighting function\n",
    "        self.reweight_func = reweight_func\n",
    "\n",
    "        # normalization (optional)\n",
    "        self.weight_norm = weight_norm\n",
    "\n",
    "        # focal loss params\n",
    "        self.focal = focal['focal']\n",
    "        self.gamma = focal['gamma']\n",
    "        self.alpha = focal['alpha'] # change to alpha\n",
    "\n",
    "        # mapping function params\n",
    "        self.map_alpha = map_param['alpha']\n",
    "        self.map_beta = map_param['beta']\n",
    "        self.map_gamma = map_param['gamma']\n",
    "\n",
    "        # CB loss params (optional)\n",
    "        self.CB_beta = CB_loss['CB_beta']\n",
    "        self.CB_mode = CB_loss['CB_mode']\n",
    "\n",
    "        self.class_freq = torch.from_numpy(np.asarray(class_freq)).float().cuda()\n",
    "        self.num_classes = self.class_freq.shape[0]\n",
    "        self.train_num = train_num # only used to be divided by class_freq\n",
    "        # regularization params\n",
    "        self.logit_reg = logit_reg\n",
    "        self.neg_scale = logit_reg[\n",
    "            'neg_scale'] if 'neg_scale' in logit_reg else 1.0\n",
    "        init_bias = logit_reg['init_bias'] if 'init_bias' in logit_reg else 0.0\n",
    "        # bug fixed https://github.com/wutong16/DistributionBalancedLoss/issues/8\n",
    "        self.init_bias = - torch.log(\n",
    "            self.train_num / self.class_freq - 1) * init_bias\n",
    "\n",
    "        self.freq_inv = torch.ones(self.class_freq.shape).cuda() / self.class_freq\n",
    "        self.propotion_inv = self.train_num / self.class_freq\n",
    "\n",
    "    def forward(self,\n",
    "                cls_score,\n",
    "                label,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None,\n",
    "                **kwargs):\n",
    "\n",
    "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
    "        reduction = (\n",
    "            reduction_override if reduction_override else self.reduction)\n",
    "\n",
    "        weight = self.reweight_functions(label)\n",
    "\n",
    "        cls_score, weight = self.logit_reg_functions(label.float(), cls_score, weight)\n",
    "\n",
    "        if self.focal:\n",
    "            logpt = self.cls_criterion(\n",
    "                cls_score.clone(), label, weight=None, reduction='none',\n",
    "                avg_factor=avg_factor)\n",
    "            # pt is sigmoid(logit) for pos or sigmoid(-logit) for neg\n",
    "            pt = torch.exp(-logpt)\n",
    "            wtloss = self.cls_criterion(\n",
    "                cls_score, label.float(), weight=weight, reduction='none')\n",
    "            alpha_t = torch.where(label==1, self.alpha, 1-self.alpha)\n",
    "            loss = alpha_t * ((1 - pt) ** self.gamma) * wtloss # balance_param should be a tensor\n",
    "            loss = reduce_loss(loss, reduction)             # add reduction\n",
    "        else:\n",
    "            loss = self.cls_criterion(cls_score, label.float(), weight,\n",
    "                                      reduction=reduction)\n",
    "\n",
    "        loss = self.loss_weight * loss\n",
    "        return loss\n",
    "\n",
    "    def reweight_functions(self, label):\n",
    "        if self.reweight_func is None:\n",
    "            return None\n",
    "        elif self.reweight_func in ['inv', 'sqrt_inv']:\n",
    "            weight = self.RW_weight(label.float())\n",
    "        elif self.reweight_func in 'rebalance':\n",
    "            weight = self.rebalance_weight(label.float())\n",
    "        elif self.reweight_func in 'CB':\n",
    "            weight = self.CB_weight(label.float())\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if self.weight_norm is not None:\n",
    "            if 'by_instance' in self.weight_norm:\n",
    "                max_by_instance, _ = torch.max(weight, dim=-1, keepdim=True)\n",
    "                weight = weight / max_by_instance\n",
    "            elif 'by_batch' in self.weight_norm:\n",
    "                weight = weight / torch.max(weight)\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def logit_reg_functions(self, labels, logits, weight=None): \n",
    "        if not self.logit_reg:\n",
    "            return logits, weight\n",
    "        if 'init_bias' in self.logit_reg:\n",
    "            logits += self.init_bias\n",
    "        if 'neg_scale' in self.logit_reg:\n",
    "            logits = logits * (1 - labels) * self.neg_scale  + logits * labels\n",
    "            if weight is not None:\n",
    "                weight = weight / self.neg_scale * (1 - labels) + weight * labels\n",
    "        return logits, weight\n",
    "\n",
    "    def rebalance_weight(self, gt_labels):\n",
    "        repeat_rate = torch.sum( gt_labels.float() * self.freq_inv, dim=1, keepdim=True)\n",
    "        pos_weight = self.freq_inv.clone().detach().unsqueeze(0) / repeat_rate\n",
    "        # pos and neg are equally treated\n",
    "        weight = torch.sigmoid(self.map_beta * (pos_weight - self.map_gamma)) + self.map_alpha\n",
    "        return weight\n",
    "\n",
    "    def CB_weight(self, gt_labels):\n",
    "        if  'by_class' in self.CB_mode:\n",
    "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
    "                     (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n",
    "        elif 'average_n' in self.CB_mode:\n",
    "            avg_n = torch.sum(gt_labels * self.class_freq, dim=1, keepdim=True) / \\\n",
    "                    torch.sum(gt_labels, dim=1, keepdim=True)\n",
    "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
    "                     (1 - torch.pow(self.CB_beta, avg_n)).cuda()\n",
    "        elif 'average_w' in self.CB_mode:\n",
    "            weight_ = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
    "                      (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n",
    "            weight = torch.sum(gt_labels * weight_, dim=1, keepdim=True) / \\\n",
    "                     torch.sum(gt_labels, dim=1, keepdim=True)\n",
    "        elif 'min_n' in self.CB_mode:\n",
    "            min_n, _ = torch.min(gt_labels * self.class_freq +\n",
    "                                 (1 - gt_labels) * 100000, dim=1, keepdim=True)\n",
    "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
    "                     (1 - torch.pow(self.CB_beta, min_n)).cuda()\n",
    "        else:\n",
    "            raise NameError\n",
    "        return weight\n",
    "\n",
    "    def RW_weight(self, gt_labels, by_class=True):\n",
    "        if 'sqrt' in self.reweight_func:\n",
    "            weight = torch.sqrt(self.propotion_inv)\n",
    "        else:\n",
    "            weight = self.propotion_inv\n",
    "        if not by_class:\n",
    "            sum_ = torch.sum(weight * gt_labels, dim=1, keepdim=True)\n",
    "            weight = sum_ / torch.sum(gt_labels, dim=1, keepdim=True)\n",
    "        return weight\n",
    "    \n",
    "\n",
    "def reduce_loss(loss, reduction):\n",
    "    \"\"\"Reduce loss as specified.\n",
    "    Args:\n",
    "        loss (Tensor): Elementwise loss tensor.\n",
    "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
    "    Return:\n",
    "        Tensor: Reduced loss tensor.\n",
    "    \"\"\"\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # none: 0, elementwise_mean:1, sum: 2\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    elif reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "    elif reduction_enum == 2:\n",
    "        return loss.sum()\n",
    "\n",
    "\n",
    "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Apply element-wise weight and reduce loss.\n",
    "    Args:\n",
    "        loss (Tensor): Element-wise loss.\n",
    "        weight (Tensor): Element-wise weights.\n",
    "        reduction (str): Same as built-in losses of PyTorch.\n",
    "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
    "    Returns:\n",
    "        Tensor: Processed loss values.\n",
    "    \"\"\"\n",
    "    # if weight is specified, apply element-wise weight\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    # if avg_factor is not specified, just reduce the loss\n",
    "    if avg_factor is None:\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    else:\n",
    "        # if reduction is mean, then average the loss by avg_factor\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.sum() / avg_factor\n",
    "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
    "        elif reduction != 'none':\n",
    "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def binary_cross_entropy(pred,\n",
    "                         label,\n",
    "                         weight=None,\n",
    "                         reduction='mean',\n",
    "                         avg_factor=None):\n",
    "\n",
    "    # weighted element-wise losses\n",
    "    if weight is not None:\n",
    "        weight = weight.float()\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, label.float(), weight, reduction='none')\n",
    "    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_weights_bce(df, classes=LABEL_LIST):\n",
    "    weights = torch.empty((len(classes),))\n",
    "\n",
    "    nb_samples = len(df)\n",
    "\n",
    "    for idx, c in enumerate(classes):\n",
    "        nb_zeros = len(df[df[c] == 0])\n",
    "        nb_ones = nb_samples - nb_zeros\n",
    "        weights[idx] = nb_zeros / nb_ones\n",
    "\n",
    "    return weights\n",
    "\n",
    "def get_label_inv_freq(df, classes=LABEL_LIST):\n",
    "    weights = torch.empty((len(classes),))\n",
    "    nb_samples = len(df)\n",
    "\n",
    "    for idx, c in enumerate(classes):\n",
    "        nb_zeros = len(df[df[c] == 0])\n",
    "        weights[idx] = (nb_zeros / nb_samples)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def get_nb_samples_lab(df, classes=LABEL_LIST):\n",
    "    nb_ones_tot, nb_zeros_tot = [], []\n",
    "    nb_tot = len(df)\n",
    "\n",
    "    for c in classes:\n",
    "        nb_zeros = len(df[df[c] == 0])\n",
    "        nb_ones = nb_tot - nb_zeros\n",
    "\n",
    "        nb_ones_tot.append(nb_ones)\n",
    "        nb_zeros_tot.append(nb_zeros)\n",
    "\n",
    "    return torch.tensor(nb_ones_tot), torch.tensor(nb_zeros_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier les différents objets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:20:11.538 | INFO     | __main__:<cell line: 23>:23 - BCEWithLogitsLoss()\n",
      "2022-03-30 23:20:11.543 | INFO     | __main__:<cell line: 25>:25 - AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = JigsawDataset(train_df, tokenizer,\n",
    "                              vocab=vocab,\n",
    "                              max_len=max_len)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             prefetch_factor=PREFETCH_FACTOR,\n",
    "                             pin_memory=PIN_MEMORY)\n",
    "\n",
    "validation_dataset = JigsawDataset(validation_df, tokenizer,\n",
    "                              vocab=vocab,\n",
    "                              max_len=max_len)\n",
    "validation_dataloader = DataLoader(validation_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             prefetch_factor=PREFETCH_FACTOR,\n",
    "                             pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Pas besoin de Sigmoid en sorti du model seulement pour `BCEWithLogitsLoss`\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "logger.info(criterion)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "logger.info(optimizer)\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variantes de Hamming Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HammingLossWithoutThreshold(Metric):\n",
    "    def __init__(self, num_classes=1, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.add_state(\"total\", default=torch.tensor(0, dtype=torch.float32), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        current_nbr_sample, current_nbr_category = preds.shape\n",
    "        if current_nbr_category != self.num_classes:\n",
    "          raise AttributeError(\"`num_classes` != `current_nbr_category` detected in `pred` parameter\")\n",
    "        \n",
    "        current_loss_per_pred = torch.absolute(target - preds)\n",
    "        current_hamming_loss = current_loss_per_pred.sum()\n",
    "\n",
    "        self.total += current_hamming_loss.float()\n",
    "        self.nbr_sample += current_nbr_sample\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total/(self.num_classes*self.nbr_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RebalancedHammingLossWithoutThreshold(Metric):\n",
    "    def __init__(self, num_classes=1, average=\"macro\", dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # average = \"macro\" or None\n",
    "        self.average = average\n",
    "\n",
    "        # Nombre de positif 1 & negatif 0 par categorie\n",
    "        self.add_state(\n",
    "            \"number_positive\",\n",
    "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "        self.add_state(\n",
    "            \"number_negative\",\n",
    "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.add_state(\n",
    "            \"hamming_loss_positive\",\n",
    "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "        self.add_state(\n",
    "            \"hamming_loss_negative\",\n",
    "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        current_nbr_sample, current_nbr_category = preds.shape\n",
    "        if current_nbr_category != self.num_classes:\n",
    "            raise AttributeError(\n",
    "                \"`num_classes` != `current_nbr_category` detected in `pred` parameter\"\n",
    "            )\n",
    "\n",
    "        # Nombre de positif 1 & negatif 0 par categorie\n",
    "        current_number_positive = target.sum(axis=0)\n",
    "        current_number_negative = current_nbr_sample - target.sum(axis=0)\n",
    "\n",
    "        self.number_positive += current_number_positive.int()\n",
    "        self.number_negative += current_number_negative.int()\n",
    "\n",
    "        self.nbr_sample += current_nbr_sample\n",
    "\n",
    "        for class_id in range(self.num_classes):\n",
    "            positive_filter = target[:, class_id] == 1\n",
    "            negative_filter = target[:, class_id] == 0\n",
    "\n",
    "            target_vector = target[:, class_id]\n",
    "            preds_vector = preds[:, class_id]\n",
    "\n",
    "            # Filtered vector\n",
    "            ## Target\n",
    "            pos_filtered_target_vector = target_vector[positive_filter]\n",
    "            neg_filtered_target_vector = target_vector[negative_filter]\n",
    "            ## Preds\n",
    "            pos_filtered_preds_vector = preds_vector[positive_filter]\n",
    "            neg_filtered_preds_vector = preds_vector[negative_filter]\n",
    "\n",
    "            # Hamming Loss without Threshold\n",
    "            hamming_loss_on_positive = torch.absolute(\n",
    "                pos_filtered_target_vector - pos_filtered_preds_vector\n",
    "            )\n",
    "            hamming_loss_on_negative = torch.absolute(\n",
    "                neg_filtered_target_vector - neg_filtered_preds_vector\n",
    "            )\n",
    "\n",
    "            self.hamming_loss_positive[class_id] += hamming_loss_on_positive.sum()\n",
    "            self.hamming_loss_negative[class_id] += hamming_loss_on_negative.sum()\n",
    "\n",
    "    def compute(self):\n",
    "        factor_pos = self.nbr_sample / (2 * self.number_positive)\n",
    "        factor_neg = self.nbr_sample / (2 * self.number_negative)\n",
    "\n",
    "        rebalanced_hamming_loss_per_class = torch.multiply(\n",
    "            self.hamming_loss_positive, factor_pos\n",
    "        ) + torch.multiply(self.hamming_loss_negative, factor_neg)\n",
    "        if self.average == \"macro\":\n",
    "            return rebalanced_hamming_loss_per_class.sum() / (\n",
    "                self.nbr_sample * self.num_classes\n",
    "            )\n",
    "        return rebalanced_hamming_loss_per_class / (self.nbr_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciation des metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(LABEL_LIST)\n",
    "train_metric_dict = dict()\n",
    "\n",
    "# AUROC Macro\n",
    "auroc_macro = AUROC(num_classes=num_classes, compute_on_step=True, average=\"macro\")\n",
    "train_metric_dict[\"auroc_macro\"] = auroc_macro\n",
    "\n",
    "# AUROC per class\n",
    "auroc_per_class = AUROC(num_classes=num_classes, compute_on_step=True, average=None)\n",
    "train_metric_dict[\"auroc_per_class\"] = auroc_per_class\n",
    "\n",
    "# F1 score global\n",
    "f1 = F1Score()\n",
    "train_metric_dict[\"f1\"] = f1\n",
    "\n",
    "# F1 score per class\n",
    "f1_per_calss = F1Score(num_classes=6, average=None)\n",
    "train_metric_dict[\"f1_per_calss\"] = f1_per_calss\n",
    "\n",
    "# Hamming Distance without Threshold\n",
    "hamming_loss_woutt = HammingLossWithoutThreshold(num_classes=num_classes)\n",
    "train_metric_dict[\"hamming_loss_without_threshold\"] = hamming_loss_woutt\n",
    "\n",
    "# Rebalanced Hamming Distance without Threshold macro\n",
    "rebalanced_hamming_loss_woutt_macro = RebalancedHammingLossWithoutThreshold(\n",
    "    num_classes=num_classes, average=\"macro\"\n",
    ")\n",
    "train_metric_dict[\n",
    "    \"rebalanced_hamming_loss_without_threshold_macro\"\n",
    "] = rebalanced_hamming_loss_woutt_macro\n",
    "\n",
    "# Rebalanced Hamming Distance without Threshold macro\n",
    "rebalanced_hamming_loss_woutt_per_class = RebalancedHammingLossWithoutThreshold(\n",
    "    num_classes=num_classes, average=None\n",
    ")\n",
    "train_metric_dict[\n",
    "    \"rebalanced_hamming_loss_without_threshold_per_class\"\n",
    "] = rebalanced_hamming_loss_woutt_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricCollection(\n",
       "  (auroc_macro): AUROC()\n",
       "  (auroc_per_class): AUROC()\n",
       "  (f1): F1Score()\n",
       "  (f1_per_calss): F1Score()\n",
       "  (hamming_loss_without_threshold): HammingLossWithoutThreshold()\n",
       "  (rebalanced_hamming_loss_without_threshold_macro): RebalancedHammingLossWithoutThreshold()\n",
       "  (rebalanced_hamming_loss_without_threshold_per_class): RebalancedHammingLossWithoutThreshold()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metric = MetricCollection(train_metric_dict)\n",
    "train_metric.to(device)\n",
    "\n",
    "validation_metric = train_metric.clone()\n",
    "validation_metric.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(object_to_serialize: Any, ensure_ascii: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Serialize any object, i.e. convert an object to JSON\n",
    "    Args:\n",
    "        object_to_serialize (Any): The object to serialize\n",
    "        ensure_ascii (bool, optional): If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is. Defaults to True.\n",
    "    Returns:\n",
    "            str: string of serialized object (JSON)\n",
    "    \"\"\"\n",
    "\n",
    "    def dumper(obj: Any) -> Union[str, Dict]:\n",
    "        \"\"\"\n",
    "        Function called recursively by json.dumps to know how to serialize an object.\n",
    "        For example, for datetime, we try to convert it to ISO format rather than\n",
    "        retrieve the list of attributes defined in its object.\n",
    "        Args:\n",
    "            obj (Any): The object to serialize\n",
    "        Returns:\n",
    "            Union[str, Dict]: Serialized object\n",
    "        \"\"\"\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().numpy().tolist()\n",
    "        elif hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "    return json.dumps(object_to_serialize, default=dumper, ensure_ascii=ensure_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_metric(metric_collection, **kwargs):\n",
    "    \"\"\"\n",
    "    Export MetricCollection to json file\n",
    "\n",
    "    Args:\n",
    "        metric_collection: MetricCollection\n",
    "        **kwargs: field to add in json line\n",
    "    \"\"\"\n",
    "    with open(METRIC_FILE_PATH, \"a\") as f:\n",
    "        metric_collection_value = metric_collection.compute()\n",
    "        metric_collection_value.update(kwargs)\n",
    "        serialized_value = serialize(metric_collection_value)\n",
    "        f.write(serialized_value)\n",
    "        f.write(\"\\n\")\n",
    "    logger.success(\"Metrics are exported !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_id=None):\n",
    "    model.train()\n",
    "    logger.info(f\"START EPOCH {epoch_id=}\")\n",
    "\n",
    "    progress = tqdm(train_dataloader, desc='training batch...', leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        if batch_id % 1_000 == 0:\n",
    "            #valid_epoch(epoch_id=epoch, batch_id=batch_id)\n",
    "            pass\n",
    "        \n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch)\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(transformed_prediction_batch.to(torch.float32), label_batch.to(torch.float32))\n",
    "\n",
    "        # Metrics\n",
    "        proba_prediction_batch = torch.sigmoid(transformed_prediction_batch)\n",
    "        train_metrics_collection_dict = train_metric(proba_prediction_batch.to(torch.float32), label_batch.to(torch.int32))\n",
    "        logger.trace(train_metrics_collection_dict)\n",
    "\n",
    "        # Backprop        \n",
    "        loss.backward()\n",
    "        # gradient clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar description\n",
    "        progress_description = \"Train Loss : {loss:.4f} - Train AUROC : {acc:.4f}\"\n",
    "        auroc_macro_value = float(train_metrics_collection_dict[\"auroc_macro\"])\n",
    "        progress_description = progress_description.format(loss=loss.item(), acc=auroc_macro_value)\n",
    "        progress.set_description(progress_description)\n",
    "\n",
    "    logger.info(f\"END EPOCH {epoch_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_epoch(epoch_id=None, batch_id=None):\n",
    "    model.eval()\n",
    "    logger.info(f\"START VALIDATION {epoch_id=}{batch_id=}\")\n",
    "    validation_metric.reset()\n",
    "\n",
    "    loss_list = []\n",
    "    prediction_list = torch.Tensor([])\n",
    "    target_list = torch.Tensor([])\n",
    "\n",
    "\n",
    "    progress = tqdm(validation_dataloader, desc=\"valid batch...\", leave=False)\n",
    "    for _, batch in enumerate(progress):\n",
    "        \n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch)\n",
    "\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(\n",
    "            transformed_prediction_batch.to(torch.float32),\n",
    "            label_batch.to(torch.float32),\n",
    "        )\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        proba_prediction_batch = torch.sigmoid(transformed_prediction_batch)\n",
    "        prediction_list = torch.concat(\n",
    "            [prediction_list, proba_prediction_batch.cpu()]\n",
    "        )\n",
    "        target_list = torch.concat([target_list, label_batch.cpu()])\n",
    "\n",
    "        # Metrics\n",
    "        validation_metric(proba_prediction_batch.to(torch.float32), label_batch.to(torch.int32))\n",
    "\n",
    "    loss_mean = np.mean(loss_list)\n",
    "    logger.trace(validation_metric.compute())\n",
    "    logger.info(f\"END VALIDATION {epoch_id=}{batch_id=}\")\n",
    "    export_metric(validation_metric, epoch_id=epoch_id, batch_id=batch_id, loss=loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592fcff135654484ae45b976348aaf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training epoch...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:20:14.966 | INFO     | __main__:train_epoch:3 - START EPOCH epoch_id=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea23a2e297e4678aa410f01155185ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training batch...:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000043vscode-remote?line=1'>2</a>\u001b[0m progress \u001b[39m=\u001b[39m  tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,NUM_EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining epoch...\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000043vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m progress:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000043vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000043vscode-remote?line=4'>5</a>\u001b[0m     train_epoch(epoch_id\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000043vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39m# Validation\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000043vscode-remote?line=7'>8</a>\u001b[0m     valid_epoch(epoch_id\u001b[39m=\u001b[39mepoch)\n",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb Cell 42'\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_id)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000041vscode-remote?line=24'>25</a>\u001b[0m \u001b[39m# Metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000041vscode-remote?line=25'>26</a>\u001b[0m proba_prediction_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(transformed_prediction_batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000041vscode-remote?line=26'>27</a>\u001b[0m train_metrics_collection_dict \u001b[39m=\u001b[39m train_metric(proba_prediction_batch\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32), label_batch\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mint32))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000041vscode-remote?line=27'>28</a>\u001b[0m logger\u001b[39m.\u001b[39mtrace(train_metrics_collection_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/glove-cct-bce.ipynb#ch0000041vscode-remote?line=29'>30</a>\u001b[0m \u001b[39m# Backprop        \u001b[39;00m\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py:112\u001b[0m, in \u001b[0;36mMetricCollection.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=104'>105</a>\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39munused\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=106'>107</a>\u001b[0m     \u001b[39m\"\"\"Iteratively call forward for each metric.\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=107'>108</a>\u001b[0m \n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=108'>109</a>\u001b[0m \u001b[39m    Positional arguments (args) will be passed to every metric in the collection, while keyword arguments (kwargs)\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=109'>110</a>\u001b[0m \u001b[39m    will be filtered based on the signature of the individual metric.\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=110'>111</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=111'>112</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mm\u001b[39m.\u001b[39m_filter_kwargs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)) \u001b[39mfor\u001b[39;00m k, m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py:112\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=104'>105</a>\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39munused\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=106'>107</a>\u001b[0m     \u001b[39m\"\"\"Iteratively call forward for each metric.\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=107'>108</a>\u001b[0m \n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=108'>109</a>\u001b[0m \u001b[39m    Positional arguments (args) will be passed to every metric in the collection, while keyword arguments (kwargs)\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=109'>110</a>\u001b[0m \u001b[39m    will be filtered based on the signature of the individual metric.\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=110'>111</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/collections.py?line=111'>112</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mm\u001b[39m.\u001b[39;49m_filter_kwargs(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)) \u001b[39mfor\u001b[39;00m k, m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py:206\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=199'>200</a>\u001b[0m     \u001b[39mraise\u001b[39;00m TorchMetricsUserError(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=200'>201</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe Metric shouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be synced when performing ``update``. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=201'>202</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mHINT: Did you forget to call ``unsync`` ?.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=202'>203</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=204'>205</a>\u001b[0m \u001b[39m# global accumulation\u001b[39;00m\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=205'>206</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=207'>208</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_on_step:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=208'>209</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_on_step\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py:267\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=264'>265</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_called \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=265'>266</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/metric.py?line=266'>267</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py:167\u001b[0m, in \u001b[0;36mAUROC.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=159'>160</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, preds: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=160'>161</a>\u001b[0m     \u001b[39m\"\"\"Update state with predictions and targets.\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=161'>162</a>\u001b[0m \n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=162'>163</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=163'>164</a>\u001b[0m \u001b[39m        preds: Predictions from model (probabilities, or labels)\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=164'>165</a>\u001b[0m \u001b[39m        target: Ground truth labels\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=165'>166</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=166'>167</a>\u001b[0m     preds, target, mode \u001b[39m=\u001b[39m _auroc_update(preds, target)\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=168'>169</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreds\u001b[39m.\u001b[39mappend(preds)\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/classification/auroc.py?line=169'>170</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget\u001b[39m.\u001b[39mappend(target)\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py:38\u001b[0m, in \u001b[0;36m_auroc_update\u001b[0;34m(preds, target)\u001b[0m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=28'>29</a>\u001b[0m \u001b[39m\"\"\"Updates and returns variables required to compute Area Under the Receiver Operating Characteristic Curve.\u001b[39;00m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=29'>30</a>\u001b[0m \u001b[39mValidates the inputs and returns the mode of the inputs.\u001b[39;00m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=30'>31</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=33'>34</a>\u001b[0m \u001b[39m    target: Ground truth tensor\u001b[39;00m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=34'>35</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=36'>37</a>\u001b[0m \u001b[39m# use _input_format_classification for validating the input and get the mode of data\u001b[39;00m\n\u001b[0;32m---> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=37'>38</a>\u001b[0m _, _, mode \u001b[39m=\u001b[39m _input_format_classification(preds, target)\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti class multi dim\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/functional/classification/auroc.py?line=40'>41</a>\u001b[0m     n_classes \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:392\u001b[0m, in \u001b[0;36m_input_format_classification\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=388'>389</a>\u001b[0m \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=389'>390</a>\u001b[0m     preds \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=391'>392</a>\u001b[0m case \u001b[39m=\u001b[39m _check_classification_inputs(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=392'>393</a>\u001b[0m     preds,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=393'>394</a>\u001b[0m     target,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=394'>395</a>\u001b[0m     threshold\u001b[39m=\u001b[39;49mthreshold,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=395'>396</a>\u001b[0m     num_classes\u001b[39m=\u001b[39;49mnum_classes,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=396'>397</a>\u001b[0m     multiclass\u001b[39m=\u001b[39;49mmulticlass,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=397'>398</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=398'>399</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=400'>401</a>\u001b[0m \u001b[39mif\u001b[39;00m case \u001b[39min\u001b[39;00m (DataType\u001b[39m.\u001b[39mBINARY, DataType\u001b[39m.\u001b[39mMULTILABEL) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m top_k:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=401'>402</a>\u001b[0m     preds \u001b[39m=\u001b[39m (preds \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold)\u001b[39m.\u001b[39mint()\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:251\u001b[0m, in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=197'>198</a>\u001b[0m \u001b[39m\"\"\"Performs error checking on inputs for classification.\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=198'>199</a>\u001b[0m \n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=199'>200</a>\u001b[0m \u001b[39mThis ensures that preds and target take one of the shape/type combinations that are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=246'>247</a>\u001b[0m \u001b[39m        'multi-dim multi-class'\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=247'>248</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=249'>250</a>\u001b[0m \u001b[39m# Basic validation (that does not need case/type information)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=250'>251</a>\u001b[0m _basic_input_validation(preds, target, threshold, multiclass)\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=252'>253</a>\u001b[0m \u001b[39m# Check that shape/types fall into one of the cases\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=253'>254</a>\u001b[0m case, implied_classes \u001b[39m=\u001b[39m _check_shape_and_type_consistency(preds, target)\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:34\u001b[0m, in \u001b[0;36m_basic_input_validation\u001b[0;34m(preds, target, threshold, multiclass)\u001b[0m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39mis_floating_point():\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=32'>33</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe `target` has to be an integer tensor.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39mmin() \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=34'>35</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe `target` has to be a non-negative tensor.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=36'>37</a>\u001b[0m preds_float \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mis_floating_point()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "progress =  tqdm(range(1,NUM_EPOCHS+1), desc='training epoch...', leave=True)\n",
    "for epoch in progress:\n",
    "    # Train\n",
    "    train_epoch(epoch_id=epoch)\n",
    "\n",
    "    # Validation\n",
    "    valid_epoch(epoch_id=epoch)\n",
    "\n",
    "    # Save\n",
    "    torch.save(model, MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del train_df\n",
    "    del validation_df\n",
    "except NameError:\n",
    "    logger.warning(\"Train DataFrame & Validation DataFrame already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_DATASET_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[~test_df.white.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = JigsawDataset(test_df, tokenizer,\n",
    "                              vocab=vocab,\n",
    "                              max_len=max_len)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluation(model):\n",
    "    model.eval()\n",
    "    logger.info(f\"START EVALUATION\")\n",
    "\n",
    "    index_tensor = torch.Tensor([])\n",
    "    prediction_tensor = torch.Tensor([])\n",
    "\n",
    "    progress = tqdm(test_dataloader, desc='test batch...', leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        index_batch = batch[\"index\"].to(device)\n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch)\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "        proba_prediction_batch = torch.sigmoid(transformed_prediction_batch)\n",
    "        \n",
    "        index_tensor = torch.concat([index_tensor, index_batch.cpu()])\n",
    "        prediction_tensor = torch.concat([prediction_tensor, proba_prediction_batch.cpu()])\n",
    "    \n",
    "    logger.info(f\"END EVALUATION\")\n",
    "    prediction_test_df = pd.DataFrame(prediction_tensor.tolist(), \n",
    "                                     columns=LABEL_LIST,\n",
    "                                     index=index_tensor.to(int).tolist())\n",
    "    prediction_test_df.to_csv(TEST_FILE_PATH)\n",
    "    logger.success(f\"Test predictions exported !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 21:59:32.790 | INFO     | __main__:evaluation:4 - START EVALUATION\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c9b351d5e543fda7e40d2533f83f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test batch...:   0%|          | 0/5353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 22:17:00.915 | INFO     | __main__:evaluation:24 - END EVALUATION\n",
      "2022-03-30 22:17:01.443 | SUCCESS  | __main__:evaluation:29 - Test predictions exported !\n"
     ]
    }
   ],
   "source": [
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporter les prédictions de la dataset de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 22:17:01.572 | INFO     | __main__:export_validation:4 - START GET PREDICTION ON VALIDATION DATASET\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924c54363b0244d39cb943a88aa24d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid batch...:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 22:21:07.303 | INFO     | __main__:export_validation:26 - END GET PREDICTION ON VALIDATION DATASET\n",
      "2022-03-30 22:21:07.438 | SUCCESS  | __main__:export_validation:35 - Validation predictions exported !\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def export_validation(model):\n",
    "    model.eval()\n",
    "    logger.info(f\"START GET PREDICTION ON VALIDATION DATASET\")\n",
    "\n",
    "    index_tensor = torch.Tensor([])\n",
    "    prediction_tensor = torch.Tensor([])\n",
    "    label_tensor = torch.Tensor([])\n",
    "\n",
    "    progress = tqdm(validation_dataloader, desc='valid batch...', leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        index_batch = batch[\"index\"].to(device)\n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch)\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "        proba_prediction_batch = torch.sigmoid(transformed_prediction_batch)\n",
    "        \n",
    "        index_tensor = torch.concat([index_tensor, index_batch.cpu()])\n",
    "        prediction_tensor = torch.concat([prediction_tensor, proba_prediction_batch.cpu()])\n",
    "        label_tensor = torch.concat([label_tensor, label_batch.cpu()])\n",
    "    \n",
    "    logger.info(f\"END GET PREDICTION ON VALIDATION DATASET\")\n",
    "    prediction_valid_df = pd.DataFrame(prediction_tensor.tolist(), \n",
    "                                     columns=LABEL_LIST,\n",
    "                                     index=index_tensor.to(int).tolist())\n",
    "    label_valid_df = pd.DataFrame(label_tensor.tolist(), \n",
    "                                     columns=LABEL_LIST,\n",
    "                                     index=index_tensor.to(int).tolist())\n",
    "    prediction_valid_df.to_csv(VALIDATION_FILE_PATH)\n",
    "    label_valid_df.to_csv(VALIDATION_DATASET_FILE_PATH)\n",
    "    logger.success(f\"Validation predictions exported !\")\n",
    "export_validation(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61bdec511af6d11ab513a76379a59f4e5a5400220f7b851eb051cbea952f89aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
